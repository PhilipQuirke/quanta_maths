{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilipQuirke/quanta_maths/blob/main/notebooks/QCatGen_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quanta: Categorization & Generation - Run Experiments\n",
        "\n",
        "Run in Google Colab connected to T4 High-RAM instance.\n",
        "\n",
        "Refer proposal https://docs.google.com/document/d/1x7n2iy1_LZXZNLQpxCzF84lZ8BEG6ZT3KWXC59erhJA\n",
        "\n",
        "Your \"Secrets\" must contain MARTIAN_API_KEY and HF_TOKEN entries"
      ],
      "metadata": {
        "id": "srrYPsiaa090"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are tasks that we test the models on\n",
        "tasks = [\n",
        "    \"minimum\",\n",
        "    \"maximum\",\n",
        "    \"sum\",\n",
        "    \"difference\",\n",
        "    \"product\",\n",
        "    \"average\",\n",
        "    \"exponential\" # Excluded for now as too hard\n",
        "]\n",
        "\n",
        "# This is the prompt template we use for each task\n",
        "prompt_template = \"Answer minimally: Given the numbers {x} and {y} calculate the {task}\""
      ],
      "metadata": {
        "id": "h-t7WVsca1GS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Good open-source models that passed the first 6 tasks for 5 instances each.\n",
        "cached_good_open_models_6tasks_5instances = [\n",
        "    {\n",
        "        'name': 'meta-llama/llama-3.2-90b-vision-instruct',\n",
        "        'hf_repo': 'meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
        "        'url': 'https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
        "        'notes': 'Multimodal (text + images), vision reasoning capabilities'\n",
        "    },\n",
        "    {\n",
        "        'name': 'meta-llama/llama-4-maverick',\n",
        "        'hf_repo': 'meta-llama/Llama-4-Maverick-17B-128E',\n",
        "        'url': 'https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E',\n",
        "        'notes': '17B active params (~400B total), 128 experts, natively multimodal, 1M context',\n",
        "        'instruct_variant': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct'\n",
        "    },\n",
        "    {\n",
        "        'name': 'meta-llama/llama-4-scout',\n",
        "        'hf_repo': 'meta-llama/Llama-4-Scout-17B-16E',\n",
        "        'url': 'https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E',\n",
        "        'notes': '17B active params (~109B total), 16 experts, natively multimodal, 10M context, fits on single H100 GPU',\n",
        "        'instruct_variant': 'meta-llama/Llama-4-Scout-17B-16E-Instruct'\n",
        "    },\n",
        "    {\n",
        "        'name': 'nvidia/llama-3.1-nemotron-70b-instruct',\n",
        "        'hf_repo': 'nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
        "        'url': 'https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
        "        'notes': '70B model fine-tuned by NVIDIA using RLHF, #1 on Arena Hard/AlpacaEval 2 LC/MT-Bench as of Oct 2024, trained for helpfulness',\n",
        "        'base_model': 'meta-llama/Llama-3.1-70B-Instruct'\n",
        "    },\n",
        "    {\n",
        "        'name': 'qwen/qwen-2.5-coder-32b-instruct',\n",
        "        'hf_repo': 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
        "        'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct',\n",
        "        'notes': 'SOTA open-source code LLM, matches GPT-4o coding abilities, 128K context, 5.5T tokens training'\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "s8Jw-hcsa8wI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "focus_model_index = 3\n",
        "focus_model_data = cached_good_open_models_6tasks_5instances[focus_model_index]\n",
        "focus_model_hf_repo = focus_model_data['hf_repo']\n",
        "print(focus_model_hf_repo)\n"
      ],
      "metadata": {
        "id": "qEN2sHW1c8pM",
        "outputId": "cfd117c0-9ca1-4aa7-888b-d5aae5854263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "5IVjEg3YZ43N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "hefWxguVZ43O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=focus_model_hf_repo)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "w8pqFoSeZ43P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(focus_model_hf_repo)\n",
        "model = AutoModelForCausalLM.from_pretrained(tokenizer = AutoTokenizer.from_pretrained(focus_model_hf_repo))\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "lGxGM24fZ43P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ],
      "metadata": {
        "id": "aKcP9cXfZ43P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'"
      ],
      "metadata": {
        "id": "Mj1NLDF4Z43P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=focus_model_hf_repo,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "HvFfez2bZ43P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}