{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab604d1a",
   "metadata": {},
   "source": [
    "# Quanta Maths: Integer Addition and Subtraction in Transformers. Scan 200+ LLMs\n",
    "\n",
    "This Colab uses the app.withmartian.com API to test 200+ models\n",
    "to see whether given query like \"Answer concisely: 4444+5559=\" they answer \"10003\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9cc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-obvious cascading carry tests\n",
    "# Units column â‰¥10, all other columns sum to exactly 9\n",
    "add_tests = [\n",
    "    ['Answer concisely: 6+5=', '11'], # score 1\n",
    "    ['Answer concisely: 19+87=', '106'], # score 2\n",
    "    ['Answer concisely: 774+229=', '1003'], # score 3\n",
    "    ['Answer concisely: 6587+3416=', '10003'], # score 4\n",
    "    ['Answer concisely: 22605+77398=', '100003'], # score 5\n",
    "    ['Answer concisely: 532847+467159=', '1000006'], # score 6\n",
    "    ['Answer concisely: 5613709+4386294=', '10000003'], # score 7\n",
    "    ['Answer concisely: 72582383+27417619=', '100000002'], # score 8\n",
    "    ['Answer concisely: 206727644+793272359=', '1000000003'], # score 9\n",
    "    ['Answer concisely: 7580116456+2419883549=', '10000000005'], # score 10\n",
    "    ['Answer concisely: 52449010267+47550989737=', '100000000004'], # score 11\n",
    "    ['Answer concisely: 888522030597+111477969406=', '1000000000003'], # score 12\n",
    "    ['Answer concisely: 3764410205738+6235589794265=', '10000000000003'], # score 13\n",
    "    ['Answer concisely: 45928371046529+54071628953474=', '100000000000003'], # score 14\n",
    "    ['Answer concisely: 712850394621847+287149605378157=', '1000000000000004'], # score 15\n",
    "]\n",
    "\n",
    "# Cascading borrow tests\n",
    "# Smaller number minus larger number, requires cascading borrows, results in negative answers\n",
    "sub_tests = [\n",
    "    ['Answer concisely: 23-47=', '-24'], # score 1\n",
    "    ['Answer concisely: 156-289=', '-133'], # score 2\n",
    "    ['Answer concisely: 2047-5183=', '-3136'], # score 3\n",
    "    ['Answer concisely: 13056-27394=', '-14338'], # score 4\n",
    "    ['Answer concisely: 240157-583269=', '-343112'], # score 5\n",
    "    ['Answer concisely: 1350268-2794736=', '-1444468'], # score 6\n",
    "    ['Answer concisely: 24601379-51836472=', '-27235093'], # score 7\n",
    "    ['Answer concisely: 135702468-269485731=', '-133783263'], # score 8\n",
    "    ['Answer concisely: 2461035789-5138274692=', '-2677238903'], # score 9\n",
    "    ['Answer concisely: 13570246801-27394851639=', '-13824604838'], # score 10\n",
    "    ['Answer concisely: 246103578924-518372469157=', '-272268890233'], # score 11\n",
    "    ['Answer concisely: 1357024680135-2739485163728=', '-1382460483593'], # score 12\n",
    "    ['Answer concisely: 24610357892461-51837246915839=', '-27226889023378'], # score 13\n",
    "    ['Answer concisely: 135702468013570-273948516372946=', '-138246048359376'], # score 14\n",
    "    ['Answer concisely: 2461035789246103-5183724691583727=', '-2722688902337624'], # score 15\n",
    "]\n",
    "\n",
    "max_score_possible = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be3bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def is_ground_truth_correct(answer, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns True if the ground_truth appears as the final number in the answer, ignoring whitespace and punctuation.\n",
    "    Accepts answers like '13', '13.', '13**', 'The answer is 13', '**13**', 'random text **13** random text', 'boxed{13}'.\n",
    "    \"\"\"\n",
    "    # Remove trailing whitespace and punctuation\n",
    "    answer_clean = answer.strip().rstrip('.!**')\n",
    "    # Find all numbers in the answer (including negative numbers and those with commas)\n",
    "    numbers = re.findall(r'-?[\\d,]+', answer_clean)\n",
    "    \n",
    "    # Remove commas from the numbers for comparison\n",
    "    numbers_clean = [num.replace(',', '') for num in numbers]\n",
    "\n",
    "    answer_no_comma = answer.replace(\",\", \"\")\n",
    "\n",
    "    return (ground_truth == answer_no_comma or\n",
    "            \"**\"+ground_truth+\"**\" in answer or\n",
    "            \"boxed{\"+ground_truth+\"}\" in answer or\n",
    "            \"\"+ground_truth+\" \" in answer_no_comma  or\n",
    "            \"\"+ground_truth+\".\" in answer_no_comma  or\n",
    "            (numbers_clean and numbers_clean[-1] == ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca9822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_ground_truth_correct(\"-14338\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"-14338 \", \"-14338\")\n",
    "assert is_ground_truth_correct(\"-14338.\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"-14,338\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"-14,338 \", \"-14338\")\n",
    "assert is_ground_truth_correct(\"-14,338.\", \"-14338\")\n",
    "assert not is_ground_truth_correct(\"-14339\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"boxed{-14338}\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"**-14338**\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"blah blah-14338 blah blah\", \"-14338\")\n",
    "assert is_ground_truth_correct(\"135,702,468 - 269,485,731 = **-133,783,263**\", \"-133783263\")\n",
    "assert is_ground_truth_correct(\"12123 - 12312 = -14338\", \"-14338\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fade73",
   "metadata": {},
   "source": [
    "## Martian LLMs\n",
    "\n",
    "Supported martian models are at https://app.withmartian.com/docs/index.html\n",
    "and https://api.withmartian.com/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6de66cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 522 models from Martian API.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch Martian model data from the API\n",
    "martian_models_url = \"https://api.withmartian.com/v1/models\"\n",
    "response = requests.get(martian_models_url)\n",
    "martian_models_json = response.json()\n",
    "\n",
    "# Parse the data into a list of tuples: (id, prompt, completion, image, reliability_tier)\n",
    "martian_models_data = []\n",
    "for model in martian_models_json[\"data\"]:\n",
    "    pricing = model.get(\"pricing\", {})\n",
    "    martian_models_data.append(\n",
    "        (\n",
    "            model.get(\"id\"),\n",
    "            float(pricing.get(\"prompt\", 0)),\n",
    "            float(pricing.get(\"completion\", 0)),\n",
    "            float(pricing.get(\"image\", 0)),\n",
    "            model.get(\"reliability_tier\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# martian_models_data now contains the latest data from the endpoint\n",
    "print(f\"Loaded {len(martian_models_data)} models from Martian API.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f812972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data analysis\n",
    "\n",
    "def extract_model_names() -> List[str]:\n",
    "    \"\"\"Extract just the model names from the data\"\"\"\n",
    "    return [model[0] for model in martian_models_data]\n",
    "\n",
    "def extract_param_size(param_str: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"Convert parameter string to float (in billions)\"\"\"\n",
    "    if param_str is None:\n",
    "        return None\n",
    "    # If already a number, just return as float\n",
    "    if isinstance(param_str, (int, float)):\n",
    "        return float(param_str)\n",
    "    param_str = str(param_str)\n",
    "    # Remove 'B' and convert to float\n",
    "    if param_str.endswith('B'):\n",
    "        param_str = param_str[:-1]\n",
    "    # Handle mixtral format like \"8x22B\" -> approximate total params\n",
    "    if 'x' in param_str:\n",
    "        parts = param_str.split('x')\n",
    "        try:\n",
    "            return float(parts[0]) * float(parts[1])\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        return float(param_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_models_by_provider() -> Dict[str, List[Tuple[str, float, float, float, Optional[str]]]]:\n",
    "    \"\"\"Group models by provider\"\"\"\n",
    "    providers = {}\n",
    "    for model_data in martian_models_data:\n",
    "        provider = model_data[0].split('/')[0]\n",
    "        if provider not in providers:\n",
    "            providers[provider] = []\n",
    "        providers[provider].append(model_data)\n",
    "    return providers\n",
    "\n",
    "def get_cost_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Get cost statistics\"\"\"\n",
    "    input_costs = [model[1] for model in martian_models_data]\n",
    "    output_costs = [model[2] for model in martian_models_data]\n",
    "    return {\n",
    "        'input_cost_range': (min(input_costs), max(input_costs)),\n",
    "        'output_cost_range': (min(output_costs), max(output_costs)),\n",
    "        'avg_input_cost': sum(input_costs) / len(input_costs),\n",
    "        'avg_output_cost': sum(output_costs) / len(output_costs)\n",
    "    }\n",
    "\n",
    "def get_param_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Get parameter count statistics\"\"\"\n",
    "    params_with_data = [extract_param_size(model[4]) for model in martian_models_data if model[4] is not None]\n",
    "    params_with_data = [p for p in params_with_data if p is not None]\n",
    "    if not params_with_data:\n",
    "        return {'count': 0}\n",
    "    return {\n",
    "        'count': len(params_with_data),\n",
    "        'range': (min(params_with_data), max(params_with_data)),\n",
    "        'avg_params': sum(params_with_data) / len(params_with_data),\n",
    "        'models_with_param_info': len([m for m in martian_models_data if m[4] is not None]),\n",
    "        'total_models': len(martian_models_data)\n",
    "    }\n",
    "\n",
    "def find_cheapest_models(top_n: int = 5) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Find the cheapest models by input cost\"\"\"\n",
    "    sorted_by_input = sorted(martian_models_data, key=lambda x: x[1])\n",
    "    return [(model[0], model[1], model[2]) for model in sorted_by_input[:top_n]]\n",
    "\n",
    "def find_most_expensive_models(top_n: int = 5) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Find the most expensive models by input cost\"\"\"\n",
    "    sorted_by_input = sorted(martian_models_data, key=lambda x: x[1], reverse=True)\n",
    "    return [(model[0], model[1], model[2]) for model in sorted_by_input[:top_n]]\n",
    "\n",
    "def find_largest_models(top_n: int = 10) -> List[Tuple[str, Optional[str], float]]:\n",
    "    \"\"\"Find the largest models by parameter count\"\"\"\n",
    "    models_with_params = [(model[0], model[4], extract_param_size(model[4])) \n",
    "                         for model in martian_models_data if model[4] is not None]\n",
    "    models_with_params = [(m[0], m[1], m[2]) for m in models_with_params if m[2] is not None]\n",
    "    sorted_by_params = sorted(models_with_params, key=lambda x: x[2], reverse=True)\n",
    "    return sorted_by_params[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0d5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MARTIAN AI MODELS ANALYSIS ===\n",
      "\n",
      "ðŸ“Š Total models: 522\n",
      "ðŸ¢ Number of providers: 43\n",
      "\n",
      "ðŸ’° Cost Analysis (per 1M tokens):\n",
      "  Input cost range:  $0.00 - $0.00\n",
      "  Output cost range: $0.00 - $0.00\n",
      "  Average input:     $0.00\n",
      "  Average output:    $0.00\n",
      "\n",
      "ðŸ§  Parameter Analysis:\n",
      "  Models with param info: 522/522\n",
      "  Parameter range: 0.0B - 2.0B\n",
      "  Average size: 1.5B parameters\n",
      "\n",
      "ðŸ’¸ 5 Cheapest Models (input cost):\n",
      "  1. meta-llama/llama-3.2-1b-instruct:cheap   $0.00/$0.00\n",
      "  2. liquid/lfm-7b:cheap                      $0.00/$0.00\n",
      "  3. meta-llama/llama-3.2-1b-instruct         $0.00/$0.00\n",
      "  4. liquid/lfm-3b:cheap                      $0.00/$0.00\n",
      "  5. liquid/lfm-7b                            $0.00/$0.00\n",
      "\n",
      "ðŸ’Ž 5 Most Expensive Models (input cost):\n",
      "  1. openai/o1-pro                            $0.00/$0.00\n",
      "  2. openai/o1-pro:cheap                      $0.00/$0.00\n",
      "  3. openai/gpt-4                             $0.00/$0.00\n",
      "  4. anthropic/claude-3-opus-20240229         $0.00/$0.00\n",
      "  5. anthropic/claude-opus-4-0                $0.00/$0.00\n",
      "\n",
      "ðŸ¦£ 10 Largest Models:\n",
      "   1. ai21/jamba-large-1.7                            2 (2.0B)\n",
      "   2. ai21/jamba-large-1.7:cheap                      2 (2.0B)\n",
      "   3. ai21/jamba-mini-1.7                             2 (2.0B)\n",
      "   4. ai21/jamba-mini-1.7:cheap                       2 (2.0B)\n",
      "   5. aion-labs/aion-rp-llama-3.1-8b                  2 (2.0B)\n",
      "   6. aion-labs/aion-rp-llama-3.1-8b:cheap            2 (2.0B)\n",
      "   7. alfredpros/codellama-7b-instruct-solidity        2 (2.0B)\n",
      "   8. alfredpros/codellama-7b-instruct-solidity:cheap        2 (2.0B)\n",
      "   9. alibaba/tongyi-deepresearch-30b-a3b             2 (2.0B)\n",
      "  10. alibaba/tongyi-deepresearch-30b-a3b:cheap        2 (2.0B)\n",
      "\n",
      "ðŸ“‹ Sample model data structure:\n",
      "   Format: (model_name, input_cost_per_1M, output_cost_per_1M, request_cost, params)\n",
      "   ('ai21/jamba-large-1.7', 2e-06, 8e-06, 0.0, 2)\n",
      "   ('ai21/jamba-large-1.7:cheap', 1e-06, 4e-06, 0.0, 2)\n",
      "   ('ai21/jamba-mini-1.7', 2e-07, 4e-07, 0.0, 2)\n",
      "\n",
      "âœ… Use 'martian_models' for just the model names list\n",
      "âœ… Use 'martian_models_data' for full data with costs and parameters\n"
     ]
    }
   ],
   "source": [
    "martian_models = extract_model_names()\n",
    "\n",
    "# Print comprehensive statistics\n",
    "\n",
    "print(\"=== MARTIAN AI MODELS ANALYSIS ===\\n\")\n",
    "\n",
    "print(f\"ðŸ“Š Total models: {len(martian_models_data)}\")\n",
    "\n",
    "# Provider breakdown\n",
    "providers = get_models_by_provider()\n",
    "print(f\"ðŸ¢ Number of providers: {len(providers)}\")\n",
    "#print(\"\\nðŸ“ˆ Models per provider:\")\n",
    "#for provider, models in sorted(providers.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "#    print(f\"  {provider:20} {len(models):3d} models\")\n",
    "\n",
    "# Cost analysis\n",
    "cost_stats = get_cost_stats()\n",
    "print(f\"\\nðŸ’° Cost Analysis (per 1M tokens):\")\n",
    "print(f\"  Input cost range:  ${cost_stats['input_cost_range'][0]:.2f} - ${cost_stats['input_cost_range'][1]:.2f}\")\n",
    "print(f\"  Output cost range: ${cost_stats['output_cost_range'][0]:.2f} - ${cost_stats['output_cost_range'][1]:.2f}\")\n",
    "print(f\"  Average input:     ${cost_stats['avg_input_cost']:.2f}\")\n",
    "print(f\"  Average output:    ${cost_stats['avg_output_cost']:.2f}\")\n",
    "\n",
    "# Parameter analysis\n",
    "param_stats = get_param_stats()\n",
    "print(f\"\\nðŸ§  Parameter Analysis:\")\n",
    "print(f\"  Models with param info: {param_stats['models_with_param_info']}/{param_stats['total_models']}\")\n",
    "if param_stats['count'] > 0:\n",
    "    print(f\"  Parameter range: {param_stats['range'][0]:.1f}B - {param_stats['range'][1]:.1f}B\")\n",
    "    print(f\"  Average size: {param_stats['avg_params']:.1f}B parameters\")\n",
    "\n",
    "# Top lists\n",
    "print(f\"\\nðŸ’¸ 5 Cheapest Models (input cost):\")\n",
    "for i, (model, input_cost, output_cost) in enumerate(find_cheapest_models(), 1):\n",
    "    print(f\"  {i}. {model:40} ${input_cost:.2f}/${output_cost:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ’Ž 5 Most Expensive Models (input cost):\")\n",
    "for i, (model, input_cost, output_cost) in enumerate(find_most_expensive_models(), 1):\n",
    "    print(f\"  {i}. {model:40} ${input_cost:.2f}/${output_cost:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ¦£ 10 Largest Models:\")\n",
    "for i, (model, param_str, param_float) in enumerate(find_largest_models(), 1):\n",
    "    print(f\"  {i:2d}. {model:40} {param_str:>8} ({param_float:.1f}B)\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Sample model data structure:\")\n",
    "print(f\"   Format: (model_name, input_cost_per_1M, output_cost_per_1M, request_cost, params)\")\n",
    "for i in range(3):\n",
    "    model = martian_models_data[i]\n",
    "    print(f\"   {model}\")\n",
    "\n",
    "print(f\"\\nâœ… Use 'martian_models' for just the model names list\")\n",
    "print(f\"âœ… Use 'martian_models_data' for full data with costs and parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8794",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a10a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import time\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
    "assert MARTIAN_API_KEY, \"API key not found. Please set MARTIAN_API_KEY in your .env file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.withmartian.com/v1\",\n",
    "    api_key=MARTIAN_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa7ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_inference(model_name, prompt, ground_truth, timeout=300):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        max_tokens=1024, # Mandatory param for some models. Ignored by others.\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    success = is_ground_truth_correct(answer, ground_truth)\n",
    "    return answer, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_progressive(the_models, tests, max_workers=8):\n",
    "    model_scores = []\n",
    "\n",
    "    def score_model(model_name):\n",
    "        score = 0\n",
    "        for test_idx, (prompt, ground_truth) in enumerate(tests):\n",
    "            try:\n",
    "                answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
    "                if success:\n",
    "                    score = test_idx + 1\n",
    "                else:\n",
    "                    break\n",
    "            except openai.APIError as e:\n",
    "                if hasattr(e, 'status_code'):\n",
    "                    score = -e.status_code\n",
    "                else:\n",
    "                    score = -999\n",
    "                break\n",
    "            except Exception as e:\n",
    "                score = -999\n",
    "                break\n",
    "        return {\"model\": model_name, \"score\": score}\n",
    "\n",
    "    print(f\"Evaluating {len(the_models)} models concurrently with {max_workers} workers...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_model = {executor.submit(score_model, model_name): model_name for model_name in the_models}\n",
    "        for idx, future in enumerate(as_completed(future_to_model), 1):\n",
    "            model_name = future_to_model[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = {\"model\": model_name, \"score\": -999}\n",
    "            print(f\"[{idx}/{len(the_models)}] {result['model']}: Score = {result['score']}\")\n",
    "            model_scores.append(result)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a759f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 261 models concurrently with 8 workers...\n",
      "[1/261] alfredpros/codellama-7b-instruct-solidity: Score = 0\n",
      "[1/261] alfredpros/codellama-7b-instruct-solidity: Score = 0\n",
      "[2/261] allenai/molmo-7b-d: Score = 3\n",
      "[2/261] allenai/molmo-7b-d: Score = 3\n",
      "[3/261] aion-labs/aion-rp-llama-3.1-8b: Score = 2\n",
      "[3/261] aion-labs/aion-rp-llama-3.1-8b: Score = 2\n",
      "[4/261] allenai/olmo-2-0325-32b-instruct: Score = 4\n",
      "[4/261] allenai/olmo-2-0325-32b-instruct: Score = 4\n",
      "[5/261] ai21/jamba-mini-1.7: Score = 4\n",
      "[6/261] amazon/nova-micro-v1: Score = 3\n",
      "[7/261] alpindale/goliath-120b: Score = 4\n",
      "[5/261] ai21/jamba-mini-1.7: Score = 4\n",
      "[6/261] amazon/nova-micro-v1: Score = 3\n",
      "[7/261] alpindale/goliath-120b: Score = 4\n",
      "[8/261] anthracite-org/magnum-v4-72b: Score = 4\n",
      "[8/261] anthracite-org/magnum-v4-72b: Score = 4\n",
      "[9/261] ai21/jamba-large-1.7: Score = 8\n",
      "[9/261] ai21/jamba-large-1.7: Score = 8\n",
      "[10/261] anthropic/claude-3-5-haiku-20241022: Score = 6\n",
      "[10/261] anthropic/claude-3-5-haiku-20241022: Score = 6\n",
      "[11/261] amazon/nova-lite-v1: Score = 15\n",
      "[11/261] amazon/nova-lite-v1: Score = 15\n",
      "[12/261] amazon/nova-pro-v1: Score = 12\n",
      "[13/261] anthropic/claude-3-5-sonnet-20241022: Score = 12\n",
      "[12/261] amazon/nova-pro-v1: Score = 12\n",
      "[13/261] anthropic/claude-3-5-sonnet-20241022: Score = 12\n",
      "[14/261] anthropic/claude-3-5-sonnet-20240620: Score = 15\n",
      "[14/261] anthropic/claude-3-5-sonnet-20240620: Score = 15\n",
      "[15/261] anthropic/claude-3-5-sonnet-latest: Score = 12\n",
      "[15/261] anthropic/claude-3-5-sonnet-latest: Score = 12\n",
      "[16/261] anthropic/claude-3-5-haiku-latest: Score = 15\n",
      "[16/261] anthropic/claude-3-5-haiku-latest: Score = 15\n",
      "[17/261] alibaba/tongyi-deepresearch-30b-a3b: Score = 4\n",
      "[17/261] alibaba/tongyi-deepresearch-30b-a3b: Score = 4\n",
      "[18/261] anthropic/claude-3-7-sonnet-20250219: Score = 12\n",
      "[18/261] anthropic/claude-3-7-sonnet-20250219: Score = 12\n",
      "[19/261] anthropic/claude-3-haiku-20240307: Score = 15\n",
      "[19/261] anthropic/claude-3-haiku-20240307: Score = 15\n",
      "[20/261] anthropic/claude-3-7-sonnet-latest: Score = 12\n",
      "[20/261] anthropic/claude-3-7-sonnet-latest: Score = 12\n",
      "[21/261] anthropic/claude-3-opus-20240229: Score = 15\n",
      "[21/261] anthropic/claude-3-opus-20240229: Score = 15\n",
      "[22/261] anthropic/claude-opus-4-0: Score = 15\n",
      "[22/261] anthropic/claude-opus-4-0: Score = 15\n",
      "[23/261] anthropic/claude-opus-4-20250514: Score = 15\n",
      "[24/261] arcee-ai/afm-4.5b: Score = 3\n",
      "[23/261] anthropic/claude-opus-4-20250514: Score = 15\n",
      "[24/261] arcee-ai/afm-4.5b: Score = 3\n",
      "[25/261] arcee-ai/coder-large: Score = 3\n",
      "[25/261] arcee-ai/coder-large: Score = 3\n",
      "[26/261] arcee-ai/virtuoso-large: Score = 3\n",
      "[26/261] arcee-ai/virtuoso-large: Score = 3\n",
      "[27/261] anthropic/claude-sonnet-4-0: Score = 15\n",
      "[27/261] anthropic/claude-sonnet-4-0: Score = 15\n",
      "[28/261] anthropic/claude-sonnet-4-20250514: Score = 15\n",
      "[28/261] anthropic/claude-sonnet-4-20250514: Score = 15\n",
      "[29/261] baidu/ernie-4.5-vl-28b-a3b: Score = 3\n",
      "[29/261] baidu/ernie-4.5-vl-28b-a3b: Score = 3\n",
      "[30/261] anthropic/claude-sonnet-4-5: Score = 15\n",
      "[31/261] baidu/ernie-4.5-300b-a47b: Score = 4\n",
      "[30/261] anthropic/claude-sonnet-4-5: Score = 15\n",
      "[31/261] baidu/ernie-4.5-300b-a47b: Score = 4\n",
      "[32/261] baidu/ernie-4.5-21b-a3b: Score = 7\n",
      "[32/261] baidu/ernie-4.5-21b-a3b: Score = 7\n",
      "[33/261] anthropic/claude-opus-4-1: Score = 15\n",
      "[33/261] anthropic/claude-opus-4-1: Score = 15\n",
      "[34/261] bytedance/ui-tars-1.5-7b: Score = 2\n",
      "[35/261] cohere/command-r-plus-08-2024: Score = 2\n",
      "[34/261] bytedance/ui-tars-1.5-7b: Score = 2\n",
      "[35/261] cohere/command-r-plus-08-2024: Score = 2\n",
      "[36/261] cohere/command-r7b-12-2024: Score = 5\n",
      "[36/261] cohere/command-r7b-12-2024: Score = 5\n",
      "[37/261] deepcogito/cogito-v2-preview-deepseek-671b: Score = 5\n",
      "[37/261] deepcogito/cogito-v2-preview-deepseek-671b: Score = 5\n",
      "[38/261] anthropic/claude-sonnet-4-5-20250929: Score = 15\n",
      "[38/261] anthropic/claude-sonnet-4-5-20250929: Score = 15\n",
      "[39/261] cohere/command-r-08-2024: Score = 11\n",
      "[39/261] cohere/command-r-08-2024: Score = 11\n",
      "[40/261] deepinfra/google/gemini-2.0-flash-001: Score = 4\n",
      "[40/261] deepinfra/google/gemini-2.0-flash-001: Score = 4\n",
      "[41/261] arcee-ai/maestro-reasoning: Score = 4\n",
      "[41/261] arcee-ai/maestro-reasoning: Score = 4\n",
      "[42/261] deepcogito/cogito-v2-preview-llama-109b-moe: Score = 10\n",
      "[42/261] deepcogito/cogito-v2-preview-llama-109b-moe: Score = 10\n",
      "[43/261] deepinfra/google/gemma-3-12b-it: Score = 11\n",
      "[43/261] deepinfra/google/gemma-3-12b-it: Score = 11\n",
      "[44/261] deepinfra/google/gemma-3-4b-it: Score = 8\n",
      "[44/261] deepinfra/google/gemma-3-4b-it: Score = 8\n",
      "[45/261] deepinfra/microsoft/phi-4: Score = 6\n",
      "[46/261] deepinfra/microsoft/phi-4-reasoning-plus: Score = 6\n",
      "[45/261] deepinfra/microsoft/phi-4: Score = 6\n",
      "[46/261] deepinfra/microsoft/phi-4-reasoning-plus: Score = 6\n",
      "[47/261] anthropic/claude-opus-4-1-20250805: Score = 15\n",
      "[47/261] anthropic/claude-opus-4-1-20250805: Score = 15\n",
      "[48/261] deepinfra/google/gemma-3-27b-it: Score = 15\n",
      "[48/261] deepinfra/google/gemma-3-27b-it: Score = 15\n",
      "[49/261] deepinfra/google/gemini-2.5-flash: Score = 11\n",
      "[49/261] deepinfra/google/gemini-2.5-flash: Score = 11\n",
      "[50/261] deepseek/deepseek-chat: Score = 6\n",
      "[50/261] deepseek/deepseek-chat: Score = 6\n",
      "[51/261] deepseek/deepseek-chat-v3-0324: Score = 10\n",
      "[51/261] deepseek/deepseek-chat-v3-0324: Score = 10\n",
      "[52/261] deepseek/deepseek-prover-v2: Score = 11\n",
      "[52/261] deepseek/deepseek-prover-v2: Score = 11\n",
      "[53/261] baidu/ernie-4.5-vl-424b-a47b: Score = 15\n",
      "[53/261] baidu/ernie-4.5-vl-424b-a47b: Score = 15\n",
      "[54/261] deepinfra/openai/gpt-oss-20b: Score = 13\n",
      "[54/261] deepinfra/openai/gpt-oss-20b: Score = 13\n",
      "[55/261] deepseek/deepseek-chat-v3.1: Score = 11\n",
      "[55/261] deepseek/deepseek-chat-v3.1: Score = 11\n",
      "[56/261] deepseek/deepseek-r1-distill-llama-8b: Score = -404\n",
      "[56/261] deepseek/deepseek-r1-distill-llama-8b: Score = -404\n",
      "[57/261] deepseek/deepseek-r1-distill-llama-70b: Score = 2\n",
      "[57/261] deepseek/deepseek-r1-distill-llama-70b: Score = 2\n",
      "[58/261] deepseek/deepseek-r1-0528-qwen3-8b: Score = 1\n",
      "[59/261] deepinfra/openai/gpt-oss-120b: Score = 14\n",
      "[58/261] deepseek/deepseek-r1-0528-qwen3-8b: Score = 1\n",
      "[59/261] deepinfra/openai/gpt-oss-120b: Score = 14\n",
      "[60/261] deepseek/deepseek-r1-distill-qwen-14b: Score = 3\n",
      "[60/261] deepseek/deepseek-r1-distill-qwen-14b: Score = 3\n",
      "[61/261] google/gemini-2.0-flash: Score = 4\n",
      "[61/261] google/gemini-2.0-flash: Score = 4\n",
      "[62/261] google/gemini-2.0-flash-001: Score = 4\n",
      "[62/261] google/gemini-2.0-flash-001: Score = 4\n",
      "[63/261] google/gemini-2.0-flash-lite: Score = 2\n",
      "[63/261] google/gemini-2.0-flash-lite: Score = 2\n",
      "[64/261] google/gemini-2.0-flash-lite-001: Score = 2\n",
      "[64/261] google/gemini-2.0-flash-lite-001: Score = 2\n",
      "[65/261] google/gemini-2.0-flash-lite-preview: Score = 2\n",
      "[65/261] google/gemini-2.0-flash-lite-preview: Score = 2\n",
      "[66/261] google/gemini-2.0-flash-lite-preview-02-05: Score = 2\n",
      "[66/261] google/gemini-2.0-flash-lite-preview-02-05: Score = 2\n",
      "[67/261] deepinfra/google/gemini-2.5-pro: Score = 11\n",
      "[67/261] deepinfra/google/gemini-2.5-pro: Score = 11\n",
      "[68/261] google/gemini-2.5-flash: Score = 5\n",
      "[68/261] google/gemini-2.5-flash: Score = 5\n",
      "[69/261] google/gemini-2.5-flash-lite: Score = 4\n",
      "[69/261] google/gemini-2.5-flash-lite: Score = 4\n",
      "[70/261] google/gemini-2.5-flash-lite-preview-06-17: Score = 4\n",
      "[71/261] google/gemini-2.5-flash-lite-preview-09-2025: Score = 3\n",
      "[70/261] google/gemini-2.5-flash-lite-preview-06-17: Score = 4\n",
      "[71/261] google/gemini-2.5-flash-lite-preview-09-2025: Score = 3\n",
      "[72/261] google/gemini-2.5-flash-preview-09-2025: Score = 7\n",
      "[72/261] google/gemini-2.5-flash-preview-09-2025: Score = 7\n",
      "[73/261] google/gemini-2.5-flash-preview-05-20: Score = 4\n",
      "[73/261] google/gemini-2.5-flash-preview-05-20: Score = 4\n",
      "[74/261] deepseek/deepseek-r1-distill-qwen-32b: Score = 5\n",
      "[74/261] deepseek/deepseek-r1-distill-qwen-32b: Score = 5\n",
      "[75/261] deepseek/deepseek-v3.1-terminus: Score = 9\n",
      "[75/261] deepseek/deepseek-v3.1-terminus: Score = 9\n",
      "[76/261] cohere/command-a: Score = 3\n",
      "[76/261] cohere/command-a: Score = 3\n",
      "[77/261] google/gemini-flash-latest: Score = -999\n",
      "[78/261] deepseek/deepseek-r1: Score = 7\n",
      "[77/261] google/gemini-flash-latest: Score = -999\n",
      "[78/261] deepseek/deepseek-r1: Score = 7\n",
      "[79/261] google/gemini-flash-lite-latest: Score = 3\n",
      "[79/261] google/gemini-flash-lite-latest: Score = 3\n",
      "[80/261] google/gemini-robotics-er-1.5-preview: Score = -999\n",
      "[80/261] google/gemini-robotics-er-1.5-preview: Score = -999\n",
      "[81/261] gryphe/mythomax-l2-13b: Score = 1\n",
      "[82/261] google/gemini-2.5-pro-preview-05-06: Score = -999\n",
      "[81/261] gryphe/mythomax-l2-13b: Score = 1\n",
      "[82/261] google/gemini-2.5-pro-preview-05-06: Score = -999\n",
      "[83/261] inception/mercury-coder: Score = 3\n",
      "[83/261] inception/mercury-coder: Score = 3\n",
      "[84/261] inception/mercury: Score = 6\n",
      "[84/261] inception/mercury: Score = 6\n",
      "[85/261] liquid/lfm-3b: Score = 2\n",
      "[85/261] liquid/lfm-3b: Score = 2\n",
      "[86/261] liquid/lfm-7b: Score = 3\n",
      "[86/261] liquid/lfm-7b: Score = 3\n",
      "[87/261] google/gemini-2.5-pro-preview-06-05: Score = -999\n",
      "[88/261] mancer/weaver: Score = 0\n",
      "[87/261] google/gemini-2.5-pro-preview-06-05: Score = -999\n",
      "[88/261] mancer/weaver: Score = 0\n",
      "[89/261] google/gemini-2.5-pro: Score = -999\n",
      "[89/261] google/gemini-2.5-pro: Score = -999\n",
      "[90/261] meta-llama/llama-3-8b-instruct: Score = 3\n",
      "[90/261] meta-llama/llama-3-8b-instruct: Score = 3\n",
      "[91/261] google/gemini-pro-latest: Score = -999\n",
      "[91/261] google/gemini-pro-latest: Score = -999\n",
      "[92/261] meta-llama/llama-3-70b-instruct: Score = 5\n",
      "[93/261] meta-llama/llama-3.1-405b: Score = 0\n",
      "[92/261] meta-llama/llama-3-70b-instruct: Score = 5\n",
      "[93/261] meta-llama/llama-3.1-405b: Score = 0\n",
      "[94/261] meituan/longcat-flash-chat: Score = 8\n",
      "[94/261] meituan/longcat-flash-chat: Score = 8\n",
      "[95/261] meta-llama/llama-3.1-8b-instruct: Score = 3\n",
      "[95/261] meta-llama/llama-3.1-8b-instruct: Score = 3\n",
      "[96/261] meta-llama/llama-3.2-11b-vision-instruct: Score = 2\n",
      "[97/261] meta-llama/llama-3.2-1b-instruct: Score = 3\n",
      "[96/261] meta-llama/llama-3.2-11b-vision-instruct: Score = 2\n",
      "[97/261] meta-llama/llama-3.2-1b-instruct: Score = 3\n",
      "[98/261] meta-llama/llama-3.2-3b-instruct: Score = 2\n",
      "[98/261] meta-llama/llama-3.2-3b-instruct: Score = 2\n",
      "[99/261] meta-llama/llama-3.1-70b-instruct: Score = 4\n",
      "[99/261] meta-llama/llama-3.1-70b-instruct: Score = 4\n",
      "[100/261] meta-llama/llama-3.2-90b-vision-instruct: Score = 4\n",
      "[100/261] meta-llama/llama-3.2-90b-vision-instruct: Score = 4\n",
      "[101/261] meta-llama/llama-guard-4-12b: Score = 0\n",
      "[101/261] meta-llama/llama-guard-4-12b: Score = 0\n",
      "[102/261] meta-llama/llama-3.3-70b-instruct: Score = 5\n",
      "[102/261] meta-llama/llama-3.3-70b-instruct: Score = 5\n",
      "[103/261] meta-llama/llama-4-maverick: Score = 4\n",
      "[103/261] meta-llama/llama-4-maverick: Score = 4\n",
      "[104/261] meta-llama/llama-4-scout: Score = 5\n",
      "[104/261] meta-llama/llama-4-scout: Score = 5\n",
      "[105/261] google/gemini-2.5-pro-preview-03-25: Score = -999\n",
      "[105/261] google/gemini-2.5-pro-preview-03-25: Score = -999\n",
      "[106/261] microsoft/phi-3-mini-128k-instruct: Score = 3\n",
      "[107/261] microsoft/phi-3-medium-128k-instruct: Score = 4\n",
      "[106/261] microsoft/phi-3-mini-128k-instruct: Score = 3\n",
      "[107/261] microsoft/phi-3-medium-128k-instruct: Score = 4\n",
      "[108/261] microsoft/phi-3.5-mini-128k-instruct: Score = 3\n",
      "[108/261] microsoft/phi-3.5-mini-128k-instruct: Score = 3\n",
      "[109/261] microsoft/phi-4: Score = 3\n",
      "[109/261] microsoft/phi-4: Score = 3\n",
      "[110/261] microsoft/phi-4-multimodal-instruct: Score = 8\n",
      "[110/261] microsoft/phi-4-multimodal-instruct: Score = 8\n",
      "[111/261] mistralai/codestral-2501: Score = 2\n",
      "[111/261] mistralai/codestral-2501: Score = 2\n",
      "[112/261] mistralai/codestral-2508: Score = 2\n",
      "[112/261] mistralai/codestral-2508: Score = 2\n",
      "[113/261] microsoft/phi-4-reasoning-plus: Score = 10\n",
      "[113/261] microsoft/phi-4-reasoning-plus: Score = 10\n",
      "[114/261] mistralai/devstral-small: Score = 4\n",
      "[114/261] mistralai/devstral-small: Score = 4\n",
      "[115/261] mistralai/devstral-small-2505: Score = 4\n",
      "[115/261] mistralai/devstral-small-2505: Score = 4\n",
      "[116/261] deepseek/deepseek-r1-0528: Score = 5\n",
      "[116/261] deepseek/deepseek-r1-0528: Score = 5\n",
      "[117/261] mistralai/devstral-medium: Score = 15\n",
      "[118/261] mistralai/magistral-small-2506: Score = 4\n",
      "[119/261] microsoft/wizardlm-2-8x22b: Score = 9\n",
      "[117/261] mistralai/devstral-medium: Score = 15\n",
      "[118/261] mistralai/magistral-small-2506: Score = 4\n",
      "[119/261] microsoft/wizardlm-2-8x22b: Score = 9\n",
      "[120/261] meta-llama/llama-guard-3-8b: Score = 1\n",
      "[120/261] meta-llama/llama-guard-3-8b: Score = 1\n",
      "[121/261] mistralai/ministral-3b: Score = 2\n",
      "[121/261] mistralai/ministral-3b: Score = 2\n",
      "[122/261] mistralai/ministral-8b: Score = 4\n",
      "[122/261] mistralai/ministral-8b: Score = 4\n",
      "[123/261] mistralai/mistral-7b-instruct-v0.1: Score = 2\n",
      "[123/261] mistralai/mistral-7b-instruct-v0.1: Score = 2\n",
      "[124/261] mistralai/magistral-medium-2506: Score = 15\n",
      "[124/261] mistralai/magistral-medium-2506: Score = 15\n",
      "[125/261] mistralai/mistral-7b-instruct: Score = 4\n",
      "[126/261] mistralai/mistral-7b-instruct-v0.3: Score = 4\n",
      "[125/261] mistralai/mistral-7b-instruct: Score = 4\n",
      "[126/261] mistralai/mistral-7b-instruct-v0.3: Score = 4\n",
      "[127/261] mistralai/mistral-large-2411: Score = 7\n",
      "[127/261] mistralai/mistral-large-2411: Score = 7\n",
      "[128/261] mistralai/magistral-medium-2506:thinking: Score = 2\n",
      "[128/261] mistralai/magistral-medium-2506:thinking: Score = 2\n",
      "[129/261] mistralai/mistral-large: Score = 9\n",
      "[129/261] mistralai/mistral-large: Score = 9\n",
      "[130/261] mistralai/mistral-large-2407: Score = 9\n",
      "[130/261] mistralai/mistral-large-2407: Score = 9\n",
      "[131/261] mistralai/mistral-saba: Score = 4\n",
      "[131/261] mistralai/mistral-saba: Score = 4\n",
      "[132/261] mistralai/mistral-small: Score = 4\n",
      "[132/261] mistralai/mistral-small: Score = 4\n",
      "[133/261] mistralai/mistral-nemo: Score = 7\n",
      "[133/261] mistralai/mistral-nemo: Score = 7\n",
      "[134/261] mistralai/mistral-medium-3: Score = 10\n",
      "[134/261] mistralai/mistral-medium-3: Score = 10\n",
      "[135/261] mistralai/mistral-small-3.2-24b-instruct: Score = 4\n",
      "[135/261] mistralai/mistral-small-3.2-24b-instruct: Score = 4\n",
      "[136/261] mistralai/mistral-tiny: Score = 3\n",
      "[136/261] mistralai/mistral-tiny: Score = 3\n",
      "[137/261] mistralai/mistral-small-24b-instruct-2501: Score = 9\n",
      "[137/261] mistralai/mistral-small-24b-instruct-2501: Score = 9\n",
      "[138/261] mistralai/pixtral-large-2411: Score = 2\n",
      "[139/261] mistralai/mistral-medium-3.1: Score = 1\n",
      "[140/261] mistralai/mistral-small-3.1-24b-instruct: Score = 6\n",
      "[138/261] mistralai/pixtral-large-2411: Score = 2\n",
      "[139/261] mistralai/mistral-medium-3.1: Score = 1\n",
      "[140/261] mistralai/mistral-small-3.1-24b-instruct: Score = 6\n",
      "[141/261] morph/morph-v3-fast: Score = 0\n",
      "[141/261] morph/morph-v3-fast: Score = 0\n",
      "[142/261] mistralai/mixtral-8x7b-instruct: Score = 4\n",
      "[142/261] mistralai/mixtral-8x7b-instruct: Score = 4\n",
      "[143/261] morph/morph-v3-large: Score = 2\n",
      "[143/261] morph/morph-v3-large: Score = 2\n",
      "[144/261] mistralai/mixtral-8x22b-instruct: Score = 9\n",
      "[144/261] mistralai/mixtral-8x22b-instruct: Score = 9\n",
      "[145/261] neversleep/noromaid-20b: Score = 1\n",
      "[145/261] neversleep/noromaid-20b: Score = 1\n",
      "[146/261] neversleep/llama-3.1-lumimaid-8b: Score = 2\n",
      "[146/261] neversleep/llama-3.1-lumimaid-8b: Score = 2\n",
      "[147/261] moonshotai/kimi-k2: Score = 6\n",
      "[147/261] moonshotai/kimi-k2: Score = 6\n",
      "[148/261] nousresearch/hermes-3-llama-3.1-405b: Score = 5\n",
      "[148/261] nousresearch/hermes-3-llama-3.1-405b: Score = 5\n",
      "[149/261] moonshotai/kimi-k2-0905: Score = 8\n",
      "[149/261] moonshotai/kimi-k2-0905: Score = 8\n",
      "[150/261] nousresearch/hermes-3-llama-3.1-70b: Score = -502\n",
      "[150/261] nousresearch/hermes-3-llama-3.1-70b: Score = -502\n",
      "[151/261] nousresearch/hermes-4-70b: Score = 4\n",
      "[151/261] nousresearch/hermes-4-70b: Score = 4\n",
      "[152/261] nousresearch/hermes-4-405b: Score = 5\n",
      "[152/261] nousresearch/hermes-4-405b: Score = 5\n",
      "[153/261] nvidia/llama-3.1-nemotron-70b-instruct: Score = 5\n",
      "[153/261] nvidia/llama-3.1-nemotron-70b-instruct: Score = 5\n",
      "[154/261] nousresearch/hermes-2-pro-llama-3-8b: Score = 4\n",
      "[154/261] nousresearch/hermes-2-pro-llama-3-8b: Score = 4\n",
      "[155/261] nvidia/nemotron-nano-9b-v2: Score = 1\n",
      "[155/261] nvidia/nemotron-nano-9b-v2: Score = 1\n",
      "[156/261] openai/gpt-3.5-turbo-16k: Score = 4\n",
      "[157/261] nvidia/llama-3.1-nemotron-ultra-253b-v1: Score = 6\n",
      "[156/261] openai/gpt-3.5-turbo-16k: Score = 4\n",
      "[157/261] nvidia/llama-3.1-nemotron-ultra-253b-v1: Score = 6\n",
      "[158/261] openai/chatgpt-4o-latest: Score = 8\n",
      "[158/261] openai/chatgpt-4o-latest: Score = 8\n",
      "[159/261] openai/gpt-3.5-turbo: Score = 9\n",
      "[159/261] openai/gpt-3.5-turbo: Score = 9\n",
      "[160/261] openai/gpt-4: Score = 9\n",
      "[161/261] openai/gpt-4-1106-preview: Score = 7\n",
      "[160/261] openai/gpt-4: Score = 9\n",
      "[161/261] openai/gpt-4-1106-preview: Score = 7\n",
      "[162/261] openai/gpt-4-turbo-preview: Score = 8\n",
      "[162/261] openai/gpt-4-turbo-preview: Score = 8\n",
      "[163/261] openai/gpt-4.1: Score = 6\n",
      "[164/261] openai/gpt-4-turbo: Score = 7\n",
      "[163/261] openai/gpt-4.1: Score = 6\n",
      "[164/261] openai/gpt-4-turbo: Score = 7\n",
      "[165/261] openai/gpt-4.1-2025-04-14: Score = 5\n",
      "[165/261] openai/gpt-4.1-2025-04-14: Score = 5\n",
      "[166/261] openai/gpt-4.1-mini-2025-04-14: Score = 6\n",
      "[167/261] openai/gpt-4.1-nano: Score = 6\n",
      "[166/261] openai/gpt-4.1-mini-2025-04-14: Score = 6\n",
      "[167/261] openai/gpt-4.1-nano: Score = 6\n",
      "[168/261] openai/gpt-4.1-nano-2025-04-14: Score = 9\n",
      "[168/261] openai/gpt-4.1-nano-2025-04-14: Score = 9\n",
      "[169/261] openai/gpt-4o-2024-05-13: Score = 6\n",
      "[169/261] openai/gpt-4o-2024-05-13: Score = 6\n",
      "[170/261] openai/gpt-4.1-mini: Score = 11\n",
      "[170/261] openai/gpt-4.1-mini: Score = 11\n",
      "[171/261] openai/gpt-4o-2024-11-20: Score = 8\n",
      "[171/261] openai/gpt-4o-2024-11-20: Score = 8\n",
      "[172/261] openai/gpt-4o-mini: Score = 4\n",
      "[172/261] openai/gpt-4o-mini: Score = 4\n",
      "[173/261] openai/gpt-4o: Score = 10\n",
      "[173/261] openai/gpt-4o: Score = 10\n",
      "[174/261] openai/gpt-4o-mini-2024-07-18: Score = 4\n",
      "[175/261] openai/gpt-4o-2024-08-06: Score = 13\n",
      "[174/261] openai/gpt-4o-mini-2024-07-18: Score = 4\n",
      "[175/261] openai/gpt-4o-2024-08-06: Score = 13\n",
      "[176/261] openai/gpt-4o-mini-search-preview-2025-03-11: Score = 15\n",
      "[176/261] openai/gpt-4o-mini-search-preview-2025-03-11: Score = 15\n",
      "[177/261] openai/gpt-4o-mini-search-preview: Score = 15\n",
      "[177/261] openai/gpt-4o-mini-search-preview: Score = 15\n",
      "[178/261] openai/gpt-5-codex: Score = -400\n",
      "[178/261] openai/gpt-5-codex: Score = -400\n",
      "[179/261] openai/gpt-4o-search-preview: Score = 15\n",
      "[179/261] openai/gpt-4o-search-preview: Score = 15\n",
      "[180/261] openai/gpt-4o-search-preview-2025-03-11: Score = 15\n",
      "[180/261] openai/gpt-4o-search-preview-2025-03-11: Score = 15\n",
      "[181/261] openai/gpt-5: Score = 9\n",
      "[181/261] openai/gpt-5: Score = 9\n",
      "[182/261] openai/gpt-5-mini-2025-08-07: Score = 6\n",
      "[182/261] openai/gpt-5-mini-2025-08-07: Score = 6\n",
      "[183/261] openai/gpt-5-pro: Score = -400\n",
      "[183/261] openai/gpt-5-pro: Score = -400\n",
      "[184/261] openai/gpt-5-pro-2025-10-06: Score = -400\n",
      "[184/261] openai/gpt-5-pro-2025-10-06: Score = -400\n",
      "[185/261] openai/gpt-5-nano: Score = 6\n",
      "[185/261] openai/gpt-5-nano: Score = 6\n",
      "[186/261] openai/gpt-5-2025-08-07: Score = 11\n",
      "[186/261] openai/gpt-5-2025-08-07: Score = 11\n",
      "[187/261] openai/gpt-5-nano-2025-08-07: Score = 10\n",
      "[187/261] openai/gpt-5-nano-2025-08-07: Score = 10\n",
      "[188/261] openai/o1-2024-12-17: Score = 6\n",
      "[188/261] openai/o1-2024-12-17: Score = 6\n",
      "[189/261] openai/o1-pro: Score = -400\n",
      "[189/261] openai/o1-pro: Score = -400\n",
      "[190/261] openai/o1: Score = 6\n",
      "[190/261] openai/o1: Score = 6\n",
      "[191/261] openai/o1-mini: Score = 10\n",
      "[191/261] openai/o1-mini: Score = 10\n",
      "[192/261] deepseek/deepseek-v3.2-exp: Score = 11\n",
      "[192/261] deepseek/deepseek-v3.2-exp: Score = 11\n",
      "[193/261] openai/o1-mini-2024-09-12: Score = 11\n",
      "[193/261] openai/o1-mini-2024-09-12: Score = 11\n",
      "[194/261] perplexity/sonar: Score = 3\n",
      "[194/261] perplexity/sonar: Score = 3\n",
      "[195/261] openai/gpt-5-mini: Score = 13\n",
      "[195/261] openai/gpt-5-mini: Score = 13\n",
      "[196/261] perplexity/sonar-pro: Score = 1\n",
      "[196/261] perplexity/sonar-pro: Score = 1\n",
      "[197/261] qwen/qwen-2.5-72b-instruct: Score = 3\n",
      "[197/261] qwen/qwen-2.5-72b-instruct: Score = 3\n",
      "[198/261] openai/o4-mini: Score = 11\n",
      "[198/261] openai/o4-mini: Score = 11\n",
      "[199/261] openai/o4-mini-2025-04-16: Score = 12\n",
      "[199/261] openai/o4-mini-2025-04-16: Score = 12\n",
      "[200/261] minimax/minimax-m1: Score = 5\n",
      "[200/261] minimax/minimax-m1: Score = 5\n",
      "[201/261] qwen/qwen-2.5-vl-7b-instruct: Score = 4\n",
      "[201/261] qwen/qwen-2.5-vl-7b-instruct: Score = 4\n",
      "[202/261] qwen/qwen-max: Score = 3\n",
      "[202/261] qwen/qwen-max: Score = 3\n",
      "[203/261] perplexity/sonar-reasoning-pro: Score = 2\n",
      "[203/261] perplexity/sonar-reasoning-pro: Score = 2\n",
      "[204/261] qwen/qwen-plus-2025-07-28: Score = 15\n",
      "[204/261] qwen/qwen-plus-2025-07-28: Score = 15\n",
      "[205/261] qwen/qwen-2.5-7b-instruct: Score = 7\n",
      "[205/261] qwen/qwen-2.5-7b-instruct: Score = 7\n",
      "[206/261] qwen/qwen-vl-max: Score = 4\n",
      "[206/261] qwen/qwen-vl-max: Score = 4\n",
      "[207/261] qwen/qwen2.5-coder-7b-instruct: Score = 0\n",
      "[207/261] qwen/qwen2.5-coder-7b-instruct: Score = 0\n",
      "[208/261] qwen/qwen2.5-vl-32b-instruct: Score = 8\n",
      "[208/261] qwen/qwen2.5-vl-32b-instruct: Score = 8\n",
      "[209/261] qwen/qwen2.5-vl-72b-instruct: Score = 3\n",
      "[209/261] qwen/qwen2.5-vl-72b-instruct: Score = 3\n",
      "[210/261] qwen/qwen-turbo: Score = 11\n",
      "[210/261] qwen/qwen-turbo: Score = 11\n",
      "[211/261] qwen/qwen3-235b-a22b: Score = 0\n",
      "[211/261] qwen/qwen3-235b-a22b: Score = 0\n",
      "[212/261] qwen/qwen-plus: Score = 15\n",
      "[212/261] qwen/qwen-plus: Score = 15\n",
      "[213/261] qwen/qwen-2.5-coder-32b-instruct: Score = 6\n",
      "[213/261] qwen/qwen-2.5-coder-32b-instruct: Score = 6\n",
      "[214/261] perplexity/sonar-reasoning: Score = 12\n",
      "[214/261] perplexity/sonar-reasoning: Score = 12\n",
      "[215/261] qwen/qwen3-235b-a22b-2507: Score = 15\n",
      "[215/261] qwen/qwen3-235b-a22b-2507: Score = 15\n",
      "[216/261] qwen/qwen3-30b-a3b: Score = 3\n",
      "[216/261] qwen/qwen3-30b-a3b: Score = 3\n",
      "[217/261] qwen/qwen3-14b: Score = 5\n",
      "[217/261] qwen/qwen3-14b: Score = 5\n",
      "[218/261] qwen/qwen3-30b-a3b-instruct-2507: Score = 15\n",
      "[218/261] qwen/qwen3-30b-a3b-instruct-2507: Score = 15\n",
      "[219/261] qwen/qwen3-32b: Score = 4\n",
      "[219/261] qwen/qwen3-32b: Score = 4\n",
      "[220/261] qwen/qwen3-coder: Score = 15\n",
      "[220/261] qwen/qwen3-coder: Score = 15\n",
      "[221/261] qwen/qwen3-30b-a3b-thinking-2507: Score = 5\n",
      "[221/261] qwen/qwen3-30b-a3b-thinking-2507: Score = 5\n",
      "[222/261] qwen/qwen3-coder-30b-a3b-instruct: Score = 15\n",
      "[222/261] qwen/qwen3-coder-30b-a3b-instruct: Score = 15\n",
      "[223/261] qwen/qwen3-coder-flash: Score = 15\n",
      "[223/261] qwen/qwen3-coder-flash: Score = 15\n",
      "[224/261] qwen/qwen3-8b: Score = 3\n",
      "[224/261] qwen/qwen3-8b: Score = 3\n",
      "[225/261] qwen/qwen3-235b-a22b-thinking-2507: Score = 6\n",
      "[225/261] qwen/qwen3-235b-a22b-thinking-2507: Score = 6\n",
      "[226/261] perplexity/sonar-deep-research: Score = 12\n",
      "[226/261] perplexity/sonar-deep-research: Score = 12\n",
      "[227/261] qwen/qwen3-next-80b-a3b-instruct: Score = 4\n",
      "[227/261] qwen/qwen3-next-80b-a3b-instruct: Score = 4\n",
      "[228/261] qwen/qwen3-max: Score = 4\n",
      "[228/261] qwen/qwen3-max: Score = 4\n",
      "[229/261] raifle/sorcererlm-8x22b: Score = 6\n",
      "[229/261] raifle/sorcererlm-8x22b: Score = 6\n",
      "[230/261] sao10k/l3-euryale-70b: Score = 5\n",
      "[230/261] sao10k/l3-euryale-70b: Score = 5\n",
      "[231/261] qwen/qwen3-coder-plus: Score = 15\n",
      "[231/261] qwen/qwen3-coder-plus: Score = 15\n",
      "[232/261] sao10k/l3-lunaris-8b: Score = 3\n",
      "[232/261] sao10k/l3-lunaris-8b: Score = 3\n",
      "[233/261] sao10k/l3.3-euryale-70b: Score = 4\n",
      "[234/261] sao10k/l3.1-euryale-70b: Score = 4\n",
      "[233/261] sao10k/l3.3-euryale-70b: Score = 4\n",
      "[234/261] sao10k/l3.1-euryale-70b: Score = 4\n",
      "[235/261] qwen/qwen3-next-80b-a3b-thinking: Score = 8\n",
      "[235/261] qwen/qwen3-next-80b-a3b-thinking: Score = 8\n",
      "[236/261] thedrummer/anubis-70b-v1.1: Score = 9\n",
      "[236/261] thedrummer/anubis-70b-v1.1: Score = 9\n",
      "[237/261] thedrummer/cydonia-24b-v4.1: Score = 2\n",
      "[237/261] thedrummer/cydonia-24b-v4.1: Score = 2\n",
      "[238/261] thedrummer/rocinante-12b: Score = 0\n",
      "[238/261] thedrummer/rocinante-12b: Score = 0\n",
      "[239/261] qwen/qwen3-vl-235b-a22b-instruct: Score = 15\n",
      "[239/261] qwen/qwen3-vl-235b-a22b-instruct: Score = 15\n",
      "[240/261] thedrummer/skyfall-36b-v2: Score = 3\n",
      "[240/261] thedrummer/skyfall-36b-v2: Score = 3\n",
      "[241/261] thedrummer/unslopnemo-12b: Score = 2\n",
      "[241/261] thedrummer/unslopnemo-12b: Score = 2\n",
      "[242/261] undi95/remm-slerp-l2-13b: Score = 2\n",
      "[242/261] undi95/remm-slerp-l2-13b: Score = 2\n",
      "[243/261] thudm/glm-4.1v-9b-thinking: Score = 4\n",
      "[243/261] thudm/glm-4.1v-9b-thinking: Score = 4\n",
      "[244/261] qwen/qwq-32b: Score = 4\n",
      "[244/261] qwen/qwq-32b: Score = 4\n",
      "[245/261] qwen/qwen3-vl-235b-a22b-thinking: Score = 7\n",
      "[245/261] qwen/qwen3-vl-235b-a22b-thinking: Score = 7\n",
      "[246/261] x-ai/grok-3-beta: Score = 8\n",
      "[246/261] x-ai/grok-3-beta: Score = 8\n",
      "[247/261] x-ai/grok-3: Score = 9\n",
      "[247/261] x-ai/grok-3: Score = 9\n"
     ]
    }
   ],
   "source": [
    "# Run the tasks across all models except those containing ':cheap'. This tasks ~45 minutes on a laptop.\n",
    "the_models = [model[0] for model in martian_models_data if ':cheap' not in model[0]]\n",
    "add_model_scores = evaluate_models_progressive(the_models, add_tests)\n",
    "sub_model_scores = evaluate_models_progressive(the_models, sub_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af377696",
   "metadata": {},
   "source": [
    "# Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d238c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    " \n",
    "def plot_model_scores(model_scores, title, score_threshold):\n",
    "    # Summarize number of models by score\n",
    "    print(title)\n",
    "    print(f\"Total number of models: {len(martian_models)}\")\n",
    "    score_counts = Counter([m['score'] for m in model_scores])\n",
    "    print(\"Score summary (number of models by score):\")\n",
    "    for score, count in sorted(score_counts.items()):\n",
    "        print(f\"Score {score}: {count} models\") \n",
    "\n",
    "    # Print total number of models with negative and non-negative scores\n",
    "    num_negative = sum(count for score, count in score_counts.items() if isinstance(score, int) and score < 0)\n",
    "    num_nonneg = sum(count for score, count in score_counts.items() if isinstance(score, int) and score >= 0)\n",
    "    print(f\"\\nTotal models with negative score: {num_negative}\")\n",
    "    print(f\"Total models with non-negative score: {num_nonneg}\")\n",
    "\n",
    "    # Print percentage of models with non-negative scores that scored score_threshold or more\n",
    "    num_good = sum(count for score, count in score_counts.items() if isinstance(score, int) and score >= score_threshold)\n",
    "    if num_nonneg > 0:\n",
    "        percent_good = 100 * num_good / num_nonneg\n",
    "        print(f\"Percentage of non-negative models scoring {score_threshold} or more: {percent_good:.2f}%\")\n",
    "    else:\n",
    "        print(\"No models with non-negative scores.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return score_counts\n",
    "\n",
    "add_score_counts = plot_model_scores(add_model_scores, \"Addition Test Scores\", max_score_possible)\n",
    "sub_score_counts = plot_model_scores(sub_model_scores, \"Subtraction Test Scores\", max_score_possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph for Addition and Subtraction score counts, x-axis always 0 to 15\n",
    "score_range = list(range(max_score_possible+1))\n",
    "add_counts = [add_score_counts.get(s, 0) for s in score_range]\n",
    "sub_counts = [sub_score_counts.get(s, 0) for s in score_range]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars_add = plt.bar([x-0.2 for x in score_range], add_counts, width=0.4, color='skyblue', label='Addition')\n",
    "bars_sub = plt.bar([x+0.2 for x in score_range], sub_counts, width=0.4, color='salmon', label='Subtraction')\n",
    "\n",
    "plt.xlabel('Model Score', fontsize=18)\n",
    "plt.ylabel('Number of Models', fontsize=18)\n",
    "plt.xticks(score_range, [str(s) for s in score_range], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "# Add count labels inside bars only where count > 4, with larger font\n",
    "for bar, count in zip(bars_add, add_counts):\n",
    "    if count > 4:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(count), ha='center', va='center', fontsize=16, color='black')\n",
    "for bar, count in zip(bars_sub, sub_counts):\n",
    "    if count > 4:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(count), ha='center', va='center', fontsize=16, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Output Token Cost vs. Model Score for Addition and Subtraction (x-axis always 0 to 15)\n",
    "import numpy as np\n",
    "model_name_to_output_cost = {model[0]: model[2] for model in martian_models_data}\n",
    "\n",
    "# Addition scores\n",
    "add_model_names = [entry['model'] for entry in add_model_scores]\n",
    "add_scores = [entry['score'] for entry in add_model_scores]\n",
    "add_output_costs = [model_name_to_output_cost.get(name, np.nan) for name in add_model_names]\n",
    "add_filtered = [(score, cost) for score, cost in zip(add_scores, add_output_costs) if isinstance(score, int) and 0 <= score <= max_score_possible]\n",
    "if add_filtered:\n",
    "    add_filtered_scores, add_filtered_output_costs = zip(*add_filtered)\n",
    "else:\n",
    "    add_filtered_scores, add_filtered_output_costs = [], []\n",
    "\n",
    "# Subtraction scores\n",
    "sub_model_names = [entry['model'] for entry in sub_model_scores]\n",
    "sub_scores = [entry['score'] for entry in sub_model_scores]\n",
    "sub_output_costs = [model_name_to_output_cost.get(name, np.nan) for name in sub_model_names]\n",
    "sub_filtered = [(score, cost) for score, cost in zip(sub_scores, sub_output_costs) if isinstance(score, int) and 0 <= score <= max_score_possible]\n",
    "if sub_filtered:\n",
    "    sub_filtered_scores, sub_filtered_output_costs = zip(*sub_filtered)\n",
    "else:\n",
    "    sub_filtered_scores, sub_filtered_output_costs = [], []\n",
    "\n",
    "# Prepare categorical x-axis for scores 0 to max_score_possible\n",
    "score_range = list(range(max_score_possible+1))\n",
    "score_to_x = {score: i for i, score in enumerate(score_range)}\n",
    "add_x_vals = [score_to_x[score] for score in add_filtered_scores]\n",
    "sub_x_vals = [score_to_x[score] for score in sub_filtered_scores]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(add_x_vals, add_filtered_output_costs, alpha=0.6, color='blue', label='Addition')\n",
    "plt.scatter(sub_x_vals, sub_filtered_output_costs, alpha=0.6, color='red', label='Subtraction')\n",
    "plt.xlabel('Model Score', fontsize=18)\n",
    "plt.ylabel('Model Output Token Cost (USD per 1M tokens)', fontsize=16)\n",
    "plt.xticks(score_range, [str(s) for s in score_range], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4675d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only models with a negative score, zero score or score >= 10 for both Addition and Subtraction\n",
    "def list_models_by_score(model_scores, label):\n",
    "    score_to_models = {}\n",
    "    for entry in model_scores:\n",
    "        score = entry['score']\n",
    "        if score <= 0 or score >= 10:\n",
    "            score_to_models.setdefault(score, []).append(entry['model'])\n",
    "    print(f\"\\n{label}:\")\n",
    "    for score in sorted(score_to_models):\n",
    "        print(f\"Models with score {score}:\")\n",
    "        for model in score_to_models[score]:\n",
    "            print(f\"  {model}\")\n",
    "\n",
    "list_models_by_score(add_model_scores, \"Addition Models\")\n",
    "list_models_by_score(sub_model_scores, \"Subtraction Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc0d0a",
   "metadata": {},
   "source": [
    "# Check zero-score model output\n",
    "\n",
    "On occasion, check the output of the \"0 score\" models to see if unusual answer formats are hiding correct answers.\n",
    "If so, is_ground_truth_correct should be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf101ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run models that scored 0 on test[0] and show their outputs\n",
    "def run_zero_score_models(title, model_scores, tests):\n",
    "    zero_score_models = [m['model'] for m in model_scores if m['score'] == 0]\n",
    "    print(f\"Re-running {len(zero_score_models)} models that scored 0 on the first test for {title}...\")\n",
    "    outputs = []\n",
    "    for model_name in zero_score_models:\n",
    "        try:\n",
    "            answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "            success = False\n",
    "        outputs.append({'model': model_name, 'output': answer, 'success': success})\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\nSuccess: {success}\\n{'-'*40}\")\n",
    "\n",
    "\n",
    "#run_zero_score_models(\"Addition\", add_model_scores, add_tests)\n",
    "#run_zero_score_models(\"Subtraction\", sub_model_scores, sub_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800d6bc",
   "metadata": {},
   "source": [
    "# Check -400 score models\n",
    "\n",
    "Models can generate -400 scores if a mandatory parameter is missing. This may hide a model that can do addition.\n",
    "Run the models that give a -400 score and see if the detailed error message shows the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4176071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run models that scored -400 and show their outputs and errors\n",
    "def run_minus_400_score_models(title, model_scores, tests):\n",
    "    minus_400_models = [m['model'] for m in model_scores if m['score'] == -400]\n",
    "    print(f\"Re-running {len(minus_400_models)} models that scored -400 on the first test for {title}...\")\n",
    "    outputs = []\n",
    "    for model_name in minus_400_models:\n",
    "        try:\n",
    "            answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "            success = False\n",
    "        outputs.append({'model': model_name, 'output': answer, 'success': success})\n",
    "        print(f\"Model: {model_name}\\nOutput/Error: {answer}\\nSuccess: {success}\\n{'-'*40}\")\n",
    "\n",
    "#run_minus_400_score_models(\"Addition\", add_model_scores, add_tests)\n",
    "#run_minus_400_score_models(\"Subtraction\", sub_model_scores, sub_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173192cb",
   "metadata": {},
   "source": [
    "# High Variability Example\n",
    "The model \"alfredpros/codellama-7b-instruct-solidity\" has high variability. Sometimes answer is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b587aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_high_variability_model():\n",
    "    model_name = \"alfredpros/codellama-7b-instruct-solidity\"\n",
    "    for i in range(5):\n",
    "        answer, success = run_model_inference(model_name, add_tests[0][0], add_tests[0][1])\n",
    "        print(f\"Success: {success}. Output: {answer}\")\n",
    "\n",
    "#run_high_variability_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb89f",
   "metadata": {},
   "source": [
    "# Ask how the models that score >=14 work\n",
    "\n",
    "Inconclusive: \n",
    "\n",
    "Sometimes the detailed methodology is mathematically incorrect.\n",
    "Sometimes the outline methodology looks like a sentence from a text book.\n",
    "The bext valid explanations say the model does addition like a human - line up digit pairs - then sum right digit-pair to-left digit pair calculating carries \n",
    "The \"concise\" setting makes these model reply immediately - unless they have many hidden \"thinking tokens\" the human approach is not feasible.\n",
    "\n",
    "Some of these models gpt-4o-mini-search-preview and gpt-4o-search-preview explicitly say they have search/tool access. \n",
    "So they can just call a python function to calculate the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_work_for_score_best_models(title, model_scores):\n",
    "    best_models = [m['model'] for m in model_scores if m['score'] >= 14]\n",
    "    ground_prompt = \"998522030597+1477969406=\"\n",
    "    concise_prompt = f\"Answer concisely: {ground_prompt}\"\n",
    "    explain_prompt = f\"Show your work for {ground_prompt}\"\n",
    "    ground_truth = str(998522030597 + 1477969406)\n",
    "\n",
    "    print(f\"Querying {len(best_models)} models that scored best...\")\n",
    "    for model_name in best_models:\n",
    "        try:\n",
    "            answer, _ = run_model_inference(model_name, concise_prompt, ground_truth)\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\n{'-'*40}\")\n",
    "\n",
    "        try:\n",
    "            answer, _ = run_model_inference(model_name, explain_prompt, ground_truth)\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\n{'-'*40}\")\n",
    "\n",
    "#show_work_for_score_best_models(\"Addition\", add_model_scores)\n",
    "#show_work_for_score_best_models(\"Subtraction\", sub_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa154ce1",
   "metadata": {},
   "source": [
    "# Chat GPT 5\n",
    "What did the Chat GPT 5 models score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ce63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores for all models containing 'gpt-5' in both Addition and Subtraction\n",
    "def print_gpt5_scores(model_scores, label):\n",
    "    print(f\"\\n{label}:\")\n",
    "    for entry in model_scores:\n",
    "        model = entry['model']\n",
    "        score = entry['score']\n",
    "        if 'gpt-5' in model:\n",
    "            print(f\"Model: {model}, Score: {score}\")\n",
    "\n",
    "print_gpt5_scores(add_model_scores, \"Addition Scores\")\n",
    "print_gpt5_scores(sub_model_scores, \"Subtraction Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db76374",
   "metadata": {},
   "source": [
    "# Test some subtraction examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# Filter models that scored more than 0 on subtraction\n",
    "eligible_models = [entry for entry in sub_model_scores if entry['score'] > 0]\n",
    "\n",
    "if not eligible_models:\n",
    "    print(\"No models scored more than 0 on subtraction.\")\n",
    "else:\n",
    "    # Pick up to 5 random models\n",
    "    selected_models = random.sample(eligible_models, min(5, len(eligible_models)))\n",
    "\n",
    "    for entry in selected_models:\n",
    "        model = entry['model']\n",
    "        score = entry['score']\n",
    "        print(f\"\\nModel: {model} (Subtraction Score: {score})\")\n",
    "        # Run at the score they succeeded at\n",
    "        if 1 <= score <= max_score_possible:\n",
    "            test_idx = score - 1\n",
    "            test_prompt, test_answer = sub_tests[test_idx]\n",
    "            print(f\"Running at succeeded test (Score {score}): {test_prompt}\")\n",
    "            answer1, success1 = run_model_inference(model, test_prompt, test_answer)\n",
    "            print(test_answer, success1, answer1)\n",
    "            # Run at the next harder test they failed at\n",
    "            if score < max_score_possible:\n",
    "                next_test_idx = score\n",
    "                next_test_prompt, next_test_answer = sub_tests[next_test_idx]\n",
    "                print(f\"Running at next harder test (Score {score+1}): {next_test_prompt}\")\n",
    "                answer2, success2 = run_model_inference(model, next_test_prompt, next_test_answer)\n",
    "                print(next_test_answer, success2, answer2)\n",
    "        else:\n",
    "            print(\"Model did not succeed at any subtraction test in the defined range.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
