{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIIZFUAzydHG"
      },
      "source": [
        "# Quanta Maths - Out of Bounds experiments\n",
        "\n",
        "Using the 6-digit model PhilipQuirke/QuantaMaths_add_d6_l2_h3_t20K_s173289 this CoLab tests:\n",
        "\n",
        "- trained_padding: 6-digit questions in expected format\n",
        "- no_operand_padding: 6-digit questions without zero padding\n",
        "- shorter_with_padding: 5-digit questions in expected format\n",
        "- longer_questions1: 9-digit questions without zero padding\n",
        "- longer_questions2: 9-digit questions with zero padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLx-Fyw0qHoY"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZHYXbdWjtR0"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq2g8AbdqMOK"
      },
      "outputs": [],
      "source": [
        "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kRRcMhXzAO2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GszFqZuqN38"
      },
      "source": [
        "# Load 6 digit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmwKJaKBqWyY"
      },
      "outputs": [],
      "source": [
        "repo_id  = \"PhilipQuirke/QuantaMaths_add_d6_l2_h3_t20K_s173289\"\n",
        "model_pth_fname = \"model.pth\"  # or whatever your file is called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5rfIPQ5m1wl"
      },
      "outputs": [],
      "source": [
        "# Build the config â€” match your training params exactly!\n",
        "cfg = HookedTransformerConfig(\n",
        "    d_model=510,\n",
        "    n_heads=3,\n",
        "    d_head=170,\n",
        "    n_layers=2,\n",
        "    d_mlp=2040,\n",
        "    n_ctx=22,\n",
        "    d_vocab=15,\n",
        "    act_fn=\"gelu\",\n",
        "    normalization_type=\"LN\",\n",
        "    device=\"cpu\",           # or \"cuda\" if using GPU\n",
        ")\n",
        "\n",
        "# Instantiate model from config\n",
        "model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5lnz63ie-I"
      },
      "outputs": [],
      "source": [
        "# Download the file\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=model_pth_fname)\n",
        "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvWrMxStoF0T"
      },
      "outputs": [],
      "source": [
        "char_to_token = {\n",
        "    **{str(i): i for i in range(10)},\n",
        "    '+': 10,\n",
        "    '-': 11,\n",
        "    '=': 12,\n",
        "    '*': 13,\n",
        "    '\\\\': 14,   # division token as a backslash\n",
        "}\n",
        "\n",
        "token_to_char = {v: k for k, v in char_to_token.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpGqGWx3mGXE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def encode_str(s):\n",
        "    tokens = [char_to_token[c] for c in s]\n",
        "    return torch.tensor([tokens], dtype=torch.long)\n",
        "\n",
        "def decode_tokens(t):\n",
        "    return ''.join([token_to_char[int(tok)] for tok in t])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVA8Xbmxyx9F"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-numSEiMsjHd"
      },
      "outputs": [],
      "source": [
        "def pad_left(s, width):\n",
        "    return s.rjust(width, '0')\n",
        "\n",
        "def generate_sample(a_len, b_len, c_len, formatted=True, pad_operands=True):\n",
        "    a = random.randint(0, 10**a_len - 1)\n",
        "    b = random.randint(0, 10**b_len - 1)\n",
        "    c = a + b\n",
        "\n",
        "    a_str = str(a)\n",
        "    b_str = str(b)\n",
        "    c_str = str(c)\n",
        "\n",
        "    if pad_operands:\n",
        "        a_str = pad_left(a_str, a_len)\n",
        "        b_str = pad_left(b_str, b_len)\n",
        "        c_str = pad_left(c_str, c_len)\n",
        "\n",
        "    prompt = f\"{a_str}+{b_str}=\"\n",
        "\n",
        "    if formatted:\n",
        "        max_len = max(len(str(a)), len(str(b)), len(str(c)), c_len)\n",
        "        c_str = pad_left(str(c), max_len)\n",
        "        c_str = \"+\" + c_str\n",
        "    else:\n",
        "        c_str = \"+\" + str(c)\n",
        "\n",
        "    full_str = prompt + c_str\n",
        "    return prompt, c_str, full_str, a, b, c\n",
        "\n",
        "# Generate 10 test samples for each case\n",
        "results = {\n",
        "    \"trained_padding\": [generate_sample(6, 6, 7, formatted=True, pad_operands=True) for _ in range(10)],\n",
        "    \"no_operand_padding\": [generate_sample(4, 3, 7, pad_operands=False) for _ in range(10)],\n",
        "    \"shorter_with_padding\": [generate_sample(5, 5, 7, pad_operands=True) for _ in range(10)],\n",
        "    \"longer_questions1\": [generate_sample(9, 9, 10, pad_operands=False) for _ in range(10)],\n",
        "    \"longer_questions2\": [generate_sample(9, 9, 10, pad_operands=True) for _ in range(10)],\n",
        "}\n",
        "\n",
        "def format_test_df(category, samples):\n",
        "    df = pd.DataFrame(samples, columns=[\"prompt\", \"expected\", \"full_expected\", \"a\", \"b\", \"c\"])\n",
        "    df[\"category\"] = category\n",
        "    return df[[\"category\", \"prompt\", \"expected\", \"a\", \"b\", \"c\"]]\n",
        "\n",
        "df_all = pd.concat([\n",
        "    format_test_df(cat, samples)\n",
        "    for cat, samples in results.items()\n",
        "], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDrXweAJsHKq"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Use this if not already loaded\n",
        "def manual_generate(expr, max_new_tokens=10):\n",
        "    model.eval()\n",
        "    input_ids = encode_str(expr).to(model.cfg.device)\n",
        "    n_ctx = model.cfg.n_ctx\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if input_ids.shape[1] > n_ctx:\n",
        "            input_ids = input_ids[:, -n_ctx:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids)[0, -1]\n",
        "            next_token = logits.argmax(dim=-1, keepdim=True).unsqueeze(0)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    output_ids = input_ids[0]\n",
        "    return decode_tokens(output_ids[len(expr):])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9mQDoppwiS6"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(df):\n",
        "    outputs = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        prompt = row[\"prompt\"]\n",
        "        expected = row[\"expected\"]\n",
        "        a, b, c = row[\"a\"], row[\"b\"], row[\"c\"]\n",
        "\n",
        "        # Set correct number of tokens needed\n",
        "        max_tokens = len(expected)  # max needed characters\n",
        "\n",
        "        try:\n",
        "            pred = manual_generate(prompt, max_new_tokens=max_tokens)\n",
        "        except Exception as e:\n",
        "            pred = f\"<error: {str(e)}>\"\n",
        "\n",
        "        # Check correctness\n",
        "        try:\n",
        "            numeric_correct = (int(pred.lstrip(\"+\")) == c)\n",
        "        except:\n",
        "            numeric_correct = False\n",
        "\n",
        "        format_correct = (pred == expected)\n",
        "\n",
        "        outputs.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"expected\": expected,\n",
        "            \"predicted\": pred,\n",
        "            \"numeric_correct\": numeric_correct,\n",
        "            \"format_correct\": format_correct,\n",
        "            \"category\": row[\"category\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJqGBlOXwl10"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "results_df = evaluate_model(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_X8th58vVvM"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)     # Don't truncate columns\n",
        "pd.set_option('display.expand_frame_repr', False)  # Don't wrap rows to multiple lines\n",
        "pd.set_option('display.max_columns', None)      # Show all columns\n",
        "print()\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OLx-Fyw0qHoY",
        "8GszFqZuqN38"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
