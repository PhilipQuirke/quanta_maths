{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2gZSoSJD5C"
      },
      "source": [
        "# Quanta Maths: Integer Addition and Subtraction in Transformers - Analyze a Model\n",
        "\n",
        "This Colab analyzes the behavior and algorithm feature sub-tasks performed by nodes in Transformer models.\n",
        "\n",
        "The models perform integer addition and/or subtraction e.g. 133357+182243=+0315600 and 123450-345670=-0123230. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens.\n",
        "\n",
        "This Colab follows on from https://github.com/PhilipQuirke/quanta_maths/blob/main/notebooks/QuantaMathsTrain.ipynb which trained the models, and outputs model.pth and training_loss.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkGrSqHJKqN"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd3nmsMJV5T"
      },
      "source": [
        "# Part 0: Import libraries\n",
        "Imports standard libraries.\n",
        "\n",
        "Imports \"QuantaMaths\" public library as \"mmi\". Refer to [README.md](https://github.com/PhilipQuirke/QuantaMaths/blob/main/README.md) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCdmr6-_Jkzi",
        "outputId": "e7c60176-8d71-479c-e9ff-f8268bd5b91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    !pip install matplotlib\n",
        "    !pip install kaleido\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "    def setup_jupyter(install_libraries=False):\n",
        "        if install_libraries:\n",
        "            !pip install matplotlib\n",
        "            !pip install kaleido\n",
        "\n",
        "        print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "        from IPython import get_ipython\n",
        "\n",
        "        ipython = get_ipython()\n",
        "        # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "        ipython.magic(\"load_ext autoreload\")\n",
        "        ipython.magic(\"autoreload 2\")\n",
        "\n",
        "    # setup_jupyter(install_libraries=True)   # Uncomment if you need to install libraries in notebook.\n",
        "    setup_jupyter(install_libraries=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2P3cndolKDM",
        "outputId": "7fcb1ca0-6731-4333-dfa1-d312b369596f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
            "  Cloning https://github.com/PhilipQuirke/quanta_mech_interp.git to /tmp/pip-req-build-8dcr6cp0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PhilipQuirke/quanta_mech_interp.git /tmp/pip-req-build-8dcr6cp0\n",
            "  Resolved https://github.com/PhilipQuirke/quanta_mech_interp.git to commit e17badd114f60ed4695c750e8073337d97fd8bb4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVBZ373B0dYI"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_maths.git\n",
        "import MathsMechInterp as mmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up2QLAZLJnG9"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve-TndERJoaJ"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zOEFryJqGN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6TE7A9SxySA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOh-m1lO4hrO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sklearn # Aka scikit.learn\n",
        "import skopt # Aka scikit.optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8VQ4e0QJsIB"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "from transformer_lens.utils import download_file_from_hf\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGPkaokJQM5"
      },
      "source": [
        "# Part 1A: Configuration\n",
        "\n",
        "Which existing model do we want to analyze?\n",
        "\n",
        "The existing model weightings created by the sister Colab [QuantaMathsTrain](https://github.com/PhilipQuirke/quanta_maths/blob/main/assets/QuantaMathsTrain.ipynb) are loaded from HuggingFace (in Part 5). Refer https://github.com/PhilipQuirke/quanta_maths/blob/main/README.md for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DXZQ2E6yAi"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"main\" configuration class. MathsConfig is derived from the chain AlgoConfig > UsefulConfig > ModelConfig\n",
        "cfg = mmi.MathsConfig()\n",
        "\n",
        "\n",
        "# Which pre-existing model do we want to analyze? Uncomment exactly one line:\n",
        "\n",
        "# Addition models\n",
        "#cfg.set_model_names( \"add_d5_l1_h3_t15K_s372001\" )  # AddAccuracy=Two9s. Inaccurate as only has one layer. Reproduces previous paper model.\n",
        "cfg.set_model_names( \"add_d5_l2_h3_t15K_s372001\" )  # AddAccuracy=Six9s. AvgFinalLoss=1.6e-08\n",
        "#cfg.set_model_names( \"add_d5_l2_h3_t40K_s372001\" )  # AddAccuracy=Six9s. AvgFinalLoss=2e-09. (0/M fail). Fewest nodes\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t15K_s372001\" )  # AddAccuracy=Fives. AvgFinalLoss=1.7e-08. (2/M fail)\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t20K_s173289\" )  # AddAccuracy=Six9s. AvgFinalLoss=1.5e-08. (0/M fail). Fewest nodes\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t20K_s572091\" )  # AddAccuracy=Six9s. AvgFinalLoss=7e-09.  (0/M fail)\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t40K_s372001\" )  # AddAccuracy=Six9s. AvgFinalLoss=2e-09. (0/M fail)\n",
        "#cfg.set_model_names( \"add_d7_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s. AvgFinalLoss=3e-09. (0/M fail)\n",
        "#cfg.set_model_names( \"add_d8_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s. AvgFinalLoss=3e-09. (0/M fail)\n",
        "#cfg.set_model_names( \"add_d9_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s. AvgFinalLoss=3e-09. (0/M fail)\n",
        "#cfg.set_model_names( \"add_d10_l2_h3_t40K_s572091\" ) # AddAccuracy=Six9s. AvgFinalLoss=7e-09. (1/M fail)\n",
        "#cfg.set_model_names( \"add_d10_l2_h3_t40K_gf_s572091\" ) # AddAccuracy=Six9s. AvgFinalLoss=3.5-09. GrokFast. Minor accuracy improvement\n",
        "#cfg.set_model_names( \"add_d11_l2_h3_t50K_s572091\" )  # AddAccuracy=Five9s. AvgFinalLoss=8e-09. (2/M fail)\n",
        "#cfg.set_model_names( \"add_d12_l2_h3_t50K_s572091\" )  # AddAccuracy=Five9s. AvgFinalLoss=5e-09. (3/M fail)\n",
        "#cfg.set_model_names( \"add_d13_l2_h3_t50K_s572091\" )  # AddAccuracy=Six9s. AvgFinalLoss=6.3e-08. (1/M fail)\n",
        "#cfg.set_model_names( \"add_d14_l2_h3_t60K_s572091\" )  # AddAccuracy=Three9S. AvgFinalLoss=5.6e-06. (199/M fail)\n",
        "#cfg.set_model_names( \"add_d15_l2_h3_t80K_s572091\" ) # AddAccuracy=Five9s. AvgFinalLoss=8.6e-08 (10/M fail)\n",
        "#cfg.set_model_names( \"add_d20_l2_h3_t80K_s572091\" ) # AddAccuracy=Poor! AvgFinalLoss=0.20!\n",
        "\n",
        "# Subtraction model\n",
        "#cfg.set_model_names( \"sub_d6_l2_h3_t30K_s372001\" )  # SubAccuracy=Six9s. AvgFinalLoss=5.8e-06\n",
        "#cfg.set_model_names( \"sub_d10_l2_h3_t75K_s173289\" )  # SubAccuracy=Two9s. AvgFinalLoss=0.002002. (6672/M fails)\n",
        "#cfg.set_model_names( \"sub_d10_l2_h3_t75K_gf_s173289\" )  # SubAccuracy=Two9s. GrokFast. AvgFinalLoss=0.001197. (5246/M fails). Minor accuracy improvement\n",
        "\n",
        "# Mixed (addition and subtraction) model\n",
        "#cfg.set_model_names( \"mix_d5_l3_h4_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Six9s. AvgFinalLoss=9e-09. (0/M fails, 0/M fails)\n",
        "#cfg.set_model_names( \"mix_d6_l3_h4_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Six9s. AvgFinalLoss=5e-09. (1/M fail)\n",
        "#cfg.set_model_names( \"mix_d7_l3_h4_t50K_s372001\" )  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=2e-08. (2/M fails, 6/M fails)\n",
        "#cfg.set_model_names( \"mix_d8_l3_h4_t60K_s173289\" )  # Add/SubAccuracy=Six9s/Five9s. AvgFinalLoss=4.7e-08. (0/M fails, 7/M fails)\n",
        "#cfg.set_model_names( \"mix_d9_l3_h4_t60K_s173289\" )  # Add/SubAccuracy=Six9s/Four9s. AvgFinalLoss=3.2e-07. (1/M fails, 33/M fails)\n",
        "#cfg.set_model_names( \"mix_d10_l3_h4_t75K_s173289\" )  # Add/SubAccuracy=Five9s/Two9s. AvgFinalLoss=1.125e-06 (2/M fail, 295/M fail)\n",
        "#cfg.set_model_names( \"mix_d10_l3_h4_t75K_gf_s173289\" )  # Add/SubAccuracy=Six9s/Three9s. GrokFast. AvgFinalLoss=8.85e-07 (1/M fail, 294/M fail). Minor accuracy improvement\n",
        "#cfg.set_model_names( \"mix_d11_l3_h4_t80K_s572091\" )  # Add/SubAccuracy=Six9s/Four9s AvgFinalLoss=3.9e-08 (0/M fail, 13/M fail)\n",
        "#cfg.set_model_names( \"mix_d12_l3_h4_t85K_s572091\" )  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=1.7e-08. (2/M fail, 10/M fail)\n",
        "#cfg.set_model_names( \"mix_d13_l3_h4_t85K_s572091\" )  # Add/SubAccuracy=Three9s/Two9s. AvgFinalLoss=9.5e-06. (399/M fail, 4164/M fail)\n",
        "\n",
        "# Mixed models initialized with addition model. Params fine-tuned during training\n",
        "#cfg.set_model_names( \"ins1_mix_d5_l2_h3_t40K_s572091\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l2_h3_t40K_s572091\" )  # Add/SubAccuracy=Six9s/Five9s. AvgLoss = 2.4e-08 (5/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h3_t40K_s572091\" )  # Add/SubAccuracy=Six9s/Five9s. AvgFinalLoss=1.8e-08. (3/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h3_t80K_s572091\" )  # Add/SubAccuracy=Six9s/Five9s AvgLoss = 1.6e-08 (3/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Six9s. AvgFinalLoss=8e-09. MAIN FOCUS\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t40K_s173289\" )  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=1.4e-08. (3/M fails, 2/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t50K_s572091\" )  # Add/SubAccuracy=Six9s/Five9s. AvgFinalLoss=2.9e-08. (4/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d7_l3_h4_t50K_s572091\" )  # Add/SubAccuracy=Five9s/Six9s. AvgFinalLoss=1.3e-08. (4/M fails, 1/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d8_l3_h4_t70K_s572091\" )  # Add/SubAccuracy=Four9s/Two9s. AvgFinalLoss=7.2e-06. (50/M fails, 1196/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d9_l3_h4_t70K_s572091\" )  # Add/SubAccuracy=TODO. AvgFinalLoss=TODO. (50/M fails, TODO/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d10_l3_h3_t50K_s572091\" )  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss 6.3e-07 (6/M fails, 7/M fails)\n",
        "#cfg.set_model_names( \"ins1_mix_d10_l3_h3_t50K_gf_s572091\" ) # Add/SubAccuracy=Five9s/Two9s. GrokFast. AvgFinalLoss=4.0e-06 (2/M fails, 1196/M fails). Worse accuracy than without GF!\n",
        "#cfg.set_model_names( \"ins1_mix_d11_l3_h4_t75K_s572091\" )  # Add/SubAccuracy=TODO. AvgFinalLoss=TODO. (TODO/M fails, TODO/M fails)\n",
        "\n",
        "# Mixed model initialized with addition model. Reset useful heads every 100 epochs during training\n",
        "#cfg.set_model_names( \"ins2_mix_d6_l4_h4_t40K_s372001\" )  # Add/SubAccuracy=Five9s/Five9s. AvgFinalLoss=1.7e-08. (3/M fails, 8/M fails)\n",
        "\n",
        "# Mixed model initialized with addition model. Reset useful heads & MLPs every 100 epochs during training\n",
        "#cfg.set_model_names( \"ins3_mix_d6_l4_h3_t40K_s372001\" )  # Add/SubAccuracy=Four9s/Two9s. AvgFinalLoss=3.0e-04. (17/M fails, 3120/M fails)\n",
        "\n",
        "# Addition&Subtraction model initialized with addition model. Reset useful heads & MLPs every training epoch\n",
        "#cfg.set_model_names( \"ins4_mix_d6_l4_h3_t40K_s372001\" )  # AvgFinalLoss=4.7e-06\n",
        "\n",
        "# Mixed models initialized with addition model. Insert mode 5\n",
        "#cfg.set_model_names( \"ins5_mix_d6_l3_h4_t30K_s775824\" )  # Add/SubAccuracy=???/??? TODO\n",
        "#cfg.set_model_names( \"ins5_mix_d6_l2_h4_t30K_s775824\" )  # Add/SubAccuracy=???/??? TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_IIpX2H2tNe"
      },
      "source": [
        "# Part 1B: Configuration: Input and Output file names\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0DJkn5l2gq3"
      },
      "outputs": [],
      "source": [
        "base_repo_name = 'PhilipQuirke'\n",
        "model_pth_fname = 'model.pth'\n",
        "model_training_loss_fname = 'training_loss.json'\n",
        "model_behaviors_fname = 'behaviors.json'\n",
        "model_features_fname = 'features.json'\n",
        "\n",
        "cfg.hf_repo = base_repo_name + \"/QuantaMaths_\" + cfg.model_name # \"PhilipQuirke/QuantaMaths_add_d6_l2_h3_t15K_s372001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5BiELcf53ms"
      },
      "outputs": [],
      "source": [
        "cfg.batch_size = 512 # Default analysis batch size\n",
        "if cfg.n_layers >= 3 and cfg.n_heads >= 4:\n",
        "  cfg.batch_size = 256 # Reduce batch size to avoid memory constraint issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfdCBTuqh4zq"
      },
      "outputs": [],
      "source": [
        "# Update \"cfg\" with additional training config (including cfg.insert_*) with information stored in:\n",
        "#      https://huggingface.co/PhilipQuirke/QuantaMaths_ins1_mix_d6_l3_h4_t40K_s372001/training_loss.json\"\n",
        "training_data_json = qmi.download_huggingface_json(cfg.hf_repo, model_training_loss_fname)\n",
        "training_loss_list = qmi.load_training_json(cfg, training_data_json)\n",
        "print('Loaded main model training config / loss from', cfg.hf_repo, model_training_loss_fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI4vY-vSxvxC"
      },
      "outputs": [],
      "source": [
        "def print_config():\n",
        "  print(\"%Add=\", cfg.perc_add, \"%Sub=\", cfg.perc_sub, \"%Mult=\", cfg.perc_mult, \"InsertMode=\", cfg.insert_mode, \"File=\", cfg.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f48_G6wr730Q"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print(\"weight_decay=\", cfg.weight_decay, \"lr=\", cfg.lr, \"batch_size=\", cfg.batch_size)\n",
        "print('Model files will be read from HuggingFace repo:', base_repo_name, model_pth_fname, 'and', model_training_loss_fname)\n",
        "print('Model analysis tags will be saved to Colab temporary files:', model_behaviors_fname, 'and', model_features_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keTceF_3zbVP"
      },
      "source": [
        "# Part 2: Results: Model training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCwcStUps62W"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print( \"Avg loss over last 5 epochs (AvgFinalLoss)\", cfg.avg_final_loss)\n",
        "print( \"Final epoch loss\", cfg.final_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RfHXneJw6n"
      },
      "source": [
        "# Part 3A: Set Up: Vocabulary / Embedding / Unembedding\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeObQk2kzAv7"
      },
      "outputs": [],
      "source": [
        "cfg.initialize_maths_token_positions()\n",
        "mmi.set_maths_vocabulary(cfg)\n",
        "mmi.set_maths_question_meanings(cfg)\n",
        "print(cfg.token_position_meanings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6rUaYvjOcE"
      },
      "source": [
        "# Part 3B: Set Up: Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA16Nb2PJ7MB"
      },
      "outputs": [],
      "source": [
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = cfg.get_HookedTransformerConfig()\n",
        "\n",
        "# Create the main transformer model\n",
        "cfg.main_model = HookedTransformer(ht_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiJhch4KCej"
      },
      "source": [
        "# Part 4: Set Up: Loss Function & Data Generator\n",
        "This maths loss function and data generator are imported from QuantaTools as logits_to_tokens_loss, loss_fn, maths_data_generator_core and maths_data_generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqzljhQ4KJU5"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" maths \"questions\" data generator function. Invoked using next().\n",
        "cfg.set_seed(cfg.analysis_seed)\n",
        "ds = mmi.maths_data_generator( cfg )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtmioT1THbJA"
      },
      "outputs": [],
      "source": [
        "# Generate sample data generator (unit test)\n",
        "print(next(ds)[:3,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KJhCxFtNKfm"
      },
      "source": [
        "# Part 5: Set Up: Load Model from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMkB_8GNRc0"
      },
      "outputs": [],
      "source": [
        "print(\"Loading model from HuggingFace\", cfg.hf_repo, model_pth_fname)\n",
        "\n",
        "cfg.main_model.load_state_dict(download_file_from_hf(repo_name=cfg.hf_repo, file_name=model_pth_fname, force_is_torch=True))\n",
        "cfg.main_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS_zUfwrXl6P"
      },
      "source": [
        "# Part 6: Set Up: Create sample maths questions\n",
        "\n",
        "Create a batch of manually-curated mathematics test questions, and cache some sample model prediction outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6Kt5XD-XU9t"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"ablation intervention\" configuration class\n",
        "acfg = qmi.acfg\n",
        "acfg.reset_ablate()\n",
        "cfg.configure_acfg_singleton()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiK-0P44gvfS"
      },
      "outputs": [],
      "source": [
        "varied_questions = mmi.make_maths_test_questions_and_answers(cfg)\n",
        "num_varied_questions = varied_questions.shape[0]\n",
        "\n",
        "qmi.a_set_ablate_hooks(cfg) # Updates acfg\n",
        "qmi.a_calc_mean_values(cfg, varied_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5clJnASWCPVN"
      },
      "outputs": [],
      "source": [
        "print(\"Num questions:\", num_varied_questions, \"Question length:\", len(varied_questions[0]))\n",
        "print(\"Sample Question:\", varied_questions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FwJW0tv4Nf"
      },
      "source": [
        "# Part 7: Results: Can the model correctly predict sample questions?\n",
        "\n",
        "Ask the model to predict the varied_questions (without intervention) to see if the model gets them all right. Categorize answers by complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56siA_QKe0W"
      },
      "outputs": [],
      "source": [
        "# Test maths question prediction accuracy on the sample questions provided.\n",
        "# Does NOT use UsefulInfo.* information\n",
        "# Used to estimate the accuracy of the model's predictions.\n",
        "# Returns a reduced set of questions - removing questions that the model failed to answer.\n",
        "print_config()\n",
        "\n",
        "acfg.show_test_failures = True\n",
        "varied_questions = mmi.test_maths_questions_by_complexity(cfg, acfg, varied_questions)\n",
        "acfg.show_test_failures = False\n",
        "\n",
        "num_varied_questions = varied_questions.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PjaQvhhayUL"
      },
      "source": [
        "# Part 9 : Results: Is the model 99.9999% accurate?\n",
        "\n",
        "The model's accuracy is 99.9999% (aka \"six 9s\") if it can predict one million questions with 0 or 1 failed predictions. If it has 2 to 10 failed predictions the model's accuracy is called 99.999% (aka \"five 9s\").\n",
        "\n",
        "Note: There may be very rare edge cases (say 1 in ten million) that did not appear in the test questions. So this empirical test can **not** prove 100% accuracy.\n",
        "\n",
        "If the model fails some questions, consider:\n",
        "- Adding a few of the failures into the \"test questions\" into mmi.make_maths_test_questions_and_answers()\n",
        "- Understand the \"use case(s)\" driving these failures\n",
        "- Alter mmi.maths_data_generator_core to enrich the training data with examples if these use case(s)\n",
        "- Retrain the model using the QuantaMathsTrain Colab.  \n",
        "\n",
        "Takes ~25 mins to run for ins_mix_d6_l3_h4_t40K_s372001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeB86OU_8X9l"
      },
      "outputs": [],
      "source": [
        "run_1m_test = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFaarlLVAMTy"
      },
      "source": [
        "Enriching data means adding more \"hard\" and subtractions questions. Enriched data was used during training. Using enrich_data does not much impact the  model accuracy measured here.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bz-kDiwAKNR"
      },
      "outputs": [],
      "source": [
        "enrich_data = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsauYqjaxok"
      },
      "outputs": [],
      "source": [
        "if run_1m_test:\n",
        "    acfg.show_test_failures = False\n",
        "    mmi.test_correctness_on_num_questions(cfg, acfg, num_questions=1000000, enrich_data=enrich_data)\n",
        "    acfg.show_test_failures = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXBYdxj-jLZc"
      },
      "source": [
        "# Part 10: Set Up: Which token positions are used by the model?\n",
        "\n",
        "Ablate all nodes in each (question and answer) token position (by overriding the model memory aka residual stream). If the model's prediction loss increases, the token position is useful to the algorithm. Unused token positions are excluded from further analysis. Used to populate the UsefulInfo.useful_positions data. This is token **position level** information.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGL57MRBLdUh"
      },
      "outputs": [],
      "source": [
        "num_failures_list = []\n",
        "acfg.show_test_failures = False\n",
        "\n",
        "for position in range(cfg.n_ctx):\n",
        "  # Test accuracy of model in predicting question answers. Ablates all nodes at acfg.ablate_position. Does NOT use UsefulInfo.* information.\n",
        "  num_fails = mmi.test_maths_questions_by_impact(cfg, acfg, varied_questions, position, ablate=True)\n",
        "\n",
        "  if num_fails > 0:\n",
        "    # Add position to UsefulInfo.useful_positions\n",
        "    cfg.add_useful_position(position)\n",
        "    num_failures_list += [num_fails]\n",
        "  else:\n",
        "    num_failures_list += \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsGWUbILYin"
      },
      "source": [
        "# Part 11: Results: Which token positions are used by the model?\n",
        "\n",
        "Which token positions are is used in the model's predictions? Unused token positions are excluded from further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpRp5YMmLe1y"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print(\"num_questions=\", num_varied_questions)\n",
        "print(\"useful_positions=\", cfg.useful_positions )\n",
        "print()\n",
        "\n",
        "cfg.calc_position_failures_map(num_failures_list)\n",
        "qmi.save_plt_to_file(cfg=cfg, full_title=\"Failures When Position Ablated\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904WBkTOLg_5"
      },
      "source": [
        "# Part 12A: Set Up: Which nodes are used by the model?\n",
        "\n",
        "Here we ablate each (attention head and MLP neuron) node in each (question and answer) token position see if the model's prediction loss increases. If loss increases then the \"node + token position\" is used by the algorithm. Used to calculate the UsefulInfo.useful_node_location. This is **position+node level** information. Unused nodes are excluded from further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9ot9RMMTAi"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes = qmi.UsefulNodeList()\n",
        "\n",
        "qmi.ablate_mlp_and_add_useful_node_tags(cfg, varied_questions, mmi.test_maths_questions_and_add_useful_node_tags)\n",
        "qmi.ablate_head_and_add_useful_node_tags(cfg, varied_questions, mmi.test_maths_questions_and_add_useful_node_tags)\n",
        "qmi.add_node_attention_tags(cfg, varied_questions)\n",
        "\n",
        "cfg.useful_nodes.sort_nodes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw-Wteh-JBd6"
      },
      "source": [
        "# Part 12B: Results: Which nodes are used by the model?\n",
        "\n",
        "Here are the (attention head and MLP neuron) node in each (question and answer) token position used by the model during predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhEWUt7yJBuh"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmQHiLALp-3"
      },
      "source": [
        " # Part 13: Set up: Show and save Quanta map\n",
        "\n",
        " Using the UsefulNodes and filtering their tags, show a 2D map of the nodes and the tag minor versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emcB0_CpYTaJ"
      },
      "outputs": [],
      "source": [
        "def show_quanta_map( title, major_tag : qmi.QType, minor_tag : str, get_node_details,\n",
        "        image_width_inches : int = -1, image_height_inches : int = -1,\n",
        "        blue_shades : bool = True, cell_num_shades : int = 6,\n",
        "        filters : qmi.FilterNode = None, cell_fontsize : int = 9,\n",
        "        combine_identical_cells : bool = True, show_perc_circles : bool = False ):\n",
        "\n",
        "  test_nodes = cfg.useful_nodes\n",
        "  if filters is not None:\n",
        "    test_nodes = qmi.filter_nodes(test_nodes, filters)\n",
        "\n",
        "  ax1, quanta_results, num_results = qmi.calc_quanta_map(\n",
        "      cfg, blue_shades, cell_num_shades,\n",
        "      test_nodes, major_tag.value, minor_tag, get_node_details,\n",
        "      cell_fontsize, combine_identical_cells, show_perc_circles,\n",
        "      image_width_inches, image_height_inches )\n",
        "\n",
        "  if num_results > 0:\n",
        "    if cfg.graph_file_suffix > \"\":\n",
        "      print(\"Saving quanta map:\", title)\n",
        "      qmi.save_plt_to_file(cfg=cfg, full_title=title)\n",
        "    else:\n",
        "      ax1.set_title(cfg.file_config_prefix + ' ' + title + ' ({} nodes)'.format(len(quanta_results)))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpkyhHRoMOSw"
      },
      "source": [
        "# Part 16A: Results: Show failure percentage map\n",
        "\n",
        "Show the percentage failure rate (incorrect prediction) when individual Attention Heads and MLPs are ablated. Lower percentages correspond to rarer edge cases. The grey space represents nodes that are not used by the model.\n",
        "\n",
        "A cell containing \"< 1\" may add some risk to the accuracy of the overall analysis process. Check to see if this represents a new use case. Improve the test data set to contain more instances of this (new or existing) use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ28dx5YLs0P"
      },
      "outputs": [],
      "source": [
        "show_quanta_map( \"Failure Frequency Behavior Per Node\", qmi.QType.FAIL, \"\", qmi.get_quanta_fail_perc,\n",
        "                cell_num_shades = qmi.FAIL_SHADES, combine_identical_cells = False, show_perc_circles = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifmtnLoTCN5"
      },
      "source": [
        "# Part 16B - Show answer impact behavior map\n",
        "\n",
        "This map shows the answer digit(s) A0 .. An+1 impacted when we ablate each useful node in the  model. Cells containing values like A5..2 are used in multiple prediction steps to calculate multiple answer digits e.g. A2 to A5. Late token\n",
        "positions focus on predicting one answer digit - partially by using results calculated in early token positions.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K9PyCKYTUzX"
      },
      "outputs": [],
      "source": [
        "show_quanta_map( \"Answer Impact Behavior Per Node\", qmi.QType.IMPACT, \"\", qmi.get_quanta_impact,\n",
        "                cell_num_shades = cfg.num_answer_positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avCfaCT1Puhz"
      },
      "source": [
        "# Part 16C: Result: Show attention map\n",
        "\n",
        "This map shows the input tokens each useful attention head attends to at each token position.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTGoEWHgvFH6"
      },
      "outputs": [],
      "source": [
        "# Only maps attention heads, not MLP layers\n",
        "show_quanta_map( \"Attention Behavior Per Head\", qmi.QType.ATTN, \"\", qmi.get_quanta_attention,\n",
        "                # image_height_inches = 8, # image_width_inches = 11,\n",
        "                cell_num_shades = qmi.ATTN_SHADES )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7-99ZxDOrbF"
      },
      "source": [
        "# Part 16C - Show question complexity map\n",
        "\n",
        "This map shows whether each useful node is used\n",
        "to answer the quesstion classes: addition (S), positive-answer subtraction (M) and/or negative-answer subtraction (N) questions. In mixed models, nodes may be used in prediction of two or three questions classes. That is they are polysemantic.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKeybAj3d6QU"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  num_add, num_sub, num_neg, num_triple, num_double, num_single = mmi.get_maths_nodes_operation_coverage(cfg.useful_nodes.nodes)\n",
        "  print( \"# useful nodes:\", len(cfg.useful_nodes.nodes))\n",
        "  print( \"# useful nodes involved in S, M, N operations:\", num_add, num_sub, num_neg )\n",
        "  print( \"# useful nodes involved in 3, 2, 1 operations:\", num_triple, num_double, num_single)\n",
        "  print()\n",
        "\n",
        "  # For each useful cell, show if addition (S), positive-answer subtraction (M) and negative-answer subtraction (N) questions relies on the node.\n",
        "  show_quanta_map( \"Maths Operation Coverage\", qmi.QType.MATH, \"\", mmi.get_maths_operation_complexity,\n",
        "                  blue_shades = False, cell_num_shades = 4, combine_identical_cells = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxsi3TN7S0S"
      },
      "source": [
        "This map shows the simpliest (lowest complexity) addition quanta S0, S1, etc impacted when we ablate each node in an addition or mixed model. To answer S0 questions, only the S0 nodes are used. To answer S1 questions, S0 and S1 nodes are used, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyoErRoCA-pz"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_add > 0:\n",
        "  # For each useful cell, show the minimum addition question complexity that relies on the node, as measured using quanta S0, S1, S2, ...\n",
        "  show_quanta_map( \"Addition Min-Complexity\", qmi.QType.MATH_ADD, mmi.MathsBehavior.ADD_COMPLEXITY_PREFIX.value, mmi.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qmi.MATH_ADD_SHADES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4DTadZo7e9a"
      },
      "source": [
        "This map shows the simpliest (lowest complexity) subtraction quanta M0, M1, etc impacted when we ablate each node in an subtraction or mixed model. To answer M0 questions, only the M0 nodes are used. To answer M1 questions, M0 and M1 nodes are used, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZzbjxUBQGE"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  # For each useful cell, show the minimum \"positive-answer subtraction\" question complexity that relies on the node, as measured using quanta M0, M1, M2, ...\n",
        "  show_quanta_map( \"Positive-answer Subtraction Min-Complexity\", qmi.QType.MATH_SUB, mmi.MathsBehavior.SUB_COMPLEXITY_PREFIX.value, mmi.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qmi.MATH_SUB_SHADES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUaT47ettc0M"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  # For each useful cell, show the minimum \"negative-answer subtraction\" question complexity that relies on the node, as measured using quanta N0, N1, N2, ...\n",
        "  show_quanta_map( \"Negative-answer Subtraction Min-Complexity\", qmi.QType.MATH_NEG, mmi.MathsBehavior.NEG_COMPLEXITY_PREFIX.value, mmi.get_maths_min_complexity,\n",
        "                  blue_shades = False, cell_num_shades = qmi.MATH_SUB_SHADES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbiau9foMp3h"
      },
      "source": [
        "# Part 19A: Detect attention head output clustering w.r.t ST8/9/10 (Manual)\n",
        "\n",
        "Principal Component Analysis (PCA) is a powerful technique that aids in mechanistic interpretability by simplifying complex datasets into principal components that capture the most significant variance within the data.\n",
        "\n",
        "This library uses PCA to help understand the purpose of individual useful nodes. For more background refer https://github.com/PhilipQuirke/QuantaMaths/blob/main/pca.md\n",
        "\n",
        "If an attention head and an answer digit An gives an interpretable response (2 or 3 distinct output clusters) on 3 groups of questions aligned to ST8, ST9 and ST10 definitions, then plot the response and add a PCA tag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlgEAPV1g7W"
      },
      "outputs": [],
      "source": [
        "# Create a cache of sample maths questions based on the T8, T9, T10 categorisation in cfg.tricase_questions_dict\n",
        "mmi.make_maths_tricase_questions(cfg)\n",
        "\n",
        "cfg.useful_nodes.reset_node_tags(qmi.QType.MATH_ADD.value, mmi.MathsBehavior.ADD_PCA_TAG.value)\n",
        "cfg.useful_nodes.reset_node_tags(qmi.QType.MATH_SUB.value, mmi.MathsBehavior.SUB_PCA_TAG.value)\n",
        "cfg.useful_nodes.reset_node_tags(qmi.QType.MATH_NEG.value, mmi.MathsBehavior.NEG_PCA_TAG.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ5fS3XNMs8e"
      },
      "outputs": [],
      "source": [
        "manual_pca = False\n",
        "\n",
        "# Plot all attention heads with the clearest An selected. Data is manually selected\n",
        "if cfg.model_name == \"add_d5_l1_h3_t15K_s372001\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[ 12, 0, 0, 4 ],\n",
        "    [ 12, 0, 2, 3 ],\n",
        "    [ 13, 0, 0, 3 ],\n",
        "    [ 14, 0, 0, 2 ],\n",
        "    [ 15, 0, 0, 1 ]])\n",
        "\n",
        "elif cfg.model_name == \"add_d5_l2_h3_t15K_s372001\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[10, 0, 0, 2 ],\n",
        "    [ 10, 0, 2, 1 ],\n",
        "    [ 12, 0, 0, 3 ],\n",
        "    [ 12, 1, 0, 3 ],\n",
        "    [ 12, 1, 1, 4 ],\n",
        "    [ 12, 1, 2, 4 ],\n",
        "    [ 13, 0, 0, 3 ],\n",
        "    [ 13, 1, 2, 2 ],\n",
        "    [ 14, 0, 0, 2 ],\n",
        "    [ 14, 1, 2, 2 ],\n",
        "    [ 15, 0, 0, 1 ],\n",
        "    [ 15, 1, 1, 1 ]])\n",
        "\n",
        "elif cfg.model_name == \"add_d6_l2_h3_t15K_s372001\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[11, 0, 1, 1 ],\n",
        "    [ 11, 0, 0, 2 ],\n",
        "    [ 12, 0, 0, 3 ],\n",
        "    [ 13, 0, 0, 1 ],\n",
        "    [ 13, 0, 1, 0 ],\n",
        "    [ 14, 0, 0, 4 ],\n",
        "    [ 14, 1, 1, 4 ],\n",
        "    [ 15, 0, 0, 4 ],\n",
        "    [ 15, 1, 1, 4 ],\n",
        "    [ 15, 1, 2, 4 ],\n",
        "    [ 16, 0, 0, 3 ],\n",
        "    [ 16, 1, 1, 3 ],\n",
        "    [ 17, 0, 0, 2 ],\n",
        "    [ 17, 1, 1, 2 ],\n",
        "    [ 18, 0, 0, 1 ]])\n",
        "\n",
        "\n",
        "elif cfg.model_name == \"add_d6_l2_h3_t20K_s173289\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[ 14, 1, 0, 5 ],\n",
        "    [ 14, 1, 1, 4 ],\n",
        "    [ 14, 1, 2, 4 ],\n",
        "    [ 15, 1, 1, 4 ],\n",
        "    [ 15, 1, 2, 4 ],\n",
        "    [ 16, 1, 1, 3 ],\n",
        "    [ 16, 1, 2, 3 ],\n",
        "    [ 17, 1, 1, 2 ],\n",
        "    [ 18, 1, 1, 1 ]])\n",
        "\n",
        "elif cfg.model_name == \"add_d6_l2_h3_t20K_s572091\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[ 10, 0, 0, 3 ],\n",
        "    [ 11, 0, 0, 2 ],\n",
        "    [ 12, 0, 0, 1 ],\n",
        "    [ 14, 0, 0, 4 ],\n",
        "    [ 14, 1, 0, 4 ],\n",
        "    [ 14, 1, 1, 4 ],\n",
        "    [ 14, 1, 2, 3 ],\n",
        "    [ 15, 0, 0, 4 ],\n",
        "    [ 15, 1, 0, 4 ],\n",
        "    [ 15, 1, 1, 4 ],\n",
        "    [ 15, 1, 2, 4 ],\n",
        "    [ 16, 0, 0, 3 ],\n",
        "    [ 16, 1, 0, 3 ],\n",
        "    [ 16, 1, 1, 3 ],\n",
        "    [ 17, 0, 0, 2 ],\n",
        "    [ 17, 1, 1, 2 ],\n",
        "    [ 18, 0, 0, 1 ]])\n",
        "\n",
        "elif cfg.model_name == \"add_d10_l2_h3_t40K_s572091\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[ 20, 0, 0, 1 ],\n",
        "    [ 23, 1, 0, 8 ],\n",
        "    [ 24, 1, 0, 7 ],\n",
        "    [ 22, 1, 0, 8 ],\n",
        "    [ 25, 1, 0, 6 ],\n",
        "    [ 26, 1, 0, 5 ],\n",
        "    [ 27, 1, 0, 4 ],\n",
        "    [ 27, 1, 2, 4 ],\n",
        "    [ 28, 1, 0, 3 ],\n",
        "    [ 29, 1, 0, 2 ],\n",
        "    [ 30, 1, 0, 1 ]])\n",
        "\n",
        "elif cfg.model_name.startswith(\"sub_d6_l2_h3_t30K\"):\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.MINUS,\n",
        "    [[ 9, 0, 1, 3 ],\n",
        "    [ 10, 0, 1, 2 ],\n",
        "    [ 11, 0, 1, 1 ],\n",
        "    [ 13, 0, 1, 4 ],\n",
        "    [ 13, 1, 2, 5 ],\n",
        "    [ 15, 0, 0, 5 ],\n",
        "    [ 15, 1, 1, 0 ],\n",
        "    [ 15, 1, 1, 1 ],\n",
        "    [ 15, 1, 1, 2 ],\n",
        "    [ 15, 1, 1, 3 ],\n",
        "    [ 15, 1, 1, 4 ],\n",
        "    [ 15, 1, 2, 0 ],\n",
        "    [ 15, 1, 2, 1 ],\n",
        "    [ 15, 1, 2, 2 ],\n",
        "    [ 15, 1, 2, 3 ],\n",
        "    [ 16, 0, 0, 0 ],\n",
        "    [ 16, 0, 0, 1 ],\n",
        "    [ 16, 0, 0, 2 ],\n",
        "    [ 16, 0, 0, 3 ],\n",
        "    [ 16, 0, 0, 4 ],\n",
        "    [ 16, 0, 0, 5 ],\n",
        "    [ 16, 1, 0, 4 ],\n",
        "    [ 16, 1, 1, 0 ],\n",
        "    [ 16, 1, 1, 1 ],\n",
        "    [ 16, 1, 1, 2 ],\n",
        "    [ 16, 1, 1, 3 ],\n",
        "    [ 16, 1, 1, 4 ],\n",
        "    [ 16, 1, 2, 0 ],\n",
        "    [ 16, 1, 2, 1 ],\n",
        "    [ 16, 1, 2, 2 ],\n",
        "    [ 16, 1, 2, 3 ],\n",
        "    [ 16, 1, 2, 4 ],\n",
        "    [ 16, 1, 2, 5 ],\n",
        "    [ 17, 0, 0, 0 ],\n",
        "    [ 17, 0, 0, 1 ],\n",
        "    [ 17, 0, 0, 2 ],\n",
        "    [ 17, 0, 0, 3 ],\n",
        "    [ 17, 1, 0, 3 ],\n",
        "    [ 17, 1, 0, 4 ],\n",
        "    [ 17, 1, 2, 4 ],\n",
        "    [ 18, 0, 0, 0 ],\n",
        "    [ 18, 0, 0, 1 ],\n",
        "    [ 18, 0, 0, 2 ],\n",
        "    [ 18, 1, 0, 2 ],\n",
        "    [ 18, 1, 2, 3 ],\n",
        "    [ 19, 0, 0, 0 ],\n",
        "    [ 19, 1, 2, 2 ],\n",
        "    [ 20, 0, 0, 0 ]])\n",
        "\n",
        "elif cfg.model_name == \"mix_d6_l3_h4_t40K_s372001\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[ 8, 1, 0, 4 ],\n",
        "    [ 11, 1, 0, 1 ],\n",
        "    [ 12, 1, 0, 0 ],\n",
        "    [ 13, 1, 1, 1 ],\n",
        "    [ 14, 2, 1, 5 ],\n",
        "    [ 15, 2, 1, 4 ],\n",
        "    [ 16, 2, 1, 3 ],\n",
        "    [ 17, 2, 1, 2 ],\n",
        "    [ 18, 1, 0, 4 ],\n",
        "    [ 18, 2, 1, 1 ]])\n",
        "\n",
        "elif cfg.model_name == \"ins1_mix_d6_l3_h4_t40K_s372001\":\n",
        "  manual_pca = True\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "    [[10, 0, 0, 2 ],\n",
        "    [ 10, 0, 1, 2 ],\n",
        "    [ 11, 0, 0, 2 ],\n",
        "    [ 11, 0, 1, 1 ],\n",
        "    [ 13, 0, 1, 0 ],\n",
        "    [ 13, 1, 3, 1 ],\n",
        "    [ 14, 1, 2, 0 ],\n",
        "    [ 14, 1, 2, 2 ],\n",
        "    [ 14, 1, 3, 4 ],\n",
        "    [ 15, 0, 3, 5 ],\n",
        "    [ 15, 1, 2, 2 ],\n",
        "    [ 15, 1, 3, 4 ],\n",
        "    [ 16, 0, 3, 4 ],\n",
        "    [ 16, 1, 2, 0 ],\n",
        "    [ 16, 1, 2, 1 ],\n",
        "    [ 16, 1, 2, 2 ],\n",
        "    [ 16, 1, 3, 2 ],\n",
        "    [ 17, 0, 3, 3 ],\n",
        "    [ 17, 1, 2, 2 ],\n",
        "    [ 17, 1, 3, 2 ],\n",
        "    [ 18, 0, 3, 2 ],\n",
        "    [ 18, 1, 3, 1 ],\n",
        "    [ 19, 0, 3, 1 ],\n",
        "    [ 19, 2, 0, 0 ],\n",
        "    [ 19, 2, 1, 0 ],\n",
        "    [ 20, 0, 0, 0 ],\n",
        "    [ 20, 0, 3, 0 ]])\n",
        "\n",
        "  mmi.manual_nodes_pca(cfg, mmi.MathsToken.MINUS,\n",
        "    [[10, 0, 0, 2 ],\n",
        "    [ 10, 0, 1, 2 ],\n",
        "    [ 11, 0, 0, 2 ],\n",
        "    [ 11, 0, 1, 1 ],\n",
        "    [ 13, 0, 1, 0 ],\n",
        "    [ 14, 0, 0, 4 ],\n",
        "    [ 14, 0, 2, 5 ],\n",
        "    [ 14, 1, 2, 0 ],\n",
        "    [ 14, 1, 2, 2 ],\n",
        "    [ 14, 1, 3, 4 ]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu3Pr9CMx3l"
      },
      "source": [
        "# Part 19B: Setup: Detect attention head output clustering w.r.t ST8/9/10 (Auto)\n",
        "\n",
        "Automatic detection of attention heads that have output clustered into 2 or 3 clusters aligned to ST8, ST9 and ST10 categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-8LlNMiqxyW"
      },
      "outputs": [],
      "source": [
        "# PCA explained variance [0] percent threshold\n",
        "evr_perc_threshold = 30\n",
        "# Silhouette Score threshold. Range is 0 to 100\n",
        "silhouette_threshold = 20\n",
        "# Calinski-Harabasz Score (aka Variance Ratio Criterion) threshold. Ratio of the sum of between-clusters dispersion and of within-cluster dispersion. Higher values indicate better-defined clusters.\n",
        "calinski_harabasz_threshold = 50\n",
        "# Label Agreement Score threshold. Custom metric to measure how well the clustering aligns with the three question types. It ranges from 0 to 100.\n",
        "label_agreement_threshold = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UADsW9c1g7X"
      },
      "outputs": [],
      "source": [
        "def auto_node_pca(ax, index, node_location, operation, answer_digit):\n",
        "\n",
        "    base_title, error_message = mmi._build_title_and_error_message(\n",
        "        cfg=cfg, node_location=node_location, operation=operation, answer_digit=answer_digit\n",
        "    )\n",
        "\n",
        "    if (answer_digit, operation) in cfg.tricase_questions_dict:\n",
        "        test_inputs = cfg.tricase_questions_dict[(answer_digit, operation)]\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    # Full_title is \"P10L0H1 A3 78/43/62/42\" = NodeLocation AnswerDigit EVR[0]/MaxSilhouetteScore/MaxCalinskiHarabaszScore/MaxLabelAgreementScore\n",
        "    pca, pca_attn_outputs, full_title, cluster_results = qmi.calc_pca_for_an(\n",
        "        cfg=cfg, node_location=node_location, title=base_title, error_message=error_message, test_inputs=test_inputs\n",
        "    )\n",
        "\n",
        "    if pca is not None:\n",
        "        evr_perc = qmi.pca_evr_0_percent(pca)\n",
        "        if evr_perc > evr_perc_threshold:\n",
        "\n",
        "            silhouette_scores = cluster_results['silhouette_scores']\n",
        "            calinski_harabasz_scores = cluster_results['calinski_harabasz_scores']\n",
        "            label_agreement_scores = cluster_results['label_agreement_scores']\n",
        "\n",
        "            silhouette_score = max( silhouette_scores['2_clusters'], silhouette_scores['3_clusters'] )\n",
        "            calinski_harabasz_score = max( calinski_harabasz_scores['2_clusters'], calinski_harabasz_scores['3_clusters'])\n",
        "            label_agreement_score = max( label_agreement_scores['2_clusters'], label_agreement_scores['3_clusters'])\n",
        "\n",
        "            if silhouette_score >= silhouette_threshold and calinski_harabasz_score >= calinski_harabasz_threshold and label_agreement_score > label_agreement_threshold:\n",
        "                mmi.plot_pca_for_an(ax, pca_attn_outputs, full_title)\n",
        "\n",
        "                major_tag = qmi.QType.MATH_ADD if operation == mmi.MathsToken.PLUS else qmi.QType.MATH_SUB # Does not handle NEG case\n",
        "                cfg.add_useful_node_tag( node_location, major_tag.value, mmi.pca_op_tag(answer_digit, operation) )\n",
        "                return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDZ4--e11g7X"
      },
      "outputs": [],
      "source": [
        "def auto_find_pca(operation):\n",
        "    print(\"Automatic PCA tags for\", cfg.model_name, \"with operation \", qmi.token_to_char(cfg, operation))\n",
        "    title = cfg.model_name + \"_PCA_\" + qmi.token_to_char(cfg, operation)\n",
        "\n",
        "    n_cols, n_rows, fig, axs = mmi.plot_nodes_pca_start_core(4, 5)\n",
        "\n",
        "    index = 0\n",
        "    for node in cfg.useful_nodes.nodes:\n",
        "        if node.is_head:\n",
        "          for answer_digit in range(cfg.n_digits+1):\n",
        "            ax = axs[index // n_cols, index % n_cols]\n",
        "            if auto_node_pca(ax, index, node, operation, answer_digit):\n",
        "              index += 1\n",
        "\n",
        "              if index == n_cols * n_rows:\n",
        "                  mmi.plot_nodes_pca_end(n_cols, n_rows, axs, cfg, title, index)\n",
        "                  n_cols, n_rows, fig, axs = mmi.plot_nodes_pca_start_core(4, 5)\n",
        "                  index = 0\n",
        "\n",
        "    if index > 0:\n",
        "        mmi.plot_nodes_pca_end(n_cols, n_rows, axs, cfg, title, index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVsa9QuQ27Tz"
      },
      "source": [
        "# Part 19B: Results: Detect attention head output clustering w.r.t ST8/9/10 (Auto)\n",
        "\n",
        "Automatic detection of attention heads that have output clustered into 2 or 3 clusters aligned to ST8, ST9 and ST10 categories.\n",
        "Output includes sklearn warnings and many include plots that are not 2 or 3 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2Mwk8Ip-pB"
      },
      "outputs": [],
      "source": [
        "if not manual_pca:\n",
        "    if cfg.perc_add > 0:\n",
        "        auto_find_pca(mmi.MathsToken.PLUS)\n",
        "    if cfg.perc_sub > 0:\n",
        "        auto_find_pca(mmi.MathsToken.MINUS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFcCpfmKwlAH"
      },
      "source": [
        "# Part 20: Results: Show useful nodes and behaviour tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzIaqmtwmkH"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.sort_nodes()\n",
        "cfg.useful_nodes.print_node_tags()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVkOJRmPvPms"
      },
      "source": [
        "# Part 21: Results: Save useful nodes and behaviour tags to json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naD2eCZevOsi"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list to a temporary CoLab file in JSON format\n",
        "print( \"Saving useful node list with behavior tags:\", model_behaviors_fname)\n",
        "cfg.useful_nodes.save_nodes(model_behaviors_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXc9bD2q0zC"
      },
      "source": [
        "# Part 22 : Results: Search for model algorithm tasks\n",
        "\n",
        "Here we find which model nodes perform which specific algorithm task.\n",
        "- **Automatic searches** for node purposes are preferred, as they applicable to several models, and survive (non-significant, node-reordering) differences between models caused by differences in training.\n",
        "- **Manually written tests** of node purposes, specific to a single model instance are also supported.\n",
        "\n",
        "The qmi.search_and_tag searches for a task on useful nodes by:\n",
        "- **filtering** useful nodes, based on \"tag\" pre-requisites, to find the few nodes worth doing investigating. For more detail refer https://github.com/PhilipQuirke/QuantaMaths/blob/main/filter.md\n",
        "- **intervention ablation** testing on the interesting nodes:\n",
        "  - The first \"store\" question is run without hooks\n",
        "  - The second \"clean\" question is run with hooks interjecting some data from the \"store\" run. This run gives, not a \"clean\" answer, but an \"intervened\" answer, which mixes the \"store\" answer and the \"clean\" answer. Our beliefs about the nodes algorthmic purpose are baked into the store question, clean question and intervened answer.\n",
        "- An **algorithm tag** is added to all interesting nodes that pass the intervention ablation test(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyVYckFtV-RV"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.reset_node_tags(qmi.QType.ALGO.value)\n",
        "acfg.show_test_failures = False\n",
        "acfg.show_test_successes = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgg43Sqe8oF1"
      },
      "source": [
        "## Part 22B: Automated An.SS search\n",
        "\n",
        " Search for addition \"Use Sum 9\" (SS) tasks e.g. 34633+55555=+090188 where D4 and D'4 sum to 9 (4+5), and D3 + D'3 > 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUYf20mk8zls"
      },
      "outputs": [],
      "source": [
        "qmi.search_and_tag( cfg, acfg, mmi.add_ss_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5DMV3I_25ST"
      },
      "source": [
        "## Part 22C: Automated An.SC search\n",
        "\n",
        "Search for addition \"Make Carry 1\" (SC) tasks e.g. 222222+666966=+0889188 where D2 + D'2 > 10.\n",
        "\n",
        "(Sometimes model chooses to use ST **instead** of SC. Sometimes model chooses to use ST **and** SC. For A1, model can **accurately** use just SC. For A0,  SC and ST are not needed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvS1eOReZUq1"
      },
      "outputs": [],
      "source": [
        "#acfg.show_test_failures = True\n",
        "qmi.search_and_tag( cfg, acfg, mmi.add_sc_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iosx5zE_macF"
      },
      "source": [
        "## Part 22D: Automated An.SA search\n",
        "\n",
        "Search for addition \"Simple Add\" (SA) tasks e.g. 555555+111111=+0666666 where D3 + D'3 < 10\n",
        "\n",
        "The SA tasks is sometimes split/shared over 2 attention heads in the same position and layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIktdaRL-BfP"
      },
      "outputs": [],
      "source": [
        "#acfg.show_test_failures = True\n",
        "qmi.search_and_tag( cfg, acfg, mmi.add_sa_functions,\n",
        "                  do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThzFD1FxBXg8"
      },
      "source": [
        "## Part 22E: Automated An.ST search\n",
        "\n",
        "Search for A0.ST to A5.ST with impact \"A65432\" to \"A65\" in early tokens.\n",
        "\n",
        "A0 and A1 are simple to calculate and so do NOT use An.ST or An.STm values. So A0 and A1 are excluded from the answer impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFJV4pfWSwHQ"
      },
      "outputs": [],
      "source": [
        "qmi.search_and_tag( cfg, acfg, mmi.add_st_functions,\n",
        "                  do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jOBIUDzRrGz"
      },
      "source": [
        "## Part 22F: Automated An.MD search\n",
        "\n",
        "Search for positive-answer subtraction \"Difference\" (MD) tasks e.g. 666666-222222=+0444444 where D3 >= D'3\n",
        "\n",
        "The MD task may be split/shared over 2 attention heads in the same position at the same layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKag6sUnVS2N"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.sub_md_functions,\n",
        "                    do_pair_search = True, allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyItdhmhIn8"
      },
      "source": [
        "## Part 22G: Automated An.MB search\n",
        "\n",
        "Search for positive-answer subtraction \"Borrow One\" (MB) tasks e.g. 222222-111311=+0110911 where D2 > D'2\n",
        "\n",
        "(Sometimes model chooses to use MT **instead** of MB. Sometimes model chooses to use MT **and** MB. For A1, model can **accurately** use just MB. For A0,  MB and MT are not needed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzjlFwEui7K9"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.sub_mb_functions,\n",
        "                    allow_impact_mismatch = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJo37Qg2ZQpJ"
      },
      "source": [
        "## Part 22H: Automated An.MT search\n",
        "\n",
        "For accuracy, the addition algorithm calculates cascading \"carry one\" in early tokens using the An.ST sub-task. Paralleling this, the subtraction algorithm calculates cascading \"borrow one\" in early tokens using the An.MT sub-task.\n",
        "\n",
        "This section locates An.MT sub-tasks.\n",
        "\n",
        "Define An.MT = +1 if Dn > D'n else 0 if Dn == D'n else -1  \n",
        "The cascading \"borrow one\" calculation is then:\n",
        "A3.MV = fn(A3.MT, fn(A2.MT, fn(A1.MT, A0.MT)))\n",
        "where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "and the output \"-1\" means a cascading borrow one.\n",
        "\n",
        "The above fn could be simplified, but the (below) GT sub-task often relies on the above definition. The tricase An.MT definition also mirrors the addition An.ST definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOJ4J214g3La"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "    #acfg.show_test_failures = True\n",
        "    #acfg.show_test_successes = True\n",
        "    qmi.search_and_tag( cfg, acfg, mmi.sub_mt_functions,\n",
        "                      do_pair_search = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agw7np7gZmgP"
      },
      "source": [
        "## Part 22I: Automated OPR search\n",
        "\n",
        "For mixed models that do addition and subtraction the operation token \"+/-\" (in the middle of the question) is key. Find nodes that attend to the question operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0s2ZTMLimIR"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0 and cfg.perc_add > 0 :\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.opr_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztfj5n8FjYNt"
      },
      "source": [
        "## Part 22J: Automated SGN search\n",
        "\n",
        "For mixed models that do addition and subtraction, and for our subtraction models, the answer sign token \"+/-\" (at the start of the answer) is important. Find nodes that attend to the answer sign token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8smeZ3vBkIQ3"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.sgn_functions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOVsZIpoQN_r"
      },
      "source": [
        "## Part 22K: Automated An.ND search\n",
        "\n",
        "Search for negative-answer subtraction Difference (ND) tasks e.g. 033333-111111=-077778 where D < D'\n",
        "\n",
        "The ND task may be split/shared over 2 attention heads in the same position at the same layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmBOMTeqRJXf"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.neg_nd_functions,\n",
        "                    do_pair_search = True, allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF4AjvfG2QDy"
      },
      "source": [
        "## Part 22L: Automated An.NB search\n",
        "\n",
        "Search for negative-answer subtraction Borrow One (NB) tasks e.g. 033333-111411=-078078 where D < D' and D2 < D'2\n",
        "\n",
        "(Sometimes model chooses to use NT **instead** of NB. Sometimes model chooses to use NT **and** NB.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvtV6ZM48O4"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "  qmi.search_and_tag( cfg, acfg, mmi.neg_nb_functions,\n",
        "                    allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdWYQKi1Sr3M"
      },
      "source": [
        "## Part 22M: Automated An.GT search\n",
        "\n",
        "Both SUB (e.g. 00600-00201=+000399) and NEG (00100-00201=-000101) questions rely on knowing whether D > D'. How is this calculated?\n",
        "\n",
        "Approach 1: Model has specific GT nodes:\n",
        "Define An.GT = +1 if Dn > D'n else 0 if Dn = D'n else -1  \n",
        "When n_digits = 4, D > D' = f(A3.GT, fn(A2.GT, fn(A1.GT, A0.GT)))\n",
        "Where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "\n",
        "Approach 2: Model leverages the existing MT nodes:\n",
        "When n_digits = 4, D > D' = f(A3.MT, fn(A2.MT, fn(A1.MT, A0.MT)))\n",
        "where f(A,B) = +1 if A=1 or (A == 0 and B <> -1) else -1\n",
        "\n",
        "Usually, one node performs both say A3.MT and A3.GT sub-tasks, but in some models the A3.MT and A3.GT functions are performed by distinct nodes. Hence we test for the MT and GT behavior separately.\n",
        "\n",
        "Both approaches mirrors the calculation style used in ADD to calculate Amax as 1 or 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5f6Y2N-Vrry"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_sub > 0:\n",
        "    #acfg.show_test_successes = False\n",
        "    qmi.search_and_tag(cfg, acfg, mmi.sub_gt_functions,\n",
        "                      allow_impact_mismatch = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQLfkdlbVAl_"
      },
      "source": [
        "# Part 23: Show algorithm quanta map\n",
        "\n",
        "This map shows a compacted view of all useful token positions (horizontally) and all useful attention heads and MLP layers\n",
        "(vertically) used in predictions as blue cells. In each cell, the algorithm sub-task(s) Base Add SA, Make Carry SC, TriCase ST, etc found by automated search with ablation testing are shown.\n",
        "\n",
        "Sometimes a subtask is logIcally shared across two attention heads. The SA, MD and ND subtasks sometimes do this.\n",
        "\n",
        "This map plots the \"algorithm\" tags generated in previous steps as a quanta map. This is an automatically generated partial explanation of the model algorithm.\n",
        "\n",
        "Nodes with multiple tags were tagged (found) by more than one of the above subtask searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4bvm3yzQ8Le"
      },
      "outputs": [],
      "source": [
        "# If a cell only has an OPR tag or only has a SGN tag then we do not understand its purpose.\n",
        "# The tag is just an \"attention\" fact. Remove these tags from the algorithm map\n",
        "# (A cell that has both OPR and SGN tags, we believe it is a \"Select question case\" node. We keep it)\n",
        "for node in cfg.useful_nodes.nodes:\n",
        "    tags = node.filter_tags(qmi.QType.ALGO.value)\n",
        "    if len(tags) == 1:\n",
        "        only_tag = tags[0]\n",
        "        if mmi.MathsTask.OPR_TAG.value in only_tag or mmi.MathsTask.SGN_TAG.value in only_tag:\n",
        "            print( \"Removing\", node.name(), only_tag)\n",
        "            node.reset_tags(qmi.QType.ALGO.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCHcurqILvtN"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "qmi.print_algo_purpose_results(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRWSKaURK1L2"
      },
      "outputs": [],
      "source": [
        "# Show useful nodes that have identified algorithm sub-task tags\n",
        "show_quanta_map( \"Maths Purpose Per Node\", qmi.QType.ALGO, \"\", qmi.get_quanta_binary,\n",
        "                 cell_num_shades = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9VDbjTAVGrP"
      },
      "outputs": [],
      "source": [
        "# Show ALL useful nodes with their algorithm sub-task tags (if any)\n",
        "show_quanta_map( \"Maths Purpose All Nodes\", qmi.QType.IMPACT, \"\", qmi.get_quanta_algo,\n",
        "                 cell_num_shades = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_m3qiWYrjRJ"
      },
      "source": [
        "# Part 24: Show known quanta per answer digit\n",
        "\n",
        "Each of the late positions are soley focused on calculating one answer digit. Show the data have we collected on late answer digit.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT8zKdHz1-fZ"
      },
      "outputs": [],
      "source": [
        "for position in range(cfg.num_question_positions + 1, cfg.n_ctx - 1):\n",
        "  print(\"Position:\", position)\n",
        "\n",
        "  # Calculate a table of the known quanta for the specified position for each late token position\n",
        "  mmi.calc_maths_quanta_for_position_nodes(cfg, position)\n",
        "\n",
        "  qmi.save_plt_to_file(cfg=cfg, full_title=\"Quanta At \"+ qmi.position_name(position))\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jux5CWRZ7XDP"
      },
      "source": [
        "# Part 25: Compare ST and SC\n",
        "The sub-tasks ST and SC are similar: They both take Dn,D'n inputs (10x10) and generate \"carry one\" outputs. They differ in that ST occurs in early tokens and has tri-state output, whereas SC occurs in late tokens and has bi-state output. For a sample mixed model, this figure shows PCA results comparing ST and SC output for A2 and A3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp54DotP7aLJ"
      },
      "outputs": [],
      "source": [
        "if cfg.perc_add > 0 and cfg.n_layers >= 2:\n",
        "    a2st = cfg.useful_nodes.get_node_by_tag(qmi.QType.ALGO.value, \"A2.ST\")\n",
        "    a2sc = cfg.useful_nodes.get_node_by_tag(qmi.QType.ALGO.value, \"A2.SC\")\n",
        "    a3st = cfg.useful_nodes.get_node_by_tag(qmi.QType.ALGO.value, \"A3.ST\")\n",
        "    a3sc = cfg.useful_nodes.get_node_by_tag(qmi.QType.ALGO.value, \"A3.SC\")\n",
        "\n",
        "    if a2st is not None and a2sc is not None and a3st is not None and a3sc is not None:\n",
        "      # For all nodes, the attention head output may be transformed by the MLP layer. The images below do NOT show this.\n",
        "      mmi.manual_nodes_pca(cfg, mmi.MathsToken.PLUS,\n",
        "          [[a2st.position, a2st.layer, a2st.num, 2], # A2.ST Trigram needed\n",
        "          [a2sc.position, a2sc.layer, a2sc.num, 2],  # A2.SC Bigram needed (Trigram superset okay)\n",
        "          [a3st.position, a3st.layer, a3st.num, 3],  # A3.ST Trigram needed\n",
        "          [a3sc.position, a2sc.layer, a2sc.num, 3]]) # A3.SC Bigram needed (Trigram superset okay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQhheXmLTUfc"
      },
      "source": [
        "# Part 26: Save useful nodes with behaviour and algorithm tags to JSON file\n",
        "\n",
        "Show a list of the nodes that have proved useful in calculations, together with data on the nodes behavior and algorithmic purposes.\n",
        "Save the data to a Colab temporary JSON file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRW6tiM8hDOB"
      },
      "outputs": [],
      "source": [
        "cfg.useful_nodes.print_node_tags(qmi.QType.ALGO.value, \"\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktlEftAOyEd"
      },
      "outputs": [],
      "source": [
        "# Serialize and save the useful nodes list with feature tags to a temporary CoLab file in JSON format\n",
        "print( \"Saving useful node list with feature tags:\", model_features_fname)\n",
        "cfg.useful_nodes.save_nodes(model_features_fname, qmi.QType.ALGO.value)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uG2gZSoSJD5C",
        "pTd3nmsMJV5T",
        "P8RfHXneJw6n",
        "tz6rUaYvjOcE",
        "ZHiJhch4KCej",
        "-KJhCxFtNKfm",
        "qS_zUfwrXl6P",
        "D6FwJW0tv4Nf",
        "2PjaQvhhayUL",
        "nXBYdxj-jLZc",
        "IifmtnLoTCN5",
        "avCfaCT1Puhz",
        "jFcCpfmKwlAH",
        "Fgg43Sqe8oF1",
        "Z5DMV3I_25ST",
        "Iosx5zE_macF",
        "ThzFD1FxBXg8",
        "3jOBIUDzRrGz",
        "nVyItdhmhIn8",
        "sJo37Qg2ZQpJ",
        "Agw7np7gZmgP",
        "ztfj5n8FjYNt",
        "GOVsZIpoQN_r"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}