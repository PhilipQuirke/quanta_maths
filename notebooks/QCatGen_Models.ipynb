{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063c35b9",
   "metadata": {},
   "source": [
    "# Quanta: Categorization & Generation Model Selection \n",
    "\n",
    "Refer proposal https://docs.google.com/document/d/1x7n2iy1_LZXZNLQpxCzF84lZ8BEG6ZT3KWXC59erhJA \n",
    "\n",
    "Your .env file must contain MARTIAN_API_KEY and HF_TOKEN tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca67e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are tasks that we test the models on\n",
    "tasks = [\n",
    "    \"minimum\",\n",
    "    \"maximum\", \n",
    "    \"sum\",\n",
    "    \"difference\",\n",
    "    \"product\",\n",
    "    \"average\",\n",
    "    \"exponential\" # Excluded for now as too hard\n",
    "]\n",
    "\n",
    "# This is the prompt template we use for each task\n",
    "prompt_template = \"Answer minimally: Given the numbers {x} and {y} calculate the {task}\"\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538c946",
   "metadata": {},
   "source": [
    "## Martian LLMs\n",
    "\n",
    "Supported martian models are at https://app.withmartian.com/docs/index.html\n",
    "and https://api.withmartian.com/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdd3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import httpx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7c933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Martian model data from the API\n",
    "martian_models_url = \"https://api.withmartian.com/v1/models\"\n",
    "response = requests.get(martian_models_url)\n",
    "martian_models_json = response.json()\n",
    "\n",
    "martian_models_json = martian_models_json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59b8c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models after filtering ':cheap': 261\n"
     ]
    }
   ],
   "source": [
    "# Remove from martian_models_json all models whose name contains ':cheap'\n",
    "martian_models_json = [model for model in martian_models_json if ':cheap' not in model['id']]\n",
    "print(f\"Models after filtering ':cheap': {len(martian_models_json)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ab8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_param_size(param_str):\n",
    "    \"\"\"\n",
    "    Convert model name to parameter float (in billions)\n",
    "    Examples: qwen/qwen-2.5-vl-7b-instruct -> 7.0, qwen/qwen3-14 -> 14.0, qwen/qwen3-235b-a22 -> 235.0\n",
    "    \"\"\"\n",
    "    if param_str is None:\n",
    "        return None\n",
    "    \n",
    "    param_str = str(param_str).lower()\n",
    "    \n",
    "    # Handle MoE models like \"8x7b\" (e.g., Mixtral)\n",
    "    moe_match = re.search(r'(\\d+(?:\\.\\d+)?)x(\\d+(?:\\.\\d+)?)b?', param_str)\n",
    "    if moe_match:\n",
    "        try:\n",
    "            return float(moe_match.group(1)) * float(moe_match.group(2))\n",
    "        except (ValueError, AttributeError):\n",
    "            pass\n",
    "    \n",
    "    # Look for patterns like \"7b\", \"14b\", \"235b\"\n",
    "    param_match = re.search(r'(\\d+(?:\\.\\d+)?)b', param_str)\n",
    "    if param_match:\n",
    "        try:\n",
    "            return float(param_match.group(1))\n",
    "        except (ValueError, AttributeError):\n",
    "            pass\n",
    "    \n",
    "    # Look for trailing numbers like \"-14\" or \"-72\" (without 'b' suffix)\n",
    "    trailing_match = re.search(r'-(\\d+(?:\\.\\d+)?)(?:-|$)', param_str)\n",
    "    if trailing_match:\n",
    "        try:\n",
    "            return float(trailing_match.group(1))\n",
    "        except (ValueError, AttributeError):\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "    \n",
    "# Extend the martian_models_json with extracted parameter sizes\n",
    "for model in martian_models_json:\n",
    "    model['size'] = extract_param_size(model.get('id'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d470731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names from the new data structure\n",
    "def extract_model_names():\n",
    "    \"\"\"Extract just the model ids from the new data structure\"\"\"\n",
    "    return [model['id'] for model in martian_models_json]\n",
    "\n",
    "# Group models by provider (if provider info is in id, e.g., 'provider/model')\n",
    "def get_models_by_provider():\n",
    "    providers = {}\n",
    "    for model in martian_models_json:\n",
    "        provider = model['id'].split('/')[0]\n",
    "        if provider not in providers:\n",
    "            providers[provider] = []\n",
    "        providers[provider].append(model)\n",
    "    return providers\n",
    "\n",
    "# Find models by input cost\n",
    "def find_models_by_cost(top_n=5, reverse=False):\n",
    "    models_with_cost = [(model['id'], model.get('pricing', {}).get('prompt', float('inf')), model.get('pricing', {}).get('completion', float('inf'))) for model in martian_models_json]\n",
    "    sorted_by_input = sorted(models_with_cost, key=lambda x: x[1], reverse=reverse)\n",
    "    return sorted_by_input[:top_n]\n",
    "\n",
    "# Find largest models by parameter count (if available)\n",
    "def find_largest_models(top_n=5):\n",
    "    models_with_params = [(model['id'], model['size']) for model in martian_models_json ]\n",
    "    models_with_params = [(m[0], m[1]) for m in models_with_params if m[1] is not None]\n",
    "    sorted_by_params = sorted(models_with_params, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_by_params[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c61b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MARTIAN AI MODELS ANALYSIS ===\n",
      "\n",
      "Number of providers: 43\n",
      "\n",
      "Cheapest Models (input cost):\n",
      "   1 meta-llama/llama-3.2-1b-instruct 0.000000005 0.00000001\n",
      "   2 liquid/lfm-7b 0.00000001 0.00000001\n",
      "   3 liquid/lfm-3b 0.00000002 0.00000002\n",
      "   4 meta-llama/llama-3.1-8b-instruct 0.00000002 0.00000003\n",
      "   5 meta-llama/llama-3.2-3b-instruct 0.00000002 0.00000002\n",
      "\n",
      "Most Expensive Models (input cost):\n",
      "   1 openai/o1-pro 0.00015 0.0006\n",
      "   2 openai/gpt-4 0.00003 0.00006\n",
      "   3 anthropic/claude-3-opus-20240229 0.000015 0.000075\n",
      "   4 anthropic/claude-opus-4-0 0.000015 0.000075\n",
      "   5 anthropic/claude-opus-4-1 0.000015 0.000075\n",
      "\n",
      "Largest Models:\n",
      "   1 mistralai/codestral-2508 2508.0\n",
      "   2 mistralai/magistral-medium-2506 2506.0\n",
      "   3 mistralai/magistral-small-2506 2506.0\n",
      "   4 mistralai/devstral-small-2505 2505.0\n",
      "   5 mistralai/codestral-2501 2501.0\n",
      "\n",
      "JSON key structure:\n",
      "[0] id\n",
      "[0] pricing\n",
      "[0]   prompt\n",
      "[0]   completion\n",
      "[0]   image\n",
      "[0]   request\n",
      "[0]   web_search\n",
      "[0]   internal_reasoning\n",
      "[0] added_at\n",
      "[0] updated_at\n",
      "[0] reliability_tier\n",
      "[0] size\n",
      "\n",
      "Sample model data structure:\n",
      "   {'id': 'ai21/jamba-large-1.7', 'pricing': {'prompt': '0.000002', 'completion': '0.000008', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'size': 1.7}\n",
      "   {'id': 'ai21/jamba-mini-1.7', 'pricing': {'prompt': '0.0000002', 'completion': '0.0000004', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'size': 1.7}\n",
      "   {'id': 'aion-labs/aion-rp-llama-3.1-8b', 'pricing': {'prompt': '0.0000002', 'completion': '0.0000002', 'image': '0', 'request': '0', 'web_search': '0', 'internal_reasoning': '0'}, 'added_at': '2025-10-08T20:59:29.471604+00:00', 'updated_at': '2025-10-08T20:59:29.471604+00:00', 'reliability_tier': 2, 'size': 8.0}\n"
     ]
    }
   ],
   "source": [
    "martian_models_names = extract_model_names()\n",
    "\n",
    "print(\"=== MARTIAN AI MODELS ANALYSIS ===\\n\")\n",
    "\n",
    "providers = get_models_by_provider()\n",
    "print(f\"Number of providers: {len(providers)}\")\n",
    "\n",
    "print(f\"\\nCheapest Models (input cost):\")\n",
    "models = find_models_by_cost(reverse=False)\n",
    "for i, model in enumerate(models, 1):\n",
    "    print( \"  \", i, model[0], model[1], model[2])\n",
    "\n",
    "print(f\"\\nMost Expensive Models (input cost):\")\n",
    "models = find_models_by_cost(reverse=True)\n",
    "for i, model in enumerate(models, 1):\n",
    "    print( \"  \", i, model[0], model[1], model[2])\n",
    "\n",
    "print(f\"\\nLargest Models:\")\n",
    "models = find_largest_models()\n",
    "for i, model in enumerate(models, 1):\n",
    "    print( \"  \", i, model[0], model[1])\n",
    "\n",
    "# Print top-level and nested JSON keys for inspection\n",
    "def print_json_keys(obj, prefix=\"\"):\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            print(f\"{prefix}{key}\")\n",
    "            print_json_keys(value, prefix + \"  \")\n",
    "    elif isinstance(obj, list) and obj:\n",
    "        print_json_keys(obj[0], prefix + \"[0] \")\n",
    "print(\"\\nJSON key structure:\")\n",
    "print_json_keys(martian_models_json)\n",
    "\n",
    "print(f\"\\nSample model data structure:\")\n",
    "for i in range(3):\n",
    "    model = martian_models_json[i]\n",
    "    print(f\"   {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe171e2d",
   "metadata": {},
   "source": [
    "## Analyze Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e2a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ground_truth_correct(answer: str, ground_truth: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the ground_truth appears as the final number in the answer, ignoring whitespace and punctuation.\n",
    "    Accepts answers like '13', '13.', '13**', 'The answer is 13', '**13**', 'random text **13** random text', 'boxed{13}'.\n",
    "    \"\"\"\n",
    "    # Remove trailing whitespace and punctuation\n",
    "    answer_clean = answer.strip().rstrip('.!**')\n",
    "    # Find all numbers in the answer (including negative numbers and those with commas)\n",
    "    numbers = re.findall(r'-?[\\d,]+', answer_clean)\n",
    "    \n",
    "    # Remove commas from the numbers for comparison\n",
    "    numbers_clean = [num.replace(',', '') for num in numbers]\n",
    "\n",
    "    answer_no_comma = answer.replace(\",\", \"\")\n",
    "\n",
    "    return (ground_truth == answer_no_comma or\n",
    "            \"**\"+ground_truth+\"**\" in answer or\n",
    "            \"boxed{\"+ground_truth+\"}\" in answer or\n",
    "            \"\"+ground_truth+\" \" in answer_no_comma  or\n",
    "            \"\"+ground_truth+\".\" in answer_no_comma  or\n",
    "            # Check that the last number matches within 0.001 tolerance\n",
    "            (numbers_clean and abs(float(numbers_clean[-1]) - float(ground_truth)) < 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcd0aa",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffe4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
    "assert MARTIAN_API_KEY, \"API key not found. Please set MARTIAN_API_KEY in your .env file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495d2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.withmartian.com/v1\",\n",
    "    api_key=MARTIAN_API_KEY,\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    "    timeout=httpx.Timeout(60.0, connect=10.0)  # Separate connect timeout     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8566b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_inference(model_name, prompt, ground_truth, timeout=60):\n",
    "    \"\"\"\n",
    "    Send a model a prompt, get the response, compare it to the ground_truth.\n",
    "    Any model taking longer than 60 seconds to respond is consider to have failed or died. \n",
    "    Returns (answer, success). If timeout, returns (\"TIMEOUT\", False).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The OpenAI client has its own timeout parameter\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            timeout=timeout  \n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        success = is_ground_truth_correct(answer, ground_truth)\n",
    "        return answer, success\n",
    "    \n",
    "    except openai.APITimeoutError:\n",
    "        return \"TIMEOUT\", False\n",
    "    except openai.APIError as e:\n",
    "        return f\"Error: {str(e)}\", False\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf22bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple models concurrently, scoring them based on progressive test success.\n",
    "def evaluate_models_progressive(tests, max_workers=32):\n",
    "    model_scores = []\n",
    "\n",
    "    def score_model(model_name):\n",
    "        score = 0\n",
    "        for test_idx, (prompt, ground_truth) in enumerate(tests):\n",
    "            answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
    "            \n",
    "            if success:\n",
    "                score = test_idx + 1\n",
    "            else:\n",
    "                # Check for error codes\n",
    "                if \"TIMEOUT\" in str(answer):\n",
    "                    score = -408  # HTTP timeout code\n",
    "                    break\n",
    "                elif isinstance(answer, str) and answer.startswith(\"Error:\"):\n",
    "                    if \"400\" in answer:\n",
    "                        score = -400\n",
    "                    else:\n",
    "                        score = -999\n",
    "                    break\n",
    "                else:\n",
    "                    # Just got wrong answer\n",
    "                    break\n",
    "        \n",
    "        return {\"model\": model_name, \"score\": score}\n",
    "\n",
    "    print(f\"Evaluating {len(martian_models_json)} models concurrently with {max_workers} workers...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_model = {\n",
    "            executor.submit(score_model, model_name): model_name \n",
    "            for model_name in martian_models_names\n",
    "        }\n",
    "        \n",
    "        for idx, future in enumerate(as_completed(future_to_model, timeout=120), 1):\n",
    "            model_name = future_to_model[future]\n",
    "            try:\n",
    "                result = future.result(timeout=90)  # Add safety margin over API timeout\n",
    "            except Exception as exc:\n",
    "                result = {\"model\": model_name, \"score\": -999}\n",
    "            print(f\"[{idx}/{len(martian_models_json)}] {result['model']}: Score = {result['score']}\")\n",
    "            model_scores.append(result)\n",
    "    \n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe43f27",
   "metadata": {},
   "source": [
    "## Generate prompt and response data for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6540aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_number_pairs(n_examples: int = 200, \n",
    "                         min_val: int = 1, \n",
    "                         max_val: int = 99,\n",
    "                         include_negatives: bool = False,\n",
    "                         seed: int = 42) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Generate diverse number pairs for testing\"\"\"\n",
    "    random.seed(seed)\n",
    "    pairs = []\n",
    "    \n",
    "    # Strategy: Mix of different number ranges for variety\n",
    "    ranges = [\n",
    "        (1, 9),      # Single digits\n",
    "        (10, 99),    # Double digits\n",
    "        (1, 99),     # Mixed\n",
    "    ]\n",
    "    \n",
    "    if include_negatives:\n",
    "        ranges.extend([\n",
    "            (-99, -1),   # Negative numbers\n",
    "            (-50, 50),   # Mixed positive/negative\n",
    "        ])\n",
    "    \n",
    "    examples_per_range = n_examples // len(ranges)\n",
    "    \n",
    "    for min_r, max_r in ranges:\n",
    "        for _ in range(examples_per_range):\n",
    "            x = random.randint(min_r, max_r)\n",
    "            y = random.randint(min_r, max_r)\n",
    "            pairs.append((x, y))\n",
    "    \n",
    "    # Fill remaining with random pairs from full range\n",
    "    while len(pairs) < n_examples:\n",
    "        x = random.randint(min_val, max_val)\n",
    "        y = random.randint(min_val, max_val)\n",
    "        pairs.append((x, y))\n",
    "    \n",
    "    random.shuffle(pairs)\n",
    "    return pairs[:n_examples]\n",
    "\n",
    "def calculate_ground_truth(x: int, y: int, operation: str) -> str:\n",
    "    \"\"\"Calculate the correct answer for a given operation\"\"\"\n",
    "    if operation == \"minimum\":\n",
    "        return str(min(x, y))\n",
    "    elif operation == \"maximum\":\n",
    "        return str(max(x, y))\n",
    "    elif operation == \"sum\":\n",
    "        return str(x + y)\n",
    "    elif operation == \"difference\":\n",
    "        return str(abs(x - y))  # Assuming absolute difference\n",
    "    elif operation == \"product\":\n",
    "        return str(x * y)\n",
    "    elif operation == \"average\":\n",
    "        return str((x + y) / 2)\n",
    "    elif operation == \"exponential\":\n",
    "        # Limit exponential to prevent overflow\n",
    "        try:\n",
    "            result = x ** y\n",
    "            # Cap at reasonable size\n",
    "            if result > 10**15:\n",
    "                return \"OVERFLOW\"\n",
    "            return str(result)\n",
    "        except:\n",
    "            return \"OVERFLOW\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown operation: {operation}\")\n",
    "\n",
    "def generate_synthetic_data(n_examples_per_task: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic data for all tasks\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        #print(f\"Generating {n_examples_per_task} examples for task: {task}\")\n",
    "        \n",
    "        # For exponential, use smaller Y values to prevent overflow\n",
    "        if task == \"exponential\":\n",
    "            pairs = generate_number_pairs(n_examples_per_task, min_val=2, max_val=15)\n",
    "            # Limit Y further for exponential\n",
    "            pairs = [(x, min(y, 10)) for x, y in pairs]\n",
    "        else:\n",
    "            pairs = generate_number_pairs(n_examples_per_task)\n",
    "        \n",
    "        for x, y in pairs:\n",
    "            prompt = prompt_template.format(x=x, y=y, task=task)\n",
    "            ground_truth = calculate_ground_truth(x, y, task)\n",
    "            \n",
    "            # Skip overflow cases\n",
    "            if ground_truth == \"OVERFLOW\":\n",
    "                continue\n",
    "                \n",
    "            all_data.append({\n",
    "                \"task\": task,\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"prompt\": prompt,\n",
    "                \"ground_truth\": ground_truth\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"\\nGenerated {len(df)} total examples across {len(tasks)} tasks\")\n",
    "    print(f\"Examples per task: {df['task'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ad4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 1311 total examples across 7 tasks\n",
      "Examples per task: {'minimum': 200, 'maximum': 200, 'sum': 200, 'difference': 200, 'product': 200, 'average': 200, 'exponential': 111}\n",
      "Sample of generated data:\n",
      "       task  x  y                                                                  prompt    ground_truth\n",
      "    minimum  8  7       Answer minimally: Given the numbers 8 and 7 calculate the minimum               7\n",
      "    minimum 27 92     Answer minimally: Given the numbers 27 and 92 calculate the minimum              27\n",
      "    maximum  8  7       Answer minimally: Given the numbers 8 and 7 calculate the maximum               8\n",
      "    maximum 27 92     Answer minimally: Given the numbers 27 and 92 calculate the maximum              92\n",
      "        sum  8  7           Answer minimally: Given the numbers 8 and 7 calculate the sum              15\n",
      "        sum 27 92         Answer minimally: Given the numbers 27 and 92 calculate the sum             119\n",
      " difference  8  7    Answer minimally: Given the numbers 8 and 7 calculate the difference               1\n",
      " difference 27 92  Answer minimally: Given the numbers 27 and 92 calculate the difference              65\n",
      "    product  8  7       Answer minimally: Given the numbers 8 and 7 calculate the product              56\n",
      "    product 27 92     Answer minimally: Given the numbers 27 and 92 calculate the product            2484\n",
      "    average  8  7       Answer minimally: Given the numbers 8 and 7 calculate the average             7.5\n",
      "    average 27 92     Answer minimally: Given the numbers 27 and 92 calculate the average            59.5\n",
      "exponential  8  7   Answer minimally: Given the numbers 8 and 7 calculate the exponential         2097152\n",
      "exponential 27 10 Answer minimally: Given the numbers 27 and 10 calculate the exponential 205891132094649\n"
     ]
    }
   ],
   "source": [
    "# Generate the data\n",
    "synthetic_data_df = generate_synthetic_data(n_examples_per_task=200)\n",
    "\n",
    "# Display sample\n",
    "print(\"Sample of generated data:\")\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content\n",
    "pd.set_option('display.width', None)         # Don't wrap lines\n",
    "sample_df = synthetic_data_df.groupby('task').head(2)\n",
    "print(sample_df[['task', 'x', 'y', 'prompt', 'ground_truth']].to_string(index=False))\n",
    "\n",
    "# Save to file\n",
    "# synthetic_data_df.to_csv('synthetic_arithmetic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fc77f",
   "metadata": {},
   "source": [
    "## Find good research models\n",
    "\n",
    "Scan the model, using the synthetic data, to find models that can accurately perform several tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached results from prior runs on 13Oct25.\n",
    "\n",
    "# These models passed can accurately answer the first 4 tasks, but are closed source.\n",
    "cached_good_closed_models = [\n",
    "        'anthropic/claude-3-5-sonnet-20240620',\n",
    "        'anthropic/claude-3-7-sonnet-latest',\n",
    "        'anthropic/claude-3-haiku-20240307',\n",
    "        'google/gemini-2.0-flash',\n",
    "        'google/gemini-2.0-flash-001',\n",
    "        'google/gemini-2.0-flash-lite',\n",
    "        'google/gemini-2.0-flash-lite-001',\n",
    "        'google/gemini-2.0-flash-lite-preview',\n",
    "        'google/gemini-2.0-flash-lite-preview-02-05',\n",
    "        'google/gemini-2.5-flash' \n",
    "        'x-ai/grok-3',\n",
    "        'x-ai/grok-3-beta',\n",
    "        'x-ai/grok-3-mini',\n",
    "        'x-ai/grok-3-mini-beta',\n",
    "        'x-ai/grok-code-fast-1',\n",
    "        'deepinfra/google/gemini-2.0-flash-001',\n",
    "        'deepinfra/google/gemini-2.5-flash',\n",
    "        'deepinfra/google/gemini-2.5-pro',\n",
    "        'liquid/lfm-3b', #  Liquid AI LFM-3B model closed. Open models 350M, 700M, 1.2B, and 2.6B under Apache 2.0 \n",
    "        'mistralai/ministral-3b', # Not open source. Research allowed on 8B.\n",
    "]\n",
    "\n",
    "# Good open-source models that passed the first 4 tasks for 5 instances each. Took 25mins to run.\n",
    "cached_good_open_models_4tasks_5instances = [\n",
    "    {\n",
    "        'name': 'deepcogito/cogito-v2-preview-llama-109b-moe',\n",
    "        'hf_repo': 'deepcogito/cogito-v2-preview-llama-109B-MoE',\n",
    "        'url': 'https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE',\n",
    "        'notes': '109B MoE with reasoning capabilities, trained with IDA'\n",
    "    },\n",
    "    {\n",
    "        'name': 'deepinfra/openai/gpt-oss-120b',\n",
    "        'hf_repo': 'openai/gpt-oss-120b',\n",
    "        'url': 'https://huggingface.co/openai/gpt-oss-120b',\n",
    "        'notes': \"OpenAI's 117B MoE model (5.1B active params), Apache 2.0 license\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'deepinfra/openai/gpt-oss-20b',\n",
    "        'hf_repo': 'openai/gpt-oss-20b',\n",
    "        'url': 'https://huggingface.co/openai/gpt-oss-20b',\n",
    "        'notes': \"OpenAI's 21B MoE model (3.6B active params), Apache 2.0 license\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'deepseek/deepseek-r1-distill-qwen-14b',\n",
    "        'hf_repo': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "        'url': 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "        'notes': 'Distilled from DeepSeek-R1, reasoning model'\n",
    "    },\n",
    "    {\n",
    "        'name': 'deepseek/deepseek-r1-distill-qwen-32b',\n",
    "        'hf_repo': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "        'url': 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "        'notes': 'Distilled from DeepSeek-R1, outperforms o1-mini, SOTA for dense models'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-3-70b-instruct',\n",
    "        'hf_repo': 'meta-llama/Meta-Llama-3-70B-Instruct',\n",
    "        'url': 'https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct',\n",
    "        'notes': 'Original Llama 3 70B, released April 2024'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-3.1-70b-instruct',\n",
    "        'hf_repo': 'meta-llama/Llama-3.1-70B-Instruct',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct',\n",
    "        'notes': 'Llama 3.1 with 128K context length'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-3.2-90b-vision-instruct',\n",
    "        'hf_repo': 'meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
    "        'notes': 'Multimodal (text + images), vision reasoning capabilities'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-3.3-70b-instruct',\n",
    "        'hf_repo': 'meta-llama/Llama-3.3-70B-Instruct',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct',\n",
    "        'notes': 'Latest 70B instruct model, December 2023 cutoff'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-4-maverick',\n",
    "        'hf_repo': 'meta-llama/Llama-4-Maverick-17B-128E',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E',\n",
    "        'notes': '17B active params (~400B total), 128 experts, natively multimodal, 1M context',\n",
    "        'instruct_variant': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct'\n",
    "    },\n",
    "]\n",
    " \n",
    "# Good open-source models that passed the first 6 tasks for 5 instances each. Took 60mins to run.\n",
    "cached_good_open_models_6tasks_5instances = [\n",
    "    {\n",
    "        'name': 'meta-llama/llama-3.2-90b-vision-instruct',\n",
    "        'hf_repo': 'meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct',\n",
    "        'notes': 'Multimodal (text + images), vision reasoning capabilities'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-4-maverick',\n",
    "        'hf_repo': 'meta-llama/Llama-4-Maverick-17B-128E',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E',\n",
    "        'notes': '17B active params (~400B total), 128 experts, natively multimodal, 1M context',\n",
    "        'instruct_variant': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/llama-4-scout',\n",
    "        'hf_repo': 'meta-llama/Llama-4-Scout-17B-16E',\n",
    "        'url': 'https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E',\n",
    "        'notes': '17B active params (~109B total), 16 experts, natively multimodal, 10M context, fits on single H100 GPU',\n",
    "        'instruct_variant': 'meta-llama/Llama-4-Scout-17B-16E-Instruct'\n",
    "    },\n",
    "    {\n",
    "        'name': 'nvidia/llama-3.1-nemotron-70b-instruct',\n",
    "        'hf_repo': 'nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
    "        'url': 'https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
    "        'notes': '70B model fine-tuned by NVIDIA using RLHF, #1 on Arena Hard/AlpacaEval 2 LC/MT-Bench as of Oct 2024, trained for helpfulness',\n",
    "        'base_model': 'meta-llama/Llama-3.1-70B-Instruct'\n",
    "    },\n",
    "    {\n",
    "        'name': 'qwen/qwen-2.5-coder-32b-instruct',\n",
    "        'hf_repo': 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
    "        'url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct',\n",
    "        'notes': 'SOTA open-source code LLM, matches GPT-4o coding abilities, 128K context, 5.5T tokens training'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan models for accuracy on first few tasks. \n",
    "num_test_tasks = 6\n",
    "num_per_task = 5\n",
    "num_models_to_find = 10 # Maximum\n",
    "\n",
    "# 0. Search for good open source models (containing 'llama', 'qwen', 'oss')\n",
    "def get_open_models(possible_models):\n",
    "    open_source_keywords = {'meta', 'llama', 'qwen', 'oss'}\n",
    "    return [model for model in possible_models if any(keyword in model['id'] for keyword in open_source_keywords)]\n",
    "\n",
    "# 1. Filter models <= max params\n",
    "def get_small_models(possible_models, max_params):\n",
    "    small_models = []\n",
    "    for model in possible_models:\n",
    "        param_size = model['size']\n",
    "        if param_size is not None and param_size <= max_params:\n",
    "            small_models.append(model)\n",
    "    return small_models\n",
    "\n",
    "# 2. Select first few tasks\n",
    "def get_scan_tasks():\n",
    "    return tasks[:num_test_tasks]\n",
    "\n",
    "# 3. For each model, check accuracy on a few instances of each task\n",
    "def scan_model_accuracy(model, df, scan_tasks):\n",
    "    results = {}\n",
    "    for task in scan_tasks:\n",
    "        # Select 5 examples for this task\n",
    "        task_df = df[df['task'] == task].sample(n=num_per_task, random_state=42)\n",
    "        correct = 0\n",
    "        for _, row in task_df.iterrows():\n",
    "            try:\n",
    "                answer, success = run_model_inference(model['id'], row[\"prompt\"], row[\"ground_truth\"])\n",
    "            except Exception as e:\n",
    "                answer = f\"Error: {str(e)}\"\n",
    "                success = False\n",
    "            print(f\"Model: {model['id']}\\nOutput/Error: {answer}\\nSuccess: {success}\\n{'-'*40}\")\n",
    "\n",
    "            if success:\n",
    "                correct += 1\n",
    "            else:\n",
    "                # If any failure, stop testing this task\n",
    "                break\n",
    "        results[task] = correct\n",
    "    return results\n",
    "\n",
    "# 4. Find some models that get all questions correct for each task\n",
    "def get_good_models(small_models,scan_tasks):\n",
    "    good_models = []\n",
    "\n",
    "    for model in small_models:\n",
    "        acc = scan_model_accuracy(model, synthetic_data_df, scan_tasks)\n",
    "        if all(v == 5 for v in acc.values()):\n",
    "            good_models.append(model['id'])\n",
    "        if len(good_models) >= num_models_to_find:\n",
    "            break\n",
    "\n",
    "    return good_models\n",
    "\n",
    "use_cached_models = True\n",
    "if use_cached_models:\n",
    "    # For speed, use cached results from prior runs\n",
    "    cached_good_models = cached_good_open_models_6tasks_5instances\n",
    "    good_models = [model['name'] for model in cached_good_open_models_6tasks_5instances]\n",
    "else:\n",
    "    open_models = get_open_models(martian_models_json)\n",
    "    small_models = open_models # get_small_models(open_models, 3.0)\n",
    "    print(f\"Found {len(small_models)} models\")\n",
    "\n",
    "    scan_tasks = get_scan_tasks()\n",
    "    good_models = get_good_models(small_models, scan_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd5d5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some models with perfect accuracy on 5 instances of first 6 tasks:\n",
      "   meta-llama/llama-3.2-90b-vision-instruct\n",
      "   meta-llama/llama-4-maverick\n",
      "   meta-llama/llama-4-scout\n",
      "   nvidia/llama-3.1-nemotron-70b-instruct\n",
      "   qwen/qwen-2.5-coder-32b-instruct\n",
      "   qwen/qwen-max\n",
      "   qwen/qwen-plus\n",
      "   qwen/qwen-plus-2025-07-28\n",
      "   qwen/qwen-plus-2025-07-28:thinking\n",
      "   qwen/qwen-vl-max\n"
     ]
    }
   ],
   "source": [
    "# These models are often downloadable from HuggingFace else available via API\n",
    "print(f\"Some models with perfect accuracy on {num_per_task} instances of first {num_test_tasks} tasks:\")\n",
    "for model_name in good_models :\n",
    "    print( \"  \", model_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568249",
   "metadata": {},
   "source": [
    "## Manually inspect model output\n",
    "\n",
    "We want to avoid models that use a python sandbox to do math. This is sometimes visible in the answer detail. View sample answers here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e02c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the top models, ask one instance of one task and show the answer\n",
    "def inspect_model_answers():\n",
    "    for model_name in good_models:\n",
    "        task = random.choice(tasks)\n",
    "        example_df = synthetic_data_df[synthetic_data_df['task'] == task].sample(n=1, random_state=42).iloc[0]\n",
    "        print(f\"\\nModel: {model_name}\\nTask: {task}\\nPrompt: {example_df['prompt']}\\nGround Truth: {example_df['ground_truth']}\")\n",
    "        answer, success = run_model_inference(model_name, example_df['prompt'], example_df['ground_truth'])\n",
    "        print(f\"Answer: {answer}\\nSuccess: {success}\\n{'-'*60}\")\n",
    "\n",
    "#inspect_model_answers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b8164",
   "metadata": {},
   "source": [
    "## Download models\n",
    "\n",
    "We download the model so we can look at its internals during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91ec7626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'meta-llama/llama-4-scout', 'hf_repo': 'meta-llama/Llama-4-Scout-17B-16E', 'url': 'https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E', 'notes': '17B active params (~109B total), 16 experts, natively multimodal, 10M context, fits on single H100 GPU', 'instruct_variant': 'meta-llama/Llama-4-Scout-17B-16E-Instruct'}\n"
     ]
    }
   ],
   "source": [
    "model_index = 2  # Change index to select different model from cached_good_open_models_6tasks_5instances\n",
    "model_data = cached_good_open_models_6tasks_5instances[model_index]\n",
    "model_name = model_data['hf_repo']  \n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37d6d63d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.\n401 Client Error. (Request ID: Root=1-68ed69fb-0bc159567b4be6061fcffab0;9b9e7dba-8b7b-44d7-b4de-b02c588f3c38)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/resolve/main/config.json.\nAccess to model meta-llama/Llama-4-Scout-17B-16E is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/utils/hub.py:478\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:426\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-68ed69fb-0bc159567b4be6061fcffab0;9b9e7dba-8b7b-44d7-b4de-b02c588f3c38)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/resolve/main/config.json.\nAccess to model meta-llama/Llama-4-Scout-17B-16E is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m      3\u001b[39m hf_token = os.getenv(\u001b[33m'\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_token\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     11\u001b[39m     model_name,\n\u001b[32m     12\u001b[39m     token=hf_token,\n\u001b[32m     13\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# automatically handles device placement\u001b[39;00m\n\u001b[32m     14\u001b[39m     torch_dtype=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# uses appropriate precision\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1078\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1288\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1285\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1286\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1289\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1290\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    717\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/transformers/utils/hub.py:542\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    541\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    543\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    544\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.\n401 Client Error. (Request ID: Root=1-68ed69fb-0bc159567b4be6061fcffab0;9b9e7dba-8b7b-44d7-b4de-b02c588f3c38)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/resolve/main/config.json.\nAccess to model meta-llama/Llama-4-Scout-17B-16E is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token,\n",
    "    device_map=\"auto\",  # automatically handles device placement\n",
    "    torch_dtype=\"auto\"  # uses appropriate precision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89150da",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
