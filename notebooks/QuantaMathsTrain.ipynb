{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3OoV2Pd-w7g"
      },
      "source": [
        "# Quanta Maths: Integer Addition and Subtraction in Transformers - Train a Model\n",
        "\n",
        "This CoLab defines and trains a Transformer model that performs integer addition, subtraction and multiplication e.g. 133357+182243=+0315600, 123450-345670=-0123230 and 000345*000823=+283935. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVn4muC-5p1"
      },
      "source": [
        "This CoLab trains the model, storing the results to the Colab files. Useful models are manually copied to HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWXJvUUb-6in"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5K2kA0L_FHc"
      },
      "source": [
        "# Part 0: Import libraries\n",
        "Imports standard libraries. Do not read.\n",
        "\n",
        "Imports \"quanta_maths\" public library as \"qmi\". This library is specific to this CoLab's \"QuantaTool\" approach to transformer analysis. Refer https://github.com/PhilipQuirke/quanta_maths/blob/main/README.md for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBOAqd0ZuhAV"
      },
      "outputs": [],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    !pip install kaleido\n",
        "    !pip install circuitsvis\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoyWBsgWsDLK"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_mech_interp.git\n",
        "import QuantaMechInterp as qmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXnKeKlxtzYc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/PhilipQuirke/quanta_maths.git\n",
        "import MathsMechInterp as mmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "295MWK2owFNa"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h1MGZnAwHZz"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpHPxqhwMeQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "import circuitsvis as cv\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmeLLxG8OdDB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sklearn # Aka scikit.learn\n",
        "import skopt # Aka scikit.optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS-PO-BPwo8f"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuyTONDd_DLJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# GrokFast paper https://arxiv.org/pdf/2405.20233\n",
        "!git clone https://github.com/ironjr/grokfast.git\n",
        "sys.path.append('/content/grokfast')\n",
        "from grokfast import gradfilter_ma, gradfilter_ema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rl41loM_Apt"
      },
      "source": [
        "# Part 1A: Configuration\n",
        "\n",
        "This CoLab can train \"arithmetic\" models (e.g. add_d6_l2_h3_t15K_s372001) with:\n",
        "- \"add\": can do addition, subtraction, multiplication or a mixed of operations\n",
        "- \"d6\": supports questions with 5 to 20 digits\n",
        "- \"l2\": 1 to 4 layers\n",
        "- \"h3\": 3 to 5 attention heads\n",
        "- \"t15K\": trained for 15,000 steps\n",
        "- \"s372001\": with a specified training seed\n",
        "\n",
        "The add_d6_l2_h3_t15K_s372001.pth model has a very low loss (9e-9) and answers questions with 99.9999% accuracy.\n",
        "\n",
        "This CoLab can train a new \"mixed\" model, re-using an existing \"addition\" model's weights (e.g. ins1_mix_d6_l3_h4_t20K_s372001.pth). The insert modes are:\n",
        "- \"ins1\": initialize new model with addition model weights\n",
        "- \"ins2\": As per \"ins1\" but also resets useful attention heads every 100 training steps.\n",
        "- \"ins3\": As per \"ins1\" but also resets useful attention heads and MLP layers every 100 training steps.\n",
        "- \"ins4\": As per \"ins3\" but reset every training step.\n",
        "\n",
        "The weights for ~45 trained models are loaded on HuggingFace at https://huggingface.co/PhilipQuirke/QuantaMaths  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z1TVBObuOKF"
      },
      "outputs": [],
      "source": [
        "class TrainMathsConfig(mmi.MathsConfig):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.main_model = None\n",
        "\n",
        "    # Default batch size for training\n",
        "    self.batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdad0Np8kSC9"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"main\" configuration class. MathsConfig is derived from the chain AlgoConfig > UsefulConfig > ModelConfig\n",
        "cfg = TrainMathsConfig()\n",
        "\n",
        "# Which model do we want to train? Uncomment one line:\n",
        "\n",
        "# Addition models\n",
        "#cfg.set_model_names( \"add_d5_l1_h3_t15K_s372001\" )  # AddAccuracy=Two9s. Inaccurate as only has one layer\n",
        "#cfg.set_model_names( \"add_d5_l2_h3_t15K_s372001\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t15K_s372001\" )  # AddAccuracy=Six9s. MAIN FOCUS\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t20K_s173289\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t20K_s572091\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d5_l2_h3_t40K_s372001\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d6_l2_h3_t40K_s372001\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d7_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d8_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d9_l2_h3_t45K_s173289\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d10_l2_h3_t40K_s572091\" ) # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d10_l2_h3_t40K_gf_s572091\" ) # AddAccuracy=Six9s. GrokFast.\n",
        "#cfg.set_model_names( \"add_d11_l2_h3_t50K_s572091\" )  # AddAccuracy=Five9s\n",
        "#cfg.set_model_names( \"add_d12_l2_h3_t50K_s572091\" )  # AddAccuracy=Five9s\n",
        "#cfg.set_model_names( \"add_d13_l2_h3_t50K_s572091\" )  # AddAccuracy=Six9s\n",
        "#cfg.set_model_names( \"add_d14_l2_h3_t60K_s572091\" )  # AddAccuracy=Three9S\n",
        "#cfg.set_model_names( \"add_d15_l2_h3_t80K_s572091\" ) # AddAccuracy=Five9s\n",
        "#cfg.set_model_names( \"add_d20_l2_h3_t80K_s572091\" ) # AddAccuracy=Poor!\n",
        "\n",
        "# Subtraction model\n",
        "#cfg.set_model_names( \"sub_d6_l2_h3_t30K_s372001\" )  # SubAccuracy=Six9s\n",
        "#cfg.set_model_names( \"sub_d10_l2_h3_t75K_s173289\" )  # SubAccuracy=Two9s\n",
        "#cfg.set_model_names( \"sub_d10_l2_h3_t75K_gf_s173289\" )  # SubAccuracy=Two9s. GrokFast.\n",
        "\n",
        "# Addition & Subtraction models\n",
        "#cfg.set_model_names( \"mix_d5_l3_h4_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Six9\n",
        "#cfg.set_model_names( \"mix_d6_l3_h4_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Six9s\n",
        "#cfg.set_model_names( \"mix_d7_l3_h4_t50K_s372001\" )  # Add/SubAccuracy=Five9s/Five9s\n",
        "#cfg.set_model_names( \"mix_d8_l3_h4_t60K_s173289\" )  # Add/SubAccuracy=Six9s/Five9s\n",
        "#cfg.set_model_names( \"mix_d9_l3_h4_t60K_s173289\" )  # Add/SubAccuracy=Six9s/Four9s\n",
        "#cfg.set_model_names( \"mix_d10_l3_h4_t75K_s173289\" )  # Add/SubAccuracy=Five9s/Two9s\n",
        "#cfg.set_model_names( \"mix_d10_l3_h4_t75K_gf_s173289\" )  # Add/SubAccuracy=Six9s/Three9s. GrokFast.\n",
        "#cfg.set_model_names( \"mix_d11_l3_h4_t80K_s572091\" )  # Add/SubAccuracy=Six9s/Four9s\n",
        "#cfg.set_model_names( \"mix_d12_l3_h4_t85K_s572091\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"mix_d13_l3_h4_t85K_s572091\" )  # Add/SubAccuracy=TODO\n",
        "\n",
        "# Multiplication model\n",
        "#cfg.set_model_names( \"mul_d6_l3_h4_t75K_s372001\" )  # Inaccurate. AvgFinalLoss 0.026\n",
        "\n",
        "# Addition&Subtraction models initialized with addition model.\n",
        "#cfg.set_model_names( \"ins1_mix_d5_l2_h3_t40K_s572091,add_d5_l2_h3_t15K_s372001\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l2_h3_t40K_s572091,add_d6_l2_h3_t20K_s173289\" )  # Add/SubAccuracy=Six9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h3_t40K_s572091,add_d6_l2_h3_t20K_s173289\" )  # Add/SubAccuracy=Six9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h3_t80K_s572091,add_d6_l2_h3_t40K_s372001\" )  # Add/SubAccuracy=Six9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t40K_s372001,add_d6_l2_h3_t15K_s372001\" )  # Add/SubAccuracy=Six9s/Six9s. MAIN FOCUS\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t40K_s173289,add_d6_l2_h3_t20K_s173289\" )  # Add/SubAccuracy=Five9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d6_l3_h4_t50K_s572091,add_d6_l2_h3_t20K_s173289\" )  # Add/SubAccuracy=Six9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d7_l3_h4_t50K_s572091,add_d7_l2_h3_t45K_s173289\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d8_l3_h4_t70K_s572091,add_d8_l2_h3_t45K_s173289\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d9_l3_h4_t70K_s572091,add_d9_l2_h3_t45K_s173289\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d10_l3_h3_t50K_s572091,add_d10_l2_h3_t40K_s572091\" )  # Add/SubAccuracy=Five9s/Five9s\n",
        "#cfg.set_model_names( \"ins1_mix_d10_l3_h3_t50K_gf_s572091,add_d10_l2_h3_t40K_gf_s572091\" ) # Add/SubAccuracy=Five9s/Two9s. GrokFast.\n",
        "#cfg.set_model_names( \"ins1_mix_d11_l3_h4_t75K_s572091,add_d11_l2_h3_t50K_s572091\" )  # Add/SubAccuracy=TODO\n",
        "#cfg.set_model_names( \"ins1_mix_d12_l3_h4_t85K_s572091,add_d12_l2_h3_t50K_s572091\" )  # Add/SubAccuracy=TODO\n",
        "\n",
        "# Addition&Subtraction model initialized with addition model. Reset useful heads every 100 training steps.\n",
        "#cfg.set_model_names( \"ins2_mix_d6_l4_h4_t40K_s372001,add_d6_l2_h3_t15K_s372001\" )  # Add/SubAccuracy=Five9s/Five9s\n",
        "\n",
        "# Addition&Subtraction model initialized with addition model. Reset useful heads & MLPs every 100 training steps.\n",
        "cfg.set_model_names( \"ins3_mix_d6_l4_h3_t40K_s372001,add_d6_l2_h3_t15K_s372001\" )  # Add/SubAccuracy=Four9s/Two9s\n",
        "\n",
        "# Addition&Subtraction model initialized with addition model. Reset useful heads & MLPs every 1 training steps.\n",
        "#cfg.set_model_names( \"ins4_mix_d6_l4_h3_t40K_s372001,add_d6_l2_h3_t40K_s372001\" )  # AvgFinalLoss 4.7e-06\n",
        "\n",
        "# Multiplication & Addition & Subtraction models initialized with Addition&Subtraction model.\n",
        "#cfg.set_model_names( \"ins1_mas_d6_l3_h4_t75K_s371793,ins1_mix_d6_l3_h4_t40K_s372001\" )  # AvgFinalLoss ~0.1\n",
        "#cfg.set_model_names( \"ins1_mas_d6_l3_h4_t75K_gf_s371793,ins1_mix_d6_l3_h4_t40K_s372001\" )  # AvgFinalLoss ~0.1\n",
        "#cfg.set_model_names( \"ins1_mas_d6_l5_h4_t75K_gf_s371793,ins1_mix_d6_l3_h4_t40K_s372001\" )  # AvgFinalLoss 0.12\n",
        "\n",
        "# Mixed models initialized with addition model. Randomise heads without a known subtask\n",
        "#cfg.set_model_names( \"ins5_mix_d6_l3_h4_t30K_s775824,ins1_mix_d6_l3_h4_t40K_s372001\" )  # TODO: Code and train\n",
        "#cfg.set_model_names( \"ins5_mix_d6_l2_h4_t30K_s775824,ins1_mix_d6_l3_h4_t40K_s372001\" )  # TODO: Code and train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue-HriK4imsU"
      },
      "source": [
        "# Part 1B: Configuration: Input and Output file names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LUczqMuyWD3"
      },
      "outputs": [],
      "source": [
        "def print_config():\n",
        "  print(\"%Add=\", cfg.perc_add, \"%Sub=\", cfg.perc_sub, \"%Mult=\", cfg.perc_mult, \"InsertMode=\", cfg.insert_mode, \"GrokFast=\", cfg.grokfast, \"File=\", cfg.file_config_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKgiXwaFiBj4"
      },
      "outputs": [],
      "source": [
        "base_repo_name = 'PhilipQuirke'\n",
        "main_fname_pth = 'model.pth'\n",
        "main_fname_json = 'training_loss.json'\n",
        "cfg.hf_repo = base_repo_name + \"/QuantaMaths_\" + cfg.model_name # \"PhilipQuirke/QuantaMaths_add_d6_l2_h3_t15K_s372001\"\n",
        "\n",
        "print_config()\n",
        "print(\"weight_decay=\", cfg.weight_decay, \"lr=\", cfg.lr, \"batch_size=\", cfg.batch_size)\n",
        "print('Main model will save to Colab temporary file:', main_fname_pth)\n",
        "print('Main model config etc will save to Colab temporary file:', main_fname_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxV5g6DlGvvh"
      },
      "outputs": [],
      "source": [
        "# Singleton QuantaTool \"ablation intervention\" configuration class\n",
        "acfg = qmi.acfg\n",
        "acfg.reset_ablate()\n",
        "cfg.configure_acfg_singleton()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5A09Jitdn53"
      },
      "source": [
        "# Part 3A: Set Up: Vocabulary / Embedding / Unembedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44jZTvGxdppx"
      },
      "outputs": [],
      "source": [
        "mmi.set_maths_vocabulary(cfg)\n",
        "mmi.set_maths_question_meanings(cfg)\n",
        "print(cfg.token_position_meanings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tExv4rk_L0C"
      },
      "source": [
        "# Part 3B: Create main_model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg6uwYzew4u5"
      },
      "outputs": [],
      "source": [
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = cfg.get_HookedTransformerConfig()\n",
        "\n",
        "# Create the main transformer model\n",
        "cfg.main_model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer, scheduler = qmi.get_training_optimizer_and_scheduler(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg267eav_QSl"
      },
      "source": [
        "# Part 4: Data Generator\n",
        "This section defines the training/testing data generator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSbe7v82hB6C"
      },
      "outputs": [],
      "source": [
        "# GrokFast uses a gradient across batches, so for mixed models, \"one operation per batch\" is lumpy and hinders training.\n",
        "# Each batch must contain a mixture of maths operations.\n",
        "use_mixed_batches = cfg.grokfast and cfg.perc_add > 0 and cfg.perc_sub > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH_rXfA2xAIG"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" maths \"questions\" data generator function. Invoked using next().\n",
        "cfg.set_seed(cfg.training_seed)\n",
        "if use_mixed_batches:\n",
        "  # Each batch must contain a mixture of maths operations.\n",
        "  ds = mmi.maths_data_generator_mixed( cfg )\n",
        "else:\n",
        "  # Each batch contains questions for a single maths operations\n",
        "  ds = mmi.maths_data_generator( cfg, enrich_data=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6c62sGVxVhM"
      },
      "outputs": [],
      "source": [
        "# Test data generator\n",
        "tokens = next(ds)\n",
        "print(tokens[:3,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvijaDhSjn0B"
      },
      "source": [
        "# Part 5: Read insert_model from HuggingFace (optional)\n",
        "\n",
        "If we are initialising the untrained model with an existing model,\n",
        "then we load the existing model from HuggingFace.\n",
        "We load both the model weights and a json file stating which nodes in the model are actually doing useful calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioCE99sOl-Gt"
      },
      "outputs": [],
      "source": [
        "insert_fname_pth = \"\"\n",
        "insert_nodes_fname = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LonIqn6KjpTe"
      },
      "outputs": [],
      "source": [
        "# Read insert_model weights from HuggingFace\n",
        "if cfg.insert_mode >= 1:\n",
        "  insert_repo_name = base_repo_name + \"/QuantaMaths_\" + cfg.insert_model_name\n",
        "\n",
        "  ht_cfg = HookedTransformerConfig(\n",
        "      n_layers = cfg.insert_n_layers,\n",
        "      n_heads = cfg.insert_n_heads,\n",
        "      d_model = cfg.d_model, # Assume constant\n",
        "      d_head = cfg.d_head, # Assume constant\n",
        "      d_mlp = cfg.d_mlp, # Assume constant\n",
        "      act_fn = cfg.act_fn, # Assume constant\n",
        "      normalization_type = 'LN',\n",
        "      d_vocab = cfg.d_vocab, # Assume constant\n",
        "      d_vocab_out = cfg.d_vocab, # Assume constant\n",
        "      n_ctx = cfg.n_ctx, # Assume constant\n",
        "      init_weights = True, # Assume constant\n",
        "      device = \"cuda\",\n",
        "      seed = cfg.insert_training_seed,\n",
        "  )\n",
        "\n",
        "  insert_model = HookedTransformer(ht_cfg)\n",
        "\n",
        "  print('Loading insertion model from', insert_repo_name)\n",
        "  insert_model.load_state_dict(utils.download_file_from_hf(repo_name=insert_repo_name, file_name=\"model.pth\", force_is_torch=True))\n",
        "  insert_model.eval()\n",
        "\n",
        "  print(\"Loaded insert model\", insert_repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXw7-a1E025H"
      },
      "outputs": [],
      "source": [
        "if cfg.insert_mode >= 1:\n",
        "  # Read insert_model useful node information from HuggingFace\n",
        "  file_path = hf_hub_download(repo_id=insert_repo_name, filename=\"behaviors.json\", revision=\"main\")\n",
        "  cfg.useful_nodes.load_nodes(file_path)\n",
        "  print( \"Loaded:\", len(cfg.useful_nodes.nodes), \"Sample:\", cfg.useful_nodes.nodes[0].tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fW5galHlQqA"
      },
      "source": [
        "# Part 6B: Transfer all of insert_model into main_model (optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKe3LUst8veJ"
      },
      "outputs": [],
      "source": [
        "# Transfer all attention heads weights from the small to the main model, updating the right-most small.n_heads of main_model\n",
        "def transfer_all_heads(from_model, from_cfg, start_layer, end_layer, to_model):\n",
        "  from_n_heads = from_cfg[\"n_heads\"]\n",
        "  for from_layer_no, to_layer_no in enumerate(range(start_layer, end_layer+1)):\n",
        "    to_model.blocks[to_layer_no].attn.W_Q.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.W_Q.clone().data\n",
        "    to_model.blocks[to_layer_no].attn.W_K.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.W_K.clone().data\n",
        "    to_model.blocks[to_layer_no].attn.W_V.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.W_V.clone().data\n",
        "\n",
        "    to_model.blocks[to_layer_no].attn.b_Q.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.b_Q.clone().data\n",
        "    to_model.blocks[to_layer_no].attn.b_K.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.b_K.clone().data\n",
        "    to_model.blocks[to_layer_no].attn.b_V.data[:from_n_heads] = from_model.blocks[from_layer_no].attn.b_V.clone().data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcaxqmJC8vsa"
      },
      "outputs": [],
      "source": [
        "# Transfer all MLP layer weights from the small to the main model, updating the right-most small.d_mlp of main_model\n",
        "def transfer_all_mlps(from_model, from_cfg, start_layer, end_layer, to_model):\n",
        "  from_d_mlp = from_cfg[\"d_mlp\"]\n",
        "  for from_layer_no, to_layer_no in enumerate(range(start_layer, end_layer+1)):\n",
        "    to_model.blocks[to_layer_no].mlp.W_in.data[:, :from_d_mlp] = from_model.blocks[from_layer_no].mlp.W_in.clone().data\n",
        "    to_model.blocks[to_layer_no].mlp.b_in.data[:from_d_mlp] = from_model.blocks[from_layer_no].mlp.b_in.clone().data\n",
        "    to_model.blocks[to_layer_no].mlp.W_out.data[:from_d_mlp,] = from_model.blocks[from_layer_no].mlp.W_out.clone().data\n",
        "    to_model.blocks[to_layer_no].mlp.b_out.data = from_model.blocks[from_layer_no].mlp.b_out.clone().data #PQR????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UsGFC-b9oVC"
      },
      "outputs": [],
      "source": [
        "def transfer_all_ln(from_model, start_layer, end_layer, to_model):\n",
        "  for from_layer_no, to_layer_no in enumerate(range(start_layer, end_layer+1)):\n",
        "    to_model.blocks[to_layer_no].ln1.w.data = from_model.blocks[from_layer_no].ln1.w.clone().data\n",
        "    to_model.blocks[to_layer_no].ln1.b.data = from_model.blocks[from_layer_no].ln1.b.clone().data\n",
        "\n",
        "  to_model.ln_final.w.data = from_model.ln_final.w.clone().data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYpItGeX9-jq"
      },
      "outputs": [],
      "source": [
        "def transfer_all_embeds(from_model, to_model):\n",
        "  to_model.embed.W_E.data = from_model.embed.W_E.clone().data\n",
        "  to_model.pos_embed.W_pos.data = from_model.pos_embed.W_pos.clone().data\n",
        "  to_model.unembed.W_U.data = from_model.unembed.W_U.clone().data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnkU3fYKlQAk"
      },
      "outputs": [],
      "source": [
        "from_cfg = {}\n",
        "to_cfg = {}\n",
        "\n",
        "# Insert the \"from\" model weights into the \"to\" model\n",
        "def transfer_full_model(from_model, to_model, start_layer, end_layer, transfer_ln=True, transfer_embeds=True):\n",
        "  \"\"\"Args:\n",
        "  from_model: The model to transfer weights from\n",
        "  to_model: The model to transfer weights to\n",
        "  start_layer: The first layer to transfer weights to\n",
        "  end_layer: The last layer to transfer weights to (Note that this is end-inclusive!)\n",
        "  \"\"\"\n",
        "  global from_cfg\n",
        "  global to_cfg\n",
        "\n",
        "  from_cfg = {k: v for k,v in from_model.cfg.__dict__.items() if k in [\"d_head\", \"d_mlp\", \"d_model\", \"n_heads\", \"n_layers\"]}\n",
        "  to_cfg = {k: v for k,v in to_model.cfg.__dict__.items() if k in [\"d_head\", \"d_mlp\", \"d_model\", \"n_heads\", \"n_layers\"]}\n",
        "\n",
        "  # Sanity checks for to_model size >= from model size\n",
        "  assert from_cfg[\"d_model\"] == to_cfg[\"d_model\"]\n",
        "  assert from_cfg[\"d_head\"] == to_cfg[\"d_head\"]\n",
        "  assert from_cfg[\"n_layers\"] <= to_cfg[\"n_layers\"]\n",
        "  assert from_cfg[\"n_heads\"] <= to_cfg[\"n_heads\"]\n",
        "  assert from_cfg[\"d_mlp\"] <= to_cfg[\"d_mlp\"]\n",
        "\n",
        "  assert 0 <= start_layer < end_layer <= to_cfg[\"n_layers\"] # Make sure start_layer and end_layer are valid\n",
        "  assert end_layer - start_layer + 1 == from_cfg[\"n_layers\"] # Make sure the number of layers to transfer is correct\n",
        "\n",
        "  transfer_all_heads(from_model, from_cfg, start_layer, end_layer, to_model)\n",
        "  transfer_all_mlps(from_model, from_cfg, start_layer, end_layer, to_model)\n",
        "  if transfer_ln:\n",
        "    transfer_all_ln(from_model, start_layer, end_layer, to_model)\n",
        "  if transfer_embeds:\n",
        "    transfer_all_embeds(from_model, to_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rd7d5Gjmrd7"
      },
      "outputs": [],
      "source": [
        "def insert_existing_model( first_time ):\n",
        "  if cfg.insert_mode >= 1 :\n",
        "    # Is the destination the first few or last few layers of the main_model?\n",
        "    start_layer = max(0, cfg.n_layers - cfg.insert_n_layers) if cfg.insert_late else 0\n",
        "    end_layer = min(cfg.n_layers-1, start_layer + cfg.insert_n_layers-1)\n",
        "\n",
        "    if first_time:\n",
        "      print( \"Inserting trained from_model\", insert_fname_pth)\n",
        "      print( \"into untrained main_model\", cfg.file_config_prefix)\n",
        "      print( \"destination layers:\", start_layer, end_layer)\n",
        "\n",
        "    transfer_full_model(insert_model, cfg.main_model, start_layer, end_layer, first_time, first_time)\n",
        "\n",
        "\n",
        "insert_existing_model( True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppYbJCP_Ubj"
      },
      "source": [
        "# Part 6C: Transfer useful heads of insert_model into main_model (optional)\n",
        "\n",
        "Transfer just the useful attention heads from insert_model into main_model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nvucMtl_8tt"
      },
      "outputs": [],
      "source": [
        "# Transfer one attention head's weights from the small to the main model.\n",
        "# The right-most small.n_heads of main_model are updated\n",
        "def transfer_one_head(from_model, from_layer_no, from_head_no, to_model, start_layer):\n",
        "  to_layer_no = start_layer + from_layer_no\n",
        "  to_head_no = to_cfg[\"n_heads\"] - from_cfg[\"n_heads\"] + from_head_no\n",
        "\n",
        "  to_model.blocks[to_layer_no].attn.W_Q.data[to_head_no] = from_model.blocks[from_layer_no].attn.W_Q.clone().data[from_head_no]\n",
        "  to_model.blocks[to_layer_no].attn.W_K.data[to_head_no] = from_model.blocks[from_layer_no].attn.W_K.clone().data[from_head_no]\n",
        "  to_model.blocks[to_layer_no].attn.W_V.data[to_head_no] = from_model.blocks[from_layer_no].attn.W_V.clone().data[from_head_no]\n",
        "\n",
        "  to_model.blocks[to_layer_no].attn.b_Q.data[to_head_no] = from_model.blocks[from_layer_no].attn.b_Q.clone().data[from_head_no]\n",
        "  to_model.blocks[to_layer_no].attn.b_K.data[to_head_no] = from_model.blocks[from_layer_no].attn.b_K.clone().data[from_head_no]\n",
        "  to_model.blocks[to_layer_no].attn.b_V.data[to_head_no] = from_model.blocks[from_layer_no].attn.b_V.clone().data[from_head_no]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iefg8KGG_flE"
      },
      "outputs": [],
      "source": [
        "def transfer_useful_heads(from_model, to_model):\n",
        "  if cfg.insert_mode >= 2 and len(cfg.useful_nodes) > 0:\n",
        "    # Is the destination the first few or last few layers of the main_model?\n",
        "    start_layer = cfg.n_layers - cfg.insert_n_layers if cfg.insert_late else 0\n",
        "    transfer_count = 0\n",
        "\n",
        "    for use_cell in cfg.useful_nodes:\n",
        "      if use_cell.is_head:\n",
        "        transfer_one_head(from_model, use_cell.layer, use_cell.num, to_model, start_layer)\n",
        "        transfer_count += 1\n",
        "\n",
        "    print('Transferred', transfer_count, 'useful heads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn3YEQI1gwJK"
      },
      "source": [
        "# Part 6D: Randomise some attention heads in main_model (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VqkVE0RyI8h"
      },
      "outputs": [],
      "source": [
        "if cfg.insert_mode == 5 :\n",
        "    # We randomise any attention head that does not have an identified sub-task.\n",
        "    # The hope is that will eliminate some \"low-value\" or \"noise\" nodes from the new model.\n",
        "\n",
        "    # Load inserted model's subtask data\n",
        "    #      https://huggingface.co/PhilipQuirke/QuantaMaths_ins1_mix_d6_l3_h4_t40K_s372001/features.json\"\n",
        "    file_path = hf_hub_download(repo_id=insert_repo_name, filename=\"features.json\", revision=\"main\")\n",
        "    cfg.useful_nodes.load_nodes(file_path)\n",
        "\n",
        "    randomize_count = 0\n",
        "\n",
        "    # For each attention head in the model ...\n",
        "    for node in cfg.useful_nodes.nodes:\n",
        "        randomize = False\n",
        "        algo_tags = node.filter_tags(qmi.QType.ALGO.value)\n",
        "\n",
        "        # Some models use token 0 in predictions (possibly as a heuristic that large Dn give a positive answer in subtraction).\n",
        "        # We want an accurate model that does not depend on heuristics\n",
        "        # Randomise all attentions heads at token position zero\n",
        "        if node.position == 0:\n",
        "            randomize = True\n",
        "\n",
        "        # If node does not have any identified subtasks, randomise it\n",
        "        elif len(algo_tags) == 0:\n",
        "            randomize = True\n",
        "\n",
        "        else:\n",
        "            # The SC and MC subtasks are optional as they can be replaced by ST and MT subtasks.\n",
        "            # For each attention head in the model that only does SC and MC subtasks, randomise it\n",
        "            sc_tags = len([s for s in algo_tags if \".SC\" in s])\n",
        "            mc_tags = len([s for s in algo_tags if \".MC\" in s])\n",
        "            if (sc_tags == 1 or mc_tags == 1) and len(algo_tags) == sc_tags + mc_tags:\n",
        "                randomize = True\n",
        "\n",
        "        if randomize:\n",
        "            # PQR TODO\n",
        "            # transfer_one_head(from_model, from_layer_no, from_head_no, to_model, start_layer):\n",
        "            randomize_count += 1\n",
        "\n",
        "    print('Randomized', randomize_count, 'inserted nodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JDY_Fqa_Wfb"
      },
      "source": [
        "# Part 7: Train add/sub/mix main_model with Infinite Data\n",
        "Train main_model for n_training_steps, storing train_losses per epoch.\n",
        "\n",
        "Each training step (of n_training_steps) new training data (a batch of batch_size tokens) is generated and the model is trained and loss calculated on it. No separate \"testing\" data is need   ed, as the training data is unique each step. Memorisation of past training data by the model (if any) is minimally beneficial. For 6 digit addition or subtraction there are 1000 billion possible questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GfZPH4hP8jX"
      },
      "outputs": [],
      "source": [
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkeb6r2RH3mb"
      },
      "outputs": [],
      "source": [
        "# Convert \"mix_d10_l3_h4_t75K_gf_s173289.pth\" to \"mix_d10_l3_h4_t??K_gf_s173289.pth\"\n",
        "def replace_t_value(string, n):\n",
        "    return re.sub(r'(?<=_t)\\d+(?=K_)', str(n), string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ61_nfKxI9c"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "train_losses_list = []\n",
        "batch_op_list = []\n",
        "per_token_train_losses_list = []\n",
        "grokfast_grads = None\n",
        "\n",
        "for epoch in tqdm.tqdm(range(cfg.n_training_steps)):\n",
        "\n",
        "  tokens = next(ds)\n",
        "  logits = cfg.main_model(tokens)\n",
        "\n",
        "  per_token_train_losses_raw, _ = qmi.logits_to_tokens_loss(cfg, logits, tokens)\n",
        "  per_token_train_losses = qmi.loss_fn(per_token_train_losses_raw)\n",
        "  per_token_train_losses_list.append(utils.to_numpy(per_token_train_losses))\n",
        "\n",
        "  train_loss = per_token_train_losses.mean()\n",
        "  train_loss.backward()\n",
        "  train_losses_list.append(train_loss.item())\n",
        "  batch_op_list.append(tokens[0][cfg.n_digits] == mmi.MathsToken.PLUS)\n",
        "\n",
        "  if cfg.grokfast:\n",
        "    ### Option 1: Grokfast-EMA (Preferred. Has argument alpha, lamb)\n",
        "    grads = gradfilter_ema(cfg.main_model, grads=grokfast_grads, alpha=cfg.grokfast_alpha, lamb=cfg.grokfast_lamb)\n",
        "    ### Option 2: Grokfast-MA (Has argument window_size, lamb)\n",
        "    # grads = gradfilter_ma(cfg.main_model, grads=grokfast_grads, window_size=window_size, lamb=cfg.grokfast_lamb)\n",
        "\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    if cfg.insert_mode == 2:\n",
        "      # Freeze the useful attention heads from insert_model every 100 epochs\n",
        "      transfer_useful_heads(insert_model, cfg.main_model)\n",
        "    if cfg.insert_mode == 3:\n",
        "      # Freeze the useful attention heads and MLP layers from insert_model every 100 epochs\n",
        "      insert_existing_model( False )\n",
        "  if cfg.insert_mode == 4:\n",
        "    # Freeze the useful attention heads and MLP layers from insert_model every epoch\n",
        "    insert_existing_model( False )\n",
        "\n",
        "  if epoch % 500 == 0:\n",
        "    print(epoch, train_loss.item())\n",
        "\n",
        "  if epoch % 10000 == 0 and epoch > 0 and epoch < cfg.n_training_steps:\n",
        "      # Save a checkpoint of model\n",
        "      checkpoint_fname_pth = replace_t_value(main_fname_pth, epoch // 1000)\n",
        "      print(\"Saving main model checkpoint to temporary Colab file\", checkpoint_fname_pth)\n",
        "      torch.save(cfg.main_model.state_dict(), checkpoint_fname_pth)\n",
        "\n",
        "print(epoch, train_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwJBrnSJ23eb"
      },
      "outputs": [],
      "source": [
        "cfg.avg_final_loss = round((train_losses_list[-5]+train_losses_list[-4]+train_losses_list[-3]+train_losses_list[-2]+train_losses_list[-1])/5,9)\n",
        "cfg.final_loss = train_losses_list[-1]\n",
        "\n",
        "print( \"AvgFinalLoss\", cfg.avg_final_loss)\n",
        "print( \"FinalLoss\", cfg.final_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgzjamSn2y4j"
      },
      "outputs": [],
      "source": [
        "# These temporary Colab files can be manually downloaded from the Colab \"Files\" tab (at left).\n",
        "# The download can be manually loaded into HuggingFace so the \"QuantaMathsAnalyse\" Colab can access it.\n",
        "\n",
        "print(\"Saving main model to temporary Colab file:\", main_fname_pth)\n",
        "torch.save(cfg.main_model.state_dict(), main_fname_pth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9cFWJINkGyF"
      },
      "outputs": [],
      "source": [
        "extra_data = {\n",
        "    \"Config\": cfg.to_dict(),\n",
        "    \"TrainingLoss\": train_losses_list\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aygOXqmalo71"
      },
      "outputs": [],
      "source": [
        "print( \"Saving main model config etc to temporary Colab file:\", main_fname_json)\n",
        "save_cfg = cfg.to_dict()\n",
        "with open(main_fname_json, 'w') as file:\n",
        "    json.dump(extra_data, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiOdvQFD_fxZ"
      },
      "source": [
        "# Part 9: Line Graphs\n",
        "\n",
        "This section analyses the training loss by graphing it at a high level.\n",
        "\n",
        "The loss curve for all digits show visible inflection points (bumps), but is too high level to help understand the algorithm.\n",
        "\n",
        "When this graph is decomposed into 'per digit' graphs, the interesting distinct 'per digit' curves appear, showing each digit is being refined semi-independently, with the model algorithm refining each digit separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1FfCagWWEM3"
      },
      "outputs": [],
      "source": [
        "steps_to_graph=1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXKLerMH4zLj"
      },
      "outputs": [],
      "source": [
        "if not use_mixed_batches:\n",
        "    add_points = [val if flag else None for val, flag in zip(train_losses_list, batch_op_list)]\n",
        "    sub_points = [val if not flag else None for val, flag in zip(train_losses_list, batch_op_list)]\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=list(range(len(add_points))), y=add_points, mode='markers', name='Addition', marker=dict(color='green')))\n",
        "    fig.add_trace(go.Scatter(x=list(range(len(sub_points))), y=sub_points, mode='markers', name='Subtraction', marker=dict(color='red')))\n",
        "    fig.update_layout(title='Training Loss Graph by operation',\n",
        "                      xaxis_title='Training step',\n",
        "                      yaxis_title='Loss',\n",
        "                      showlegend=True)\n",
        "    qmi.plot_loss_lines_layout(cfg, fig, 14, np.arange(len(add_points)))\n",
        "    fig.show(bbox_inches=\"tight\")\n",
        "    pio.write_image(fig, cfg.model_name + \"_LossByOperation.\" + cfg.graph_file_suffix )\n",
        "\n",
        "    fig.update_layout(title='Training Log Loss Graph by operation',\n",
        "                      xaxis_title='Training step',\n",
        "                      yaxis_title='Log loss',\n",
        "                      showlegend=True)\n",
        "    fig.update_layout(yaxis_type=\"log\")\n",
        "    fig.show(bbox_inches=\"tight\")\n",
        "    pio.write_image(fig, cfg.model_name + \"_LogLossByOperation.\" + cfg.graph_file_suffix )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyT2TFd29at"
      },
      "outputs": [],
      "source": [
        "# Helper function to plot multiple lines\n",
        "def lines(raw_lines_list, x=None, labels=None, xaxis='Step', yaxis='Loss', title = '', log_y=False, all_epochs=True):\n",
        "    global steps_to_graph\n",
        "    global cfg\n",
        "\n",
        "    full_title, fig = qmi.plot_loss_lines(cfg=cfg, steps_to_graph=steps_to_graph, raw_lines_list=raw_lines_list, x=x, labels=labels,\n",
        "                                         xaxis=xaxis, yaxis=yaxis, log_y=log_y, all_epochs=all_epochs,\n",
        "                                         title=title, title_font_size=32, tick_font_size=24)\n",
        "\n",
        "    if cfg.graph_file_suffix != \"\":\n",
        "        filename = full_title.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"&\", \"\").replace(\",\", \"\").replace(\"%\", \"\")  + '.' + cfg.graph_file_suffix\n",
        "        pio.write_image(fig, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBxFUurEo7Rf"
      },
      "outputs": [],
      "source": [
        "title_suffix = 'Digit Loss Curves ' + cfg.file_config_prefix\n",
        "per_token_losses = np.stack(per_token_train_losses_list, axis=0)\n",
        "px.line(utils.to_numpy(train_losses_list), labels={\"x\":\"Step\", \"y\":\"Loss\"}, title=title_suffix).show(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGqcQVQFmZ2h"
      },
      "outputs": [],
      "source": [
        "answer_digits = cfg.n_digits + 1\n",
        "all_epochs = True;\n",
        "for i in range(2):\n",
        "  lines_data = [per_token_losses[:, i] for i in range(answer_digits)]+[train_losses_list]\n",
        "  line_labels = [f'A{cfg.n_digits-j}' for j in range(answer_digits)]+['All']\n",
        "  lines(raw_lines_list=lines_data, labels=line_labels, title='Per digit'+title_suffix, all_epochs=all_epochs, log_y=False)\n",
        "\n",
        "  lines_data = [per_token_losses[:, i] for i in range(answer_digits)]+[train_losses_list]\n",
        "  line_labels = [f'A{cfg.n_digits-j}' for j in range(answer_digits)]+['All']\n",
        "  lines(raw_lines_list=lines_data, labels=line_labels, title='Per digit'+title_suffix, all_epochs=all_epochs, log_y=True)\n",
        "\n",
        "  all_epochs = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlMP_5ltpgrV"
      },
      "outputs": [],
      "source": [
        "for i in range(answer_digits):\n",
        "  print('Final Loss for A' + str(cfg.n_digits-i) + ' is ', per_token_losses[-1, i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgk8m02A_lxB"
      },
      "source": [
        "# Part 10: Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BpOcko53GrI"
      },
      "outputs": [],
      "source": [
        "def make_varied_questions():\n",
        "  q0 = next(ds)\n",
        "  q1 = next(ds)\n",
        "  q2 = next(ds)\n",
        "  q3 = next(ds)\n",
        "\n",
        "  questions = torch.vstack((q0.cuda(), q1.cuda(), q2.cuda(), q3.cuda()))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJayaDiR3PgJ"
      },
      "outputs": [],
      "source": [
        "varied_questions = make_varied_questions()\n",
        "num_varied_questions = varied_questions.shape[0]\n",
        "\n",
        "qmi.a_set_ablate_hooks(cfg)\n",
        "qmi.a_calc_mean_values(cfg, varied_questions)\n",
        "\n",
        "cfg.main_model.reset_hooks()\n",
        "cfg.main_model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = cfg.main_model.run_with_cache(varied_questions.cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGhK1OC_1mD"
      },
      "source": [
        "# Part 11: Attention Patterns\n",
        "Attention patterns show which token(s) the model's attention heads are paying attention to in each token position of the prediction calculation.\n",
        "\n",
        "For the default CoLab set up, the  model has 3 attention heads, and performs 5 digit addition. The attention pattern is 18 by 18 squares (as 54321+77779=132100 is 18 tokens). Time proceeds vertically downwards, with one additional token being revealed horizontally at each token position, giving the overall triangle shape. This visualisation provided insights. After the question is fully revealed (at token position 11), each head starts attending to pairs of question digits from left to right (i.e. high-value digits before lower-value digits) giving the “double staircase\" shape. The three heads attend to a given digit pair in three different token position, giving a time ordering of heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVCSBSE3Xpk"
      },
      "outputs": [],
      "source": [
        "def show_token_attention_patterns(index, layer, token_at_index, use_case):\n",
        "\n",
        "  the_tokens = [str(token) for token in token_at_index.tolist()]\n",
        "  if layer == 0:\n",
        "    tokens_str = qmi.tokens_to_string(cfg, token_at_index)\n",
        "    print(\"Attention patterns for\", tokens_str)\n",
        "\n",
        "  attention_pattern=sample_cache[\"pattern\", layer, \"attn\"][index]\n",
        "  display(cv.attention.attention_patterns(\n",
        "      tokens=the_tokens,\n",
        "      attention=attention_pattern,\n",
        "      #attention_head_names=[f\"L{layer}H{i}\" for i in range(cfg.n_heads)],\n",
        "  ))\n",
        "\n",
        "\n",
        "sample_size = 3\n",
        "\n",
        "# Show attention patterns for some randomly chosen tokens\n",
        "for i in range(sample_size):\n",
        "  for layer in range(cfg.n_layers):\n",
        "    show_token_attention_patterns(i, layer, tokens[i], \"Misc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUCXw1k93a20"
      },
      "outputs": [],
      "source": [
        "if cfg.graph_file_suffix != \"\":\n",
        "\n",
        "  tokens_str = []\n",
        "  for i in range(cfg.n_heads):\n",
        "    one_token_str = []\n",
        "    for j in tokens[i]:\n",
        "      one_token_str.append(str(utils.to_numpy(j)))\n",
        "    tokens_str.append(one_token_str)\n",
        "\n",
        "  # Refer https://github.com/callummcdougall/CircuitsVis/blob/main/python/circuitsvis/circuitsvis_demo.ipynb\n",
        "\n",
        "  # html_object = cv.attention.from_cache(\n",
        "  #    cache = sample_cache,\n",
        "  #    tokens = tokens_str, # list of list of strings\n",
        "  #    return_mode = \"html\",\n",
        "  #)\n",
        "\n",
        "  # Create a CoLab file containing the attention pattern(s) in HTML\n",
        "  #filename = \"AttentionPattern\" + str(cfg.n_digits) + \"Digits\" + str(cfg.n_heads) + \"Heads.html\"\n",
        "  #with open(filename, \"w\") as f:\n",
        "  #    f.write(html_object.data)\n",
        "\n",
        "  # Manually download the CoLab \"html\" file and open in your local browser.\n",
        "  # Install and use the Edge extension \"FireShot\" to save a portion of the HTML page as a PDF"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F5K2kA0L_FHc"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}