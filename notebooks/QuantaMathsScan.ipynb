{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab604d1a",
   "metadata": {},
   "source": [
    "# Quanta Maths: Integer Addition and Subtraction in Transformers. Scan 200+ LLMs\n",
    "\n",
    "This Colab uses the app.withmartian.com API to test 200+ models\n",
    "to see whether given query like \"Answer concisely: 4444+5559=\" they answer contains \"10003\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fade73",
   "metadata": {},
   "source": [
    "# Martian AI Model Names List\n",
    "Extracted from https://app.withmartian.com/docs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6de66cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models: 225\n",
      "First 5 models: ['martian/code', 'agentica-org/deepcoder-14b-preview', 'ai21/jamba-1.6-large', 'ai21/jamba-1.6-mini', 'aion-labs/aion-rp-llama-3.1-8b']\n",
      "Last 5 models: ['x-ai/grok-3', 'x-ai/grok-3-beta', 'x-ai/grok-3-mini', 'x-ai/grok-3-mini-beta', 'x-ai/grok-4']\n",
      "Number of providers: 51\n",
      "Models per provider:\n",
      "  agentica-org: 1 models\n",
      "  ai21: 2 models\n",
      "  aion-labs: 1 models\n",
      "  alfredpros: 1 models\n",
      "  alpindale: 1 models\n",
      "  amazon: 3 models\n",
      "  anthracite-org: 1 models\n",
      "  anthropic: 9 models\n",
      "  arcee-ai: 5 models\n",
      "  arliai: 1 models\n",
      "  baidu: 1 models\n",
      "  bytedance: 1 models\n",
      "  cognitivecomputations: 2 models\n",
      "  cohere: 9 models\n",
      "  deepseek: 12 models\n",
      "  eleutherai: 1 models\n",
      "  eva-unit-01: 1 models\n",
      "  google: 11 models\n",
      "  gryphe: 1 models\n",
      "  inception: 2 models\n",
      "  infermatic: 1 models\n",
      "  liquid: 3 models\n",
      "  mancer: 1 models\n",
      "  martian: 1 models\n",
      "  meta-llama: 15 models\n",
      "  microsoft: 7 models\n",
      "  minimax: 1 models\n",
      "  mistralai: 27 models\n",
      "  moonshotai: 2 models\n",
      "  morph: 3 models\n",
      "  neversleep: 3 models\n",
      "  nothingiisreal: 1 models\n",
      "  nousresearch: 5 models\n",
      "  nvidia: 3 models\n",
      "  openai: 29 models\n",
      "  opengvlab: 1 models\n",
      "  perplexity: 6 models\n",
      "  pygmalionai: 1 models\n",
      "  qwen: 19 models\n",
      "  raifle: 1 models\n",
      "  sao10k: 5 models\n",
      "  sarvamai: 1 models\n",
      "  shisa-ai: 1 models\n",
      "  sophosympatheia: 1 models\n",
      "  switchpoint: 1 models\n",
      "  tencent: 1 models\n",
      "  thedrummer: 6 models\n",
      "  thudm: 3 models\n",
      "  tngtech: 1 models\n",
      "  undi95: 2 models\n",
      "  x-ai: 7 models\n"
     ]
    }
   ],
   "source": [
    "martian_models = [\n",
    "    \"martian/code\",\n",
    "    \"agentica-org/deepcoder-14b-preview\",\n",
    "    \"ai21/jamba-1.6-large\",\n",
    "    \"ai21/jamba-1.6-mini\",\n",
    "    \"aion-labs/aion-rp-llama-3.1-8b\",\n",
    "    \"alfredpros/codellama-7b-instruct-solidity\",\n",
    "    \"alpindale/goliath-120b\",\n",
    "    \"amazon/nova-lite-v1\",\n",
    "    \"amazon/nova-micro-v1\",\n",
    "    \"amazon/nova-pro-v1\",\n",
    "    \"anthracite-org/magnum-v4-72b\",\n",
    "    \"anthropic/claude-3-5-haiku-20241022\",\n",
    "    \"anthropic/claude-3-5-sonnet-20240620\",\n",
    "    \"anthropic/claude-3-5-sonnet-20241022\",\n",
    "    \"anthropic/claude-3-7-sonnet-20250219\",\n",
    "    \"anthropic/claude-3-haiku-20240307\",\n",
    "    \"anthropic/claude-3-opus-20240229\",\n",
    "    \"anthropic/claude-opus-4-1-20250805\",\n",
    "    \"anthropic/claude-opus-4-20250514\",\n",
    "    \"anthropic/claude-sonnet-4-20250514\",\n",
    "    \"arcee-ai/arcee-blitz\",\n",
    "    \"arcee-ai/caller-large\",\n",
    "    \"arcee-ai/coder-large\",\n",
    "    \"arcee-ai/maestro-reasoning\",\n",
    "    \"arcee-ai/virtuoso-large\",\n",
    "    \"arliai/qwq-32b-arliai-rpr-v1\",\n",
    "    \"baidu/ernie-4.5-300b-a47b\",\n",
    "    \"bytedance/ui-tars-1.5-7b\",\n",
    "    \"cognitivecomputations/dolphin-mixtral-8x22b\",\n",
    "    \"cognitivecomputations/dolphin3.0-r1-mistral-24b\",\n",
    "    \"cohere/command\",\n",
    "    \"cohere/command-a\",\n",
    "    \"cohere/command-r\",\n",
    "    \"cohere/command-r-03-2024\",\n",
    "    \"cohere/command-r-08-2024\",\n",
    "    \"cohere/command-r-plus\",\n",
    "    \"cohere/command-r-plus-04-2024\",\n",
    "    \"cohere/command-r-plus-08-2024\",\n",
    "    \"cohere/command-r7b-12-2024\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "    \"deepseek/deepseek-prover-v2\",\n",
    "    \"deepseek/deepseek-r1\",\n",
    "    \"deepseek/deepseek-r1-0528\",\n",
    "    \"deepseek/deepseek-r1-0528-qwen3-8b\",\n",
    "    \"deepseek/deepseek-r1-distill-llama-70b\",\n",
    "    \"deepseek/deepseek-r1-distill-llama-8b\",\n",
    "    \"deepseek/deepseek-r1-distill-qwen-1.5b\",\n",
    "    \"deepseek/deepseek-r1-distill-qwen-14b\",\n",
    "    \"deepseek/deepseek-r1-distill-qwen-32b\",\n",
    "    \"deepseek/deepseek-r1-distill-qwen-7b\",\n",
    "    \"eleutherai/llemma_7b\",\n",
    "    \"eva-unit-01/eva-qwen-2.5-72b\",\n",
    "    \"google/gemini-2.0-flash-001\",\n",
    "    \"google/gemini-2.0-flash-lite-001\",\n",
    "    \"google/gemini-2.5-flash\",\n",
    "    \"google/gemini-2.5-flash-lite\",\n",
    "    \"google/gemini-2.5-flash-lite-preview-06-17\",\n",
    "    \"google/gemini-2.5-pro\",\n",
    "    \"google/gemini-2.5-pro-preview-05-06\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3n-e4b-it\",\n",
    "    \"gryphe/mythomax-l2-13b\",\n",
    "    \"inception/mercury\",\n",
    "    \"inception/mercury-coder\",\n",
    "    \"infermatic/mn-inferor-12b\",\n",
    "    \"liquid/lfm-3b\",\n",
    "    \"liquid/lfm-40b\",\n",
    "    \"liquid/lfm-7b\",\n",
    "    \"mancer/weaver\",\n",
    "    \"meta-llama/llama-3-70b-instruct\",\n",
    "    \"meta-llama/llama-3-8b-instruct\",\n",
    "    \"meta-llama/llama-3.1-405b\",\n",
    "    \"meta-llama/llama-3.1-70b-instruct\",\n",
    "    \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
    "    \"meta-llama/llama-3.2-1b-instruct\",\n",
    "    \"meta-llama/llama-3.2-3b-instruct\",\n",
    "    \"meta-llama/llama-3.2-90b-vision-instruct\",\n",
    "    \"meta-llama/llama-3.3-70b-instruct\",\n",
    "    \"meta-llama/llama-4-maverick\",\n",
    "    \"meta-llama/llama-4-scout\",\n",
    "    \"meta-llama/llama-guard-2-8b\",\n",
    "    \"meta-llama/llama-guard-3-8b\",\n",
    "    \"meta-llama/llama-guard-4-12b\",\n",
    "    \"microsoft/phi-3-medium-128k-instruct\",\n",
    "    \"microsoft/phi-3-mini-128k-instruct\",\n",
    "    \"microsoft/phi-3.5-mini-128k-instruct\",\n",
    "    \"microsoft/phi-4\",\n",
    "    \"microsoft/phi-4-multimodal-instruct\",\n",
    "    \"microsoft/phi-4-reasoning-plus\",\n",
    "    \"microsoft/wizardlm-2-8x22b\",\n",
    "    \"minimax/minimax-m1\",\n",
    "    \"mistralai/codestral-2501\",\n",
    "    \"mistralai/devstral-medium\",\n",
    "    \"mistralai/devstral-small\",\n",
    "    \"mistralai/devstral-small-2505\",\n",
    "    \"mistralai/magistral-medium-2506\",\n",
    "    \"mistralai/magistral-medium-2506:thinking\",\n",
    "    \"mistralai/magistral-small-2506\",\n",
    "    \"mistralai/ministral-3b\",\n",
    "    \"mistralai/ministral-8b\",\n",
    "    \"mistralai/mistral-7b-instruct\",\n",
    "    \"mistralai/mistral-7b-instruct-v0.1\",\n",
    "    \"mistralai/mistral-7b-instruct-v0.2\",\n",
    "    \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "    \"mistralai/mistral-large\",\n",
    "    \"mistralai/mistral-large-2407\",\n",
    "    \"mistralai/mistral-large-2411\",\n",
    "    \"mistralai/mistral-medium-3\",\n",
    "    \"mistralai/mistral-nemo\",\n",
    "    \"mistralai/mistral-saba\",\n",
    "    \"mistralai/mistral-small\",\n",
    "    \"mistralai/mistral-small-24b-instruct-2501\",\n",
    "    \"mistralai/mistral-small-3.1-24b-instruct\",\n",
    "    \"mistralai/mistral-small-3.2-24b-instruct\",\n",
    "    \"mistralai/mistral-tiny\",\n",
    "    \"mistralai/mixtral-8x22b-instruct\",\n",
    "    \"mistralai/mixtral-8x7b-instruct\",\n",
    "    \"mistralai/pixtral-large-2411\",\n",
    "    \"moonshotai/kimi-k2\",\n",
    "    \"moonshotai/kimi-vl-a3b-thinking\",\n",
    "    \"morph/morph-v2\",\n",
    "    \"morph/morph-v3-fast\",\n",
    "    \"morph/morph-v3-large\",\n",
    "    \"neversleep/llama-3-lumimaid-70b\",\n",
    "    \"neversleep/llama-3.1-lumimaid-8b\",\n",
    "    \"neversleep/noromaid-20b\",\n",
    "    \"nothingiisreal/mn-celeste-12b\",\n",
    "    \"nousresearch/deephermes-3-mistral-24b-preview\",\n",
    "    \"nousresearch/hermes-2-pro-llama-3-8b\",\n",
    "    \"nousresearch/hermes-3-llama-3.1-405b\",\n",
    "    \"nousresearch/hermes-3-llama-3.1-70b\",\n",
    "    \"nousresearch/nous-hermes-2-mixtral-8x7b-dpo\",\n",
    "    \"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "    \"nvidia/llama-3.1-nemotron-ultra-253b-v1\",\n",
    "    \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "    \"openai/chatgpt-4o-latest\",\n",
    "    \"openai/gpt-3.5-turbo\",\n",
    "    \"openai/gpt-3.5-turbo-16k\",\n",
    "    \"openai/gpt-4\",\n",
    "    \"openai/gpt-4-0314\",\n",
    "    \"openai/gpt-4-1106-preview\",\n",
    "    \"openai/gpt-4-turbo\",\n",
    "    \"openai/gpt-4-turbo-preview\",\n",
    "    \"openai/gpt-4.1\",\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    \"openai/gpt-4.1-nano\",\n",
    "    \"openai/gpt-4o\",\n",
    "    \"openai/gpt-4o-2024-05-13\",\n",
    "    \"openai/gpt-4o-2024-08-06\",\n",
    "    \"openai/gpt-4o-2024-11-20\",\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"openai/gpt-4o-mini-2024-07-18\",\n",
    "    \"openai/gpt-4o-mini-search-preview\",\n",
    "    \"openai/gpt-4o-search-preview\",\n",
    "    \"openai/gpt-5\",\n",
    "    \"openai/gpt-5-mini\",\n",
    "    \"openai/gpt-5-nano\",\n",
    "    \"openai/o1\",\n",
    "    \"openai/o1-mini\",\n",
    "    \"openai/o1-mini-2024-09-12\",\n",
    "    \"openai/o1-preview\",\n",
    "    \"openai/o1-preview-2024-09-12\",\n",
    "    \"openai/o3-mini\",\n",
    "    \"openai/o4-mini\",\n",
    "    \"opengvlab/internvl3-14b\",\n",
    "    \"perplexity/r1-1776\",\n",
    "    \"perplexity/sonar\",\n",
    "    \"perplexity/sonar-deep-research\",\n",
    "    \"perplexity/sonar-pro\",\n",
    "    \"perplexity/sonar-reasoning\",\n",
    "    \"perplexity/sonar-reasoning-pro\",\n",
    "    \"pygmalionai/mythalion-13b\",\n",
    "    \"qwen/qwen-2-72b-instruct\",\n",
    "    \"qwen/qwen-2.5-72b-instruct\",\n",
    "    \"qwen/qwen-2.5-7b-instruct\",\n",
    "    \"qwen/qwen-2.5-coder-32b-instruct\",\n",
    "    \"qwen/qwen-2.5-vl-7b-instruct\",\n",
    "    \"qwen/qwen-max\",\n",
    "    \"qwen/qwen-plus\",\n",
    "    \"qwen/qwen-turbo\",\n",
    "    \"qwen/qwen-vl-max\",\n",
    "    \"qwen/qwen2.5-vl-32b-instruct\",\n",
    "    \"qwen/qwen2.5-vl-72b-instruct\",\n",
    "    \"qwen/qwen3-14b\",\n",
    "    \"qwen/qwen3-235b-a22b\",\n",
    "    \"qwen/qwen3-235b-a22b-07-25\",\n",
    "    \"qwen/qwen3-30b-a3b\",\n",
    "    \"qwen/qwen3-32b\",\n",
    "    \"qwen/qwen3-8b\",\n",
    "    \"qwen/qwen3-coder\",\n",
    "    \"qwen/qwq-32b\",\n",
    "    \"raifle/sorcererlm-8x22b\",\n",
    "    \"sao10k/fimbulvetr-11b-v2\",\n",
    "    \"sao10k/l3-euryale-70b\",\n",
    "    \"sao10k/l3-lunaris-8b\",\n",
    "    \"sao10k/l3.1-euryale-70b\",\n",
    "    \"sao10k/l3.3-euryale-70b\",\n",
    "    \"sarvamai/sarvam-m\",\n",
    "    \"shisa-ai/shisa-v2-llama3.3-70b\",\n",
    "    \"sophosympatheia/midnight-rose-70b\",\n",
    "    \"switchpoint/router\",\n",
    "    \"tencent/hunyuan-a13b-instruct\",\n",
    "    \"thedrummer/anubis-70b-v1.1\",\n",
    "    \"thedrummer/anubis-pro-105b-v1\",\n",
    "    \"thedrummer/rocinante-12b\",\n",
    "    \"thedrummer/skyfall-36b-v2\",\n",
    "    \"thedrummer/unslopnemo-12b\",\n",
    "    \"thedrummer/valkyrie-49b-v1\",\n",
    "    \"thudm/glm-4-32b\",\n",
    "    \"thudm/glm-4.1v-9b-thinking\",\n",
    "    \"thudm/glm-z1-32b\",\n",
    "    \"tngtech/deepseek-r1t2-chimera\",\n",
    "    \"undi95/remm-slerp-l2-13b\",\n",
    "    \"undi95/toppy-m-7b\",\n",
    "    \"x-ai/grok-2-1212\",\n",
    "    \"x-ai/grok-2-vision-1212\",\n",
    "    \"x-ai/grok-3\",\n",
    "    \"x-ai/grok-3-beta\",\n",
    "    \"x-ai/grok-3-mini\",\n",
    "    \"x-ai/grok-3-mini-beta\",\n",
    "    \"x-ai/grok-4\",\n",
    "]\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total number of models: {len(martian_models)}\")\n",
    "print(f\"First 5 models: {martian_models[:5]}\")\n",
    "print(f\"Last 5 models: {martian_models[-5:]}\")\n",
    "\n",
    "# Group by provider\n",
    "providers = {}\n",
    "for model in martian_models:\n",
    "    provider = model.split('/')[0]\n",
    "    if provider not in providers:\n",
    "        providers[provider] = []\n",
    "    providers[provider].append(model)\n",
    "\n",
    "print(f\"Number of providers: {len(providers)}\")\n",
    "print(\"Models per provider:\")\n",
    "for provider, models in sorted(providers.items()):\n",
    "    print(f\"  {provider}: {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a10a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import time\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
    "assert MARTIAN_API_KEY, \"API key not found. Please set MARTIAN_API_KEY in your .env file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.withmartian.com/v1\",\n",
    "    api_key=MARTIAN_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7796000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ground_truth_correct(answer, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns True if the ground_truth appears as the final number in the answer, ignoring whitespace and punctuation.\n",
    "    Accepts answers like '13', '13.', '13**', 'The answer is 13', '**13**', etc.\n",
    "    \"\"\"\n",
    "    # Remove trailing whitespace and punctuation\n",
    "    answer_clean = answer.strip().rstrip('.!**')\n",
    "    # Find all numbers in the answer\n",
    "    numbers = re.findall(r'\\d+', answer_clean)\n",
    "    # Check for markdown-style **13** or last number match\n",
    "    return (f\"**{ground_truth}**\" in answer or bool(numbers and numbers[-1] == ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa7ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_inference( model_name, prompt, ground_truth):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        max_tokens=1000,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    success = is_ground_truth_correct(answer, ground_truth)\n",
    "\n",
    "    return answer, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df690503",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbreak\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(the_models, prompt, ground_truth, verbose=False):\n",
    "    results = []\n",
    "    print(f\"Testing {len(the_models)} models...\")\n",
    "    \n",
    "    def call_model(model_name):\n",
    "        try:\n",
    "            answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"success\": success,\n",
    "                \"response\": answer if not success else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"success\": False,\n",
    "                \"response\": f\"Error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        for idx, model_name in enumerate(the_models):\n",
    "            future = executor.submit(call_model, model_name)\n",
    "            try:\n",
    "                result = future.result(timeout=10)\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                result = {\n",
    "                    \"model\": model_name,\n",
    "                    \"success\": False,\n",
    "                    \"response\": \"Timeout after 10 seconds\"\n",
    "                }\n",
    "            results.append(result)\n",
    "            print(f\"[{idx+1}/{len(the_models)}] {model_name}: {'SUCCESS' if result['success'] else 'FAIL:'+str(result['response'])}\")\n",
    "    \n",
    "    successes = [r['model'] for r in results if r['success']]\n",
    "    failures = [{\"model\": r['model'], \"response\": r['response']} for r in results if not r['success']]\n",
    "    print(f\"\\nTotal Successes: {len(successes)}\")\n",
    "    print(f\"Total Failures: {len(failures)}\")\n",
    "    return {\"successes\": successes, \"failures\": failures}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_progressive(the_models, tests, max_workers=8):\n",
    "    model_scores = []\n",
    "\n",
    "    def score_model(model_name):\n",
    "        score = 0\n",
    "        for test_idx, (prompt, ground_truth) in enumerate(tests):\n",
    "            try:\n",
    "                answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
    "                if success:\n",
    "                    score = test_idx + 1\n",
    "                else:\n",
    "                    break\n",
    "            except openai.APIError as e:\n",
    "                if hasattr(e, 'status_code'):\n",
    "                    score = -e.status_code\n",
    "                else:\n",
    "                    score = -999\n",
    "                break\n",
    "            except Exception as e:\n",
    "                score = -999\n",
    "                break\n",
    "        return {\"model\": model_name, \"score\": score}\n",
    "\n",
    "    print(f\"Evaluating {len(the_models)} models concurrently with {max_workers} workers...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_model = {executor.submit(score_model, model_name): model_name for model_name in the_models}\n",
    "        for idx, future in enumerate(as_completed(future_to_model), 1):\n",
    "            model_name = future_to_model[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = {\"model\": model_name, \"score\": -999}\n",
    "            print(f\"[{idx}/{len(the_models)}] {result['model']}: Score = {result['score']}\")\n",
    "            model_scores.append(result)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d13382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    [\"Answer concisely: 4+9=\", \"13\"],\n",
    "    [\"Answer concisely: 44+59=\", \"103\"],\n",
    "    [\"Answer concisely: 444+559=\", \"1003\"],\n",
    "    [\"Answer concisely: 4444+5559=\", \"10003\"],\n",
    "    [\"Answer concisely: 44444+55559=\", \"100003\"],\n",
    "    [\"Answer concisely: 444444+555559=\", \"1000003\"],\n",
    "    [\"Answer concisely: 4444444+5555559=\", \"10000003\"],\n",
    "    [\"Answer concisely: 44444444+55555559=\", \"100000003\"],\n",
    "    [\"Answer concisely: 444444444+555555559=\", \"1000000003\"],\n",
    "    [\"Answer concisely: 4444444444+5555555559=\", \"10000000003\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a759f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 225 models concurrently with 8 workers...\n",
      "[1/225] undi95/toppy-m-7b: Score = -400\n",
      "[2/225] meta-llama/llama-3.1-8b-instruct: Score = -999\n",
      "[3/225] liquid/lfm-3b: Score = -999\n",
      "[4/225] cohere/command-r-plus: Score = -999\n",
      "[5/225] thedrummer/skyfall-36b-v2: Score = -999\n",
      "[6/225] nousresearch/hermes-3-llama-3.1-70b: Score = -999\n",
      "[7/225] sao10k/l3-lunaris-8b: Score = -999\n",
      "[8/225] sao10k/l3-euryale-70b: Score = -999\n",
      "[9/225] meta-llama/llama-3.3-70b-instruct: Score = -999\n",
      "[10/225] openai/o3-mini: Score = -999\n",
      "[11/225] undi95/remm-slerp-l2-13b: Score = -999\n",
      "[12/225] meta-llama/llama-4-maverick: Score = -999\n",
      "[13/225] bytedance/ui-tars-1.5-7b: Score = -999\n",
      "[14/225] baidu/ernie-4.5-300b-a47b: Score = -999\n",
      "[15/225] mistralai/devstral-medium: Score = -999\n",
      "[16/225] meta-llama/llama-3.2-90b-vision-instruct: Score = -999\n",
      "[17/225] shisa-ai/shisa-v2-llama3.3-70b: Score = -502\n",
      "[18/225] mistralai/mistral-small-3.1-24b-instruct: Score = -999\n",
      "[19/225] liquid/lfm-40b: Score = -400\n",
      "[20/225] qwen/qwen3-32b: Score = -999\n",
      "[21/225] meta-llama/llama-3.1-70b-instruct: Score = -999\n",
      "[22/225] google/gemma-3n-e4b-it: Score = -999\n",
      "[23/225] openai/gpt-4o-mini-2024-07-18: Score = -999\n",
      "[24/225] moonshotai/kimi-k2: Score = -999\n",
      "[25/225] mistralai/mistral-large: Score = -999\n",
      "[26/225] microsoft/phi-3-mini-128k-instruct: Score = -999\n",
      "[27/225] openai/gpt-4o-search-preview: Score = -999\n",
      "[28/225] openai/gpt-4.1: Score = -999\n",
      "[29/225] openai/gpt-4o: Score = -999\n",
      "[30/225] nvidia/llama-3.1-nemotron-ultra-253b-v1: Score = -999\n",
      "[31/225] agentica-org/deepcoder-14b-preview: Score = -502\n",
      "[32/225] inception/mercury: Score = -999\n",
      "[33/225] cohere/command-r7b-12-2024: Score = -999\n",
      "[34/225] amazon/nova-micro-v1: Score = -999\n",
      "[35/225] anthropic/claude-3-5-sonnet-20241022: Score = -999\n",
      "[36/225] anthropic/claude-opus-4-20250514: Score = -999\n",
      "[37/225] qwen/qwen-2.5-7b-instruct: Score = -999\n",
      "[38/225] google/gemini-2.0-flash-lite-001: Score = -999\n",
      "[39/225] mistralai/mistral-small: Score = -999\n",
      "[40/225] microsoft/phi-4-multimodal-instruct: Score = -999\n",
      "[41/225] mistralai/mistral-saba: Score = -999\n",
      "[42/225] x-ai/grok-2-vision-1212: Score = -999\n",
      "[43/225] deepseek/deepseek-prover-v2: Score = -999\n",
      "[44/225] thedrummer/valkyrie-49b-v1: Score = -400\n",
      "[45/225] deepseek/deepseek-chat-v3-0324: Score = -999\n",
      "[46/225] mistralai/ministral-8b: Score = -999\n",
      "[47/225] mistralai/magistral-medium-2506: Score = -999\n",
      "[48/225] qwen/qwen-turbo: Score = -999\n",
      "[49/225] anthropic/claude-3-5-sonnet-20240620: Score = -999\n",
      "[50/225] neversleep/noromaid-20b: Score = -999\n",
      "[51/225] thedrummer/unslopnemo-12b: Score = -999\n",
      "[52/225] meta-llama/llama-3-8b-instruct: Score = -999\n",
      "[53/225] meta-llama/llama-3.2-1b-instruct: Score = -999\n",
      "[54/225] meta-llama/llama-3.1-405b: Score = -999\n",
      "[55/225] ai21/jamba-1.6-mini: Score = -400\n",
      "[56/225] mistralai/ministral-3b: Score = -999\n",
      "[57/225] qwen/qwen-vl-max: Score = -999\n",
      "[58/225] anthracite-org/magnum-v4-72b: Score = -999\n",
      "[59/225] cognitivecomputations/dolphin3.0-r1-mistral-24b: Score = -502\n",
      "[60/225] anthropic/claude-3-5-haiku-20241022: Score = -999\n",
      "[61/225] eva-unit-01/eva-qwen-2.5-72b: Score = -400\n",
      "[62/225] arcee-ai/arcee-blitz: Score = -400\n",
      "[63/225] opengvlab/internvl3-14b: Score = -502\n",
      "[64/225] qwen/qwen2.5-vl-32b-instruct: Score = -999\n",
      "[65/225] mistralai/mixtral-8x7b-instruct: Score = -999\n",
      "[66/225] amazon/nova-lite-v1: Score = -999\n",
      "[67/225] sao10k/l3.1-euryale-70b: Score = -999\n",
      "[68/225] qwen/qwen-2.5-coder-32b-instruct: Score = -999\n",
      "[69/225] cohere/command-r-plus-04-2024: Score = -999\n",
      "[70/225] deepseek/deepseek-r1-distill-llama-8b: Score = -999\n",
      "[71/225] openai/o1-mini-2024-09-12: Score = -999\n",
      "[72/225] x-ai/grok-3-mini: Score = -999\n",
      "[73/225] tencent/hunyuan-a13b-instruct: Score = -999\n",
      "[74/225] openai/gpt-5: Score = -999\n",
      "[75/225] nousresearch/hermes-3-llama-3.1-405b: Score = -999\n",
      "[76/225] infermatic/mn-inferor-12b: Score = -999\n",
      "[77/225] cohere/command-r: Score = -999\n",
      "[78/225] nvidia/llama-3.3-nemotron-super-49b-v1: Score = -999\n",
      "[79/225] qwen/qwen3-coder: Score = -999\n",
      "[80/225] openai/gpt-5-nano: Score = -999\n",
      "[81/225] google/gemini-2.5-pro-preview-05-06: Score = -999\n",
      "[82/225] openai/gpt-3.5-turbo-16k: Score = -999\n",
      "[83/225] openai/o1-preview: Score = -400\n",
      "[84/225] anthropic/claude-3-7-sonnet-20250219: Score = -999\n",
      "[85/225] aion-labs/aion-rp-llama-3.1-8b: Score = -999\n",
      "[86/225] mistralai/mistral-small-3.2-24b-instruct: Score = -999\n",
      "[87/225] microsoft/wizardlm-2-8x22b: Score = -999\n",
      "[88/225] cohere/command-r-03-2024: Score = -999\n",
      "[89/225] perplexity/sonar-reasoning: Score = -999\n",
      "[90/225] meta-llama/llama-3-70b-instruct: Score = -999\n",
      "[91/225] mistralai/mistral-medium-3: Score = -999\n",
      "[92/225] thedrummer/anubis-pro-105b-v1: Score = -999\n",
      "[93/225] arliai/qwq-32b-arliai-rpr-v1: Score = -502\n",
      "[94/225] thedrummer/anubis-70b-v1.1: Score = -999\n",
      "[95/225] deepseek/deepseek-r1: Score = -999\n",
      "[96/225] cohere/command: Score = -999\n",
      "[97/225] tngtech/deepseek-r1t2-chimera: Score = -400\n",
      "[98/225] thudm/glm-z1-32b: Score = -502\n",
      "[99/225] qwen/qwq-32b: Score = -999\n",
      "[100/225] qwen/qwen-2.5-vl-7b-instruct: Score = -999\n",
      "[101/225] deepseek/deepseek-r1-distill-llama-70b: Score = -999\n",
      "[102/225] sophosympatheia/midnight-rose-70b: Score = -999\n",
      "[103/225] qwen/qwen3-235b-a22b-07-25: Score = -400\n",
      "[104/225] openai/gpt-4o-2024-11-20: Score = -999\n"
     ]
    }
   ],
   "source": [
    "the_models = list(set(martian_models))\n",
    "model_scores = evaluate_models_progressive(the_models, tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d238c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    " \n",
    "# Summarize number of models by score\n",
    "print(f\"Total number of models: {len(martian_models)}\")\n",
    "score_counts = Counter([m['score'] for m in model_scores])\n",
    "print(\"Score summary (number of models by score):\")\n",
    "for score, count in sorted(score_counts.items()):\n",
    "    print(f\"Score {score}: {count} models\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the results with categorical x-axis for scores\n",
    "scores_sorted = sorted(score_counts.keys(), key=lambda x: (isinstance(x, int), x))\n",
    "counts_sorted = [score_counts[s] for s in scores_sorted]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(len(scores_sorted)), counts_sorted, color='skyblue')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.title('Number of Models by Score')\n",
    "plt.xticks(range(len(scores_sorted)), [str(s) for s in scores_sorted])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4675d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models by score\n",
    "score_to_models = {}\n",
    "for entry in model_scores:\n",
    "    score_to_models.setdefault(entry['score'], []).append(entry['model'])\n",
    " \n",
    "for score in sorted(score_to_models):\n",
    "    print(f\"\\nModels with score {score}:\")\n",
    "    for model in score_to_models[score]:\n",
    "        print(f\"  {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf101ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run models that scored 0 on test[0] and show their outputs\n",
    "zero_score_models = [m['model'] for m in model_scores if m['score'] == 0]\n",
    "print(f\"Re-running {len(zero_score_models)} models that scored 0 on the first test...\")\n",
    "outputs = []\n",
    "for model_name in zero_score_models:\n",
    "    try:\n",
    "        answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {str(e)}\"\n",
    "        success = False\n",
    "    outputs.append({'model': model_name, 'output': answer, 'success': success})\n",
    "    print(f\"Model: {model_name}\\nOutput: {answer}\\nSuccess: {success}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173192cb",
   "metadata": {},
   "source": [
    "# High Variability\n",
    "\"alfredpros/codellama-7b-instruct-solidity\" has high variability. Sometimes correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b587aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alfredpros/codellama-7b-instruct-solidity\"\n",
    "for i in range(5):\n",
    "    answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "    print(f\"Success: {success}. Output: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5228270",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'contains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#model_name = \"meta-llama/llama-guard-2-8b\"\u001b[39;00m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mperplexity/sonar-pro\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m answer, success = \u001b[43mrun_model_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtests\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtests\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_model_inference\u001b[39m\u001b[34m(model_name, prompt, ground_truth)\u001b[39m\n\u001b[32m      2\u001b[39m response = client.chat.completions.create(\n\u001b[32m      3\u001b[39m     model=model_name,\n\u001b[32m      4\u001b[39m     max_tokens=\u001b[32m1000\u001b[39m,\n\u001b[32m      5\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m answer = response.choices[\u001b[32m0\u001b[39m].message.content.strip()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m success = \u001b[43mis_ground_truth_correct\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer, success\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mis_ground_truth_correct\u001b[39m\u001b[34m(answer, ground_truth)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Find all numbers in the answer\u001b[39;00m\n\u001b[32m      9\u001b[39m numbers = re.findall(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m'\u001b[39m, answer_clean)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43manswer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m**\u001b[39m\u001b[33m\"\u001b[39m+ground_truth+\u001b[33m\"\u001b[39m\u001b[33m**\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Check if the last number matches ground_truth\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mbool\u001b[39m(numbers \u001b[38;5;129;01mand\u001b[39;00m numbers[-\u001b[32m1\u001b[39m] == ground_truth))\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'contains'"
     ]
    }
   ],
   "source": [
    "#model_name = \"meta-llama/llama-guard-2-8b\"\n",
    "model_name = \"perplexity/sonar-pro\"\n",
    "\n",
    "answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "print(f\"Model: {model_name}\\nOutput: {answer}\\nSuccess: {success}\\n{'-'*40}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
