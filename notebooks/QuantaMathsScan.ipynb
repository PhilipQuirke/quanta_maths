{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab604d1a",
   "metadata": {},
   "source": [
    "# Quanta Maths: Integer Addition and Subtraction in Transformers. Scan 200+ LLMs\n",
    "\n",
    "This Colab uses the app.withmartian.com API to test 200+ models\n",
    "to see whether given query like \"Answer concisely: 4444+5559=\" they answer \"10003\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-obvious cascading carry tests\n",
    "# Units column ≥10, all other columns sum to exactly 9\n",
    "tests = [\n",
    "    ['Answer concisely: 6+5=', '11'],\n",
    "    ['Answer concisely: 19+87=', '106'],\n",
    "    ['Answer concisely: 774+229=', '1003'],\n",
    "    ['Answer concisely: 6587+3416=', '10003'],\n",
    "    ['Answer concisely: 22605+77398=', '100003'],\n",
    "    ['Answer concisely: 532847+467159=', '1000006'],\n",
    "    ['Answer concisely: 5613709+4386294=', '10000003'],\n",
    "    ['Answer concisely: 72582383+27417619=', '100000002'],\n",
    "    ['Answer concisely: 206727644+793272359=', '1000000003'],\n",
    "    ['Answer concisely: 7580116456+2419883549=', '10000000005'],\n",
    "    ['Answer concisely: 52449010267+47550989737=', '100000000004'],\n",
    "    ['Answer concisely: 888522030597+111477969406=', '1000000000003'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ground_truth_correct(answer, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns True if the ground_truth appears as the final number in the answer, ignoring whitespace and punctuation.\n",
    "    Accepts answers like '13', '13.', '13**', 'The answer is 13', '**13**', 'random text **13** random text', 'boxed{13}'.\n",
    "    \"\"\"\n",
    "    # Remove trailing whitespace and punctuation\n",
    "    answer_clean = answer.strip().rstrip('.!**')\n",
    "    # Find all numbers in the answer\n",
    "    numbers = re.findall(r'\\d+', answer_clean)\n",
    "    # Check for markdown-style **13**, boxed{13}, or last number match\n",
    "    boxed_pattern = fr'boxed\\{{\\s*{re.escape(ground_truth)}\\s*\\}}'\n",
    "    return (f\"**{ground_truth}**\" in answer or\n",
    "            bool(numbers and numbers[-1] == ground_truth) or\n",
    "            re.search(boxed_pattern, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fade73",
   "metadata": {},
   "source": [
    "## Martian AI Model Names List\n",
    "\n",
    "Extracted from https://app.withmartian.com/docs/index.html\n",
    "Includes model names, costs, and parameter counts where available\n",
    "\n",
    "Model data with costs (per 1M tokens) and parameters\n",
    "Format: (model_name, input_cost_per_1M, output_cost_per_1M, request_cost, estimated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "martian_models_data = [\n",
    "    (\"martian/code\", 2.40, 12.00, 0.00, None),\n",
    "    (\"agentica-org/deepcoder-14b-preview\", 0.02, 0.02, 0.00, \"14B\"),\n",
    "    (\"ai21/jamba-1.6-large\", 2.00, 8.00, 0.00, None),\n",
    "    (\"ai21/jamba-1.6-mini\", 0.20, 0.40, 0.00, None),\n",
    "    (\"aion-labs/aion-rp-llama-3.1-8b\", 0.20, 0.20, 0.00, \"8B\"),\n",
    "    (\"alfredpros/codellama-7b-instruct-solidity\", 0.80, 1.20, 0.00, \"7B\"),\n",
    "    (\"alpindale/goliath-120b\", 9.00, 11.00, 0.00, \"120B\"),\n",
    "    (\"amazon/nova-lite-v1\", 0.06, 0.24, 0.00, None),\n",
    "    (\"amazon/nova-micro-v1\", 0.04, 0.14, 0.00, None),\n",
    "    (\"amazon/nova-pro-v1\", 0.80, 3.20, 0.00, None),\n",
    "    (\"anthracite-org/magnum-v4-72b\", 2.50, 3.00, 0.00, \"72B\"),\n",
    "    (\"anthropic/claude-3-5-haiku-20241022\", 0.80, 4.00, 0.00, None),\n",
    "    (\"anthropic/claude-3-5-sonnet-20240620\", 3.00, 15.00, 0.00, None),\n",
    "    (\"anthropic/claude-3-5-sonnet-20241022\", 3.00, 15.00, 0.00, None),\n",
    "    (\"anthropic/claude-3-7-sonnet-20250219\", 3.00, 15.00, 0.00, None),\n",
    "    (\"anthropic/claude-3-haiku-20240307\", 0.25, 1.25, 0.00, None),\n",
    "    (\"anthropic/claude-3-opus-20240229\", 15.00, 75.00, 0.00, None),\n",
    "    (\"anthropic/claude-opus-4-1-20250805\", 15.00, 75.00, 0.00, None),\n",
    "    (\"anthropic/claude-opus-4-20250514\", 15.00, 75.00, 0.00, None),\n",
    "    (\"anthropic/claude-sonnet-4-20250514\", 3.00, 15.00, 0.00, None),\n",
    "    (\"arcee-ai/arcee-blitz\", 0.45, 0.75, 0.00, None),\n",
    "    (\"arcee-ai/caller-large\", 0.55, 0.85, 0.00, None),\n",
    "    (\"arcee-ai/coder-large\", 0.50, 0.80, 0.00, None),\n",
    "    (\"arcee-ai/maestro-reasoning\", 0.90, 3.30, 0.00, None),\n",
    "    (\"arcee-ai/virtuoso-large\", 0.75, 1.20, 0.00, None),\n",
    "    (\"arliai/qwq-32b-arliai-rpr-v1\", 0.02, 0.02, 0.00, \"32B\"),\n",
    "    (\"baidu/ernie-4.5-300b-a47b\", 0.28, 1.10, 0.00, \"300B\"),\n",
    "    (\"bytedance/ui-tars-1.5-7b\", 0.10, 0.20, 0.00, \"7B\"),\n",
    "    (\"cognitivecomputations/dolphin-mixtral-8x22b\", 0.90, 0.90, 0.00, \"8x22B\"),\n",
    "    (\"cognitivecomputations/dolphin3.0-r1-mistral-24b\", 0.01, 0.01, 0.00, \"24B\"),\n",
    "    (\"cohere/command\", 1.00, 2.00, 0.00, None),\n",
    "    (\"cohere/command-a\", 2.50, 10.00, 0.00, None),\n",
    "    (\"cohere/command-r\", 0.50, 1.50, 0.00, None),\n",
    "    (\"cohere/command-r-03-2024\", 0.50, 1.50, 0.00, None),\n",
    "    (\"cohere/command-r-08-2024\", 0.15, 0.60, 0.00, None),\n",
    "    (\"cohere/command-r-plus\", 3.00, 15.00, 0.00, None),\n",
    "    (\"cohere/command-r-plus-04-2024\", 3.00, 15.00, 0.00, None),\n",
    "    (\"cohere/command-r-plus-08-2024\", 2.50, 10.00, 0.00, None),\n",
    "    (\"cohere/command-r7b-12-2024\", 0.04, 0.15, 0.00, \"7B\"),\n",
    "    (\"deepseek/deepseek-chat\", 0.27, 0.27, 0.00, None),\n",
    "    (\"deepseek/deepseek-chat-v3-0324\", 0.25, 0.85, 0.00, None),\n",
    "    (\"deepseek/deepseek-prover-v2\", 0.50, 2.18, 0.00, None),\n",
    "    (\"deepseek/deepseek-r1\", 0.40, 2.00, 0.00, None),\n",
    "    (\"deepseek/deepseek-r1-0528\", 0.27, 0.27, 0.00, None),\n",
    "    (\"deepseek/deepseek-r1-0528-qwen3-8b\", 0.01, 0.02, 0.00, \"8B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-llama-70b\", 0.05, 0.05, 0.00, \"70B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-llama-8b\", 0.04, 0.04, 0.00, \"8B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-qwen-1.5b\", 0.18, 0.18, 0.00, \"1.5B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-qwen-14b\", 0.15, 0.15, 0.00, \"14B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-qwen-32b\", 0.08, 0.15, 0.00, \"32B\"),\n",
    "    (\"deepseek/deepseek-r1-distill-qwen-7b\", 0.10, 0.20, 0.00, \"7B\"),\n",
    "    (\"eleutherai/llemma_7b\", 0.80, 1.20, 0.00, \"7B\"),\n",
    "    (\"eva-unit-01/eva-qwen-2.5-72b\", 4.00, 6.00, 0.00, \"72B\"),\n",
    "    (\"google/gemini-2.0-flash-001\", 0.10, 0.40, 0.00, None),\n",
    "    (\"google/gemini-2.0-flash-lite-001\", 0.08, 0.30, 0.00, None),\n",
    "    (\"google/gemini-2.5-flash\", 0.30, 2.50, 0.00, None),\n",
    "    (\"google/gemini-2.5-flash-lite\", 0.10, 0.40, 0.00, None),\n",
    "    (\"google/gemini-2.5-flash-lite-preview-06-17\", 0.10, 0.40, 0.00, None),\n",
    "    (\"google/gemini-2.5-pro\", 1.25, 10.00, 0.00, None),\n",
    "    (\"google/gemini-2.5-pro-preview-05-06\", 1.25, 10.00, 0.00, None),\n",
    "    (\"google/gemma-3-12b-it\", 0.03, 0.03, 0.00, \"12B\"),\n",
    "    (\"google/gemma-3-27b-it\", 0.09, 0.17, 0.00, \"27B\"),\n",
    "    (\"google/gemma-3-4b-it\", 0.02, 0.04, 0.00, \"4B\"),\n",
    "    (\"google/gemma-3n-e4b-it\", 0.02, 0.04, 0.00, \"4B\"),\n",
    "    (\"gryphe/mythomax-l2-13b\", 0.06, 0.06, 0.00, \"13B\"),\n",
    "    (\"inception/mercury\", 0.25, 1.00, 0.00, None),\n",
    "    (\"inception/mercury-coder\", 0.25, 1.00, 0.00, None),\n",
    "    (\"infermatic/mn-inferor-12b\", 0.80, 1.20, 0.00, \"12B\"),\n",
    "    (\"liquid/lfm-3b\", 0.02, 0.02, 0.00, \"3B\"),\n",
    "    (\"liquid/lfm-40b\", 0.15, 0.15, 0.00, \"40B\"),\n",
    "    (\"liquid/lfm-7b\", 0.01, 0.01, 0.00, \"7B\"),\n",
    "    (\"mancer/weaver\", 1.50, 1.50, 0.00, None),\n",
    "    (\"meta-llama/llama-3-70b-instruct\", 0.30, 0.40, 0.00, \"70B\"),\n",
    "    (\"meta-llama/llama-3-8b-instruct\", 0.03, 0.06, 0.00, \"8B\"),\n",
    "    (\"meta-llama/llama-3.1-405b\", 2.00, 2.00, 0.00, \"405B\"),\n",
    "    (\"meta-llama/llama-3.1-70b-instruct\", 0.10, 0.28, 0.00, \"70B\"),\n",
    "    (\"meta-llama/llama-3.1-8b-instruct\", 0.02, 0.02, 0.00, \"8B\"),\n",
    "    (\"meta-llama/llama-3.2-11b-vision-instruct\", 0.05, 0.05, 0.00, \"11B\"),\n",
    "    (\"meta-llama/llama-3.2-1b-instruct\", 0.01, 0.01, 0.00, \"1B\"),\n",
    "    (\"meta-llama/llama-3.2-3b-instruct\", 0.00, 0.01, 0.00, \"3B\"),\n",
    "    (\"meta-llama/llama-3.2-90b-vision-instruct\", 1.20, 1.20, 0.00, \"90B\"),\n",
    "    (\"meta-llama/llama-3.3-70b-instruct\", 0.04, 0.12, 0.00, \"70B\"),\n",
    "    (\"meta-llama/llama-4-maverick\", 0.15, 0.60, 0.00, None),\n",
    "    (\"meta-llama/llama-4-scout\", 0.08, 0.30, 0.00, None),\n",
    "    (\"meta-llama/llama-guard-2-8b\", 0.20, 0.20, 0.00, \"8B\"),\n",
    "    (\"meta-llama/llama-guard-3-8b\", 0.02, 0.06, 0.00, \"8B\"),\n",
    "    (\"meta-llama/llama-guard-4-12b\", 0.05, 0.05, 0.00, \"12B\"),\n",
    "    (\"microsoft/phi-3-medium-128k-instruct\", 1.00, 1.00, 0.00, None),\n",
    "    (\"microsoft/phi-3-mini-128k-instruct\", 0.10, 0.10, 0.00, None),\n",
    "    (\"microsoft/phi-3.5-mini-128k-instruct\", 0.10, 0.10, 0.00, None),\n",
    "    (\"microsoft/phi-4\", 0.06, 0.14, 0.00, None),\n",
    "    (\"microsoft/phi-4-multimodal-instruct\", 0.05, 0.10, 0.00, None),\n",
    "    (\"microsoft/phi-4-reasoning-plus\", 0.07, 0.35, 0.00, None),\n",
    "    (\"microsoft/wizardlm-2-8x22b\", 0.48, 0.48, 0.00, \"8x22B\"),\n",
    "    (\"minimax/minimax-m1\", 0.30, 1.65, 0.00, None),\n",
    "    (\"mistralai/codestral-2501\", 0.30, 0.90, 0.00, None),\n",
    "    (\"mistralai/devstral-medium\", 0.40, 2.00, 0.00, None),\n",
    "    (\"mistralai/devstral-small\", 0.07, 0.28, 0.00, None),\n",
    "    (\"mistralai/devstral-small-2505\", 0.03, 0.03, 0.00, None),\n",
    "    (\"mistralai/magistral-medium-2506\", 2.00, 5.00, 0.00, None),\n",
    "    (\"mistralai/magistral-medium-2506:thinking\", 2.00, 5.00, 0.00, None),\n",
    "    (\"mistralai/magistral-small-2506\", 0.50, 1.50, 0.00, None),\n",
    "    (\"mistralai/ministral-3b\", 0.04, 0.04, 0.00, \"3B\"),\n",
    "    (\"mistralai/ministral-8b\", 0.10, 0.10, 0.00, \"8B\"),\n",
    "    (\"mistralai/mistral-7b-instruct\", 0.03, 0.05, 0.00, \"7B\"),\n",
    "    (\"mistralai/mistral-7b-instruct-v0.1\", 0.11, 0.19, 0.00, \"7B\"),\n",
    "    (\"mistralai/mistral-7b-instruct-v0.2\", 0.20, 0.20, 0.00, \"7B\"),\n",
    "    (\"mistralai/mistral-7b-instruct-v0.3\", 0.03, 0.05, 0.00, \"7B\"),\n",
    "    (\"mistralai/mistral-large\", 2.00, 6.00, 0.00, None),\n",
    "    (\"mistralai/mistral-large-2407\", 2.00, 6.00, 0.00, None),\n",
    "    (\"mistralai/mistral-large-2411\", 2.00, 6.00, 0.00, None),\n",
    "    (\"mistralai/mistral-medium-3\", 0.40, 2.00, 0.00, None),\n",
    "    (\"mistralai/mistral-nemo\", 0.01, 0.05, 0.00, None),\n",
    "    (\"mistralai/mistral-saba\", 0.20, 0.60, 0.00, None),\n",
    "    (\"mistralai/mistral-small\", 0.20, 0.60, 0.00, None),\n",
    "    (\"mistralai/mistral-small-24b-instruct-2501\", 0.03, 0.03, 0.00, \"24B\"),\n",
    "    (\"mistralai/mistral-small-3.1-24b-instruct\", 0.03, 0.03, 0.00, \"24B\"),\n",
    "    (\"mistralai/mistral-small-3.2-24b-instruct\", 0.05, 0.10, 0.00, \"24B\"),\n",
    "    (\"mistralai/mistral-tiny\", 0.25, 0.25, 0.00, None),\n",
    "    (\"mistralai/mixtral-8x22b-instruct\", 0.90, 0.90, 0.00, \"8x22B\"),\n",
    "    (\"mistralai/mixtral-8x7b-instruct\", 0.08, 0.24, 0.00, \"8x7B\"),\n",
    "    (\"mistralai/pixtral-large-2411\", 2.00, 6.00, 0.00, None),\n",
    "    (\"moonshotai/kimi-k2\", 0.14, 2.49, 0.00, None),\n",
    "    (\"moonshotai/kimi-vl-a3b-thinking\", 0.04, 0.04, 0.00, \"3B\"),\n",
    "    (\"morph/morph-v2\", 1.20, 2.70, 0.00, None),\n",
    "    (\"morph/morph-v3-fast\", 1.20, 2.70, 0.00, None),\n",
    "    (\"morph/morph-v3-large\", 1.20, 2.70, 0.00, None),\n",
    "    (\"neversleep/llama-3-lumimaid-70b\", 4.00, 6.00, 0.00, \"70B\"),\n",
    "    (\"neversleep/llama-3.1-lumimaid-8b\", 0.18, 1.00, 0.00, \"8B\"),\n",
    "    (\"neversleep/noromaid-20b\", 1.25, 2.00, 0.00, \"20B\"),\n",
    "    (\"nothingiisreal/mn-celeste-12b\", 0.80, 1.20, 0.00, \"12B\"),\n",
    "    (\"nousresearch/deephermes-3-mistral-24b-preview\", 0.14, 0.14, 0.00, \"24B\"),\n",
    "    (\"nousresearch/hermes-2-pro-llama-3-8b\", 0.03, 0.04, 0.00, \"8B\"),\n",
    "    (\"nousresearch/hermes-3-llama-3.1-405b\", 0.70, 0.80, 0.00, \"405B\"),\n",
    "    (\"nousresearch/hermes-3-llama-3.1-70b\", 0.10, 0.28, 0.00, \"70B\"),\n",
    "    (\"nousresearch/nous-hermes-2-mixtral-8x7b-dpo\", 0.60, 0.60, 0.00, \"8x7B\"),\n",
    "    (\"nvidia/llama-3.1-nemotron-70b-instruct\", 0.12, 0.30, 0.00, \"70B\"),\n",
    "    (\"nvidia/llama-3.1-nemotron-ultra-253b-v1\", 0.60, 1.80, 0.00, \"253B\"),\n",
    "    (\"nvidia/llama-3.3-nemotron-super-49b-v1\", 0.13, 0.40, 0.00, \"49B\"),\n",
    "    (\"openai/chatgpt-4o-latest\", 5.00, 15.00, 0.00, None),\n",
    "    (\"openai/gpt-3.5-turbo\", 0.50, 1.50, 0.00, None),\n",
    "    (\"openai/gpt-3.5-turbo-16k\", 3.00, 4.00, 0.00, None),\n",
    "    (\"openai/gpt-4\", 30.00, 60.00, 0.00, None),\n",
    "    (\"openai/gpt-4-0314\", 30.00, 60.00, 0.00, None),\n",
    "    (\"openai/gpt-4-1106-preview\", 10.00, 30.00, 0.00, None),\n",
    "    (\"openai/gpt-4-turbo\", 10.00, 30.00, 0.00, None),\n",
    "    (\"openai/gpt-4-turbo-preview\", 10.00, 30.00, 0.00, None),\n",
    "    (\"openai/gpt-4.1\", 2.00, 8.00, 0.00, None),\n",
    "    (\"openai/gpt-4.1-mini\", 0.40, 1.60, 0.00, None),\n",
    "    (\"openai/gpt-4.1-nano\", 0.10, 0.40, 0.00, None),\n",
    "    (\"openai/gpt-4o\", 2.50, 10.00, 0.00, None),\n",
    "    (\"openai/gpt-4o-2024-05-13\", 5.00, 15.00, 0.00, None),\n",
    "    (\"openai/gpt-4o-2024-08-06\", 2.50, 10.00, 0.00, None),\n",
    "    (\"openai/gpt-4o-2024-11-20\", 2.50, 10.00, 0.00, None),\n",
    "    (\"openai/gpt-4o-mini\", 0.15, 0.60, 0.00, None),\n",
    "    (\"openai/gpt-4o-mini-2024-07-18\", 0.15, 0.60, 0.00, None),\n",
    "    (\"openai/gpt-4o-mini-search-preview\", 0.15, 0.60, 0.03, None),\n",
    "    (\"openai/gpt-4o-search-preview\", 2.50, 10.00, 0.04, None),\n",
    "    (\"openai/gpt-5\", 1.25, 10.00, 0.00, None),\n",
    "    (\"openai/gpt-5-mini\", 0.25, 2.00, 0.00, None),\n",
    "    (\"openai/gpt-5-nano\", 0.05, 0.40, 0.00, None),\n",
    "    (\"openai/o1\", 15.00, 60.00, 0.00, None),\n",
    "    (\"openai/o1-mini\", 1.10, 4.40, 0.00, None),\n",
    "    (\"openai/o1-mini-2024-09-12\", 1.10, 4.40, 0.00, None),\n",
    "    (\"openai/o1-preview\", 15.00, 60.00, 0.00, None),\n",
    "    (\"openai/o1-preview-2024-09-12\", 15.00, 60.00, 0.00, None),\n",
    "    (\"openai/o3-mini\", 1.10, 4.40, 0.00, None),\n",
    "    (\"openai/o4-mini\", 1.10, 4.40, 0.00, None),\n",
    "    (\"opengvlab/internvl3-14b\", 0.20, 0.40, 0.00, \"14B\"),\n",
    "    (\"perplexity/r1-1776\", 2.00, 8.00, 0.00, None),\n",
    "    (\"perplexity/sonar\", 1.00, 1.00, 0.01, None),\n",
    "    (\"perplexity/sonar-deep-research\", 2.00, 8.00, 0.00, None),\n",
    "    (\"perplexity/sonar-pro\", 3.00, 15.00, 0.00, None),\n",
    "    (\"perplexity/sonar-reasoning\", 1.00, 5.00, 0.01, None),\n",
    "    (\"perplexity/sonar-reasoning-pro\", 2.00, 8.00, 0.00, None),\n",
    "    (\"pygmalionai/mythalion-13b\", 0.80, 1.20, 0.00, \"13B\"),\n",
    "    (\"qwen/qwen-2-72b-instruct\", 0.90, 0.90, 0.00, \"72B\"),\n",
    "    (\"qwen/qwen-2.5-72b-instruct\", 0.10, 0.10, 0.00, \"72B\"),\n",
    "    (\"qwen/qwen-2.5-7b-instruct\", 0.04, 0.10, 0.00, \"7B\"),\n",
    "    (\"qwen/qwen-2.5-coder-32b-instruct\", 0.06, 0.15, 0.00, \"32B\"),\n",
    "    (\"qwen/qwen-2.5-vl-7b-instruct\", 0.20, 0.20, 0.00, \"7B\"),\n",
    "    (\"qwen/qwen-max\", 1.60, 6.40, 0.00, None),\n",
    "    (\"qwen/qwen-plus\", 0.40, 1.20, 0.00, None),\n",
    "    (\"qwen/qwen-turbo\", 0.05, 0.20, 0.00, None),\n",
    "    (\"qwen/qwen-vl-max\", 0.80, 3.20, 0.00, None),\n",
    "    (\"qwen/qwen2.5-vl-32b-instruct\", 0.20, 0.60, 0.00, \"32B\"),\n",
    "    (\"qwen/qwen2.5-vl-72b-instruct\", 0.25, 0.75, 0.00, \"72B\"),\n",
    "    (\"qwen/qwen3-14b\", 0.06, 0.24, 0.00, \"14B\"),\n",
    "    (\"qwen/qwen3-235b-a22b\", 0.13, 0.60, 0.00, \"235B\"),\n",
    "    (\"qwen/qwen3-235b-a22b-07-25\", 0.12, 0.59, 0.00, \"235B\"),\n",
    "    (\"qwen/qwen3-30b-a3b\", 0.08, 0.29, 0.00, \"30B\"),\n",
    "    (\"qwen/qwen3-32b\", 0.03, 0.03, 0.00, \"32B\"),\n",
    "    (\"qwen/qwen3-8b\", 0.04, 0.14, 0.00, \"8B\"),\n",
    "    (\"qwen/qwen3-coder\", 0.30, 0.30, 0.00, None),\n",
    "    (\"qwen/qwq-32b\", 0.08, 0.15, 0.00, \"32B\"),\n",
    "    (\"raifle/sorcererlm-8x22b\", 4.50, 4.50, 0.00, \"8x22B\"),\n",
    "    (\"sao10k/fimbulvetr-11b-v2\", 0.80, 1.20, 0.00, \"11B\"),\n",
    "    (\"sao10k/l3-euryale-70b\", 1.48, 1.48, 0.00, \"70B\"),\n",
    "    (\"sao10k/l3-lunaris-8b\", 0.02, 0.05, 0.00, \"8B\"),\n",
    "    (\"sao10k/l3.1-euryale-70b\", 0.65, 0.75, 0.00, \"70B\"),\n",
    "    (\"sao10k/l3.3-euryale-70b\", 0.65, 0.75, 0.00, \"70B\"),\n",
    "    (\"sarvamai/sarvam-m\", 0.02, 0.02, 0.00, None),\n",
    "    (\"shisa-ai/shisa-v2-llama3.3-70b\", 0.03, 0.03, 0.00, \"70B\"),\n",
    "    (\"sophosympatheia/midnight-rose-70b\", 0.80, 0.80, 0.00, \"70B\"),\n",
    "    (\"switchpoint/router\", 0.85, 3.40, 0.00, None),\n",
    "    (\"tencent/hunyuan-a13b-instruct\", 0.03, 0.03, 0.00, \"13B\"),\n",
    "    (\"thedrummer/anubis-70b-v1.1\", 0.50, 0.80, 0.00, \"70B\"),\n",
    "    (\"thedrummer/anubis-pro-105b-v1\", 0.50, 1.00, 0.00, \"105B\"),\n",
    "    (\"thedrummer/rocinante-12b\", 0.20, 0.50, 0.00, \"12B\"),\n",
    "    (\"thedrummer/skyfall-36b-v2\", 0.02, 0.02, 0.00, \"36B\"),\n",
    "    (\"thedrummer/unslopnemo-12b\", 0.40, 0.40, 0.00, \"12B\"),\n",
    "    (\"thedrummer/valkyrie-49b-v1\", 0.65, 1.00, 0.00, \"49B\"),\n",
    "    (\"thudm/glm-4-32b\", 0.24, 0.24, 0.00, \"32B\"),\n",
    "    (\"thudm/glm-4.1v-9b-thinking\", 0.04, 0.14, 0.00, \"9B\"),\n",
    "    (\"thudm/glm-z1-32b\", 0.03, 0.03, 0.00, \"32B\"),\n",
    "    (\"tngtech/deepseek-r1t2-chimera\", 0.30, 0.30, 0.00, None),\n",
    "    (\"undi95/remm-slerp-l2-13b\", 0.70, 1.00, 0.00, \"13B\"),\n",
    "    (\"undi95/toppy-m-7b\", 0.80, 1.20, 0.00, \"7B\"),\n",
    "    (\"x-ai/grok-2-1212\", 2.00, 10.00, 0.00, None),\n",
    "    (\"x-ai/grok-2-vision-1212\", 2.00, 10.00, 0.00, None),\n",
    "    (\"x-ai/grok-3\", 3.00, 15.00, 0.00, None),\n",
    "    (\"x-ai/grok-3-beta\", 3.00, 15.00, 0.00, None),\n",
    "    (\"x-ai/grok-3-mini\", 0.30, 0.50, 0.00, None),\n",
    "    (\"x-ai/grok-3-mini-beta\", 0.30, 0.50, 0.00, None),\n",
    "    (\"x-ai/grok-4\", 3.00, 15.00, 0.00, None),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f812972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data analysis\n",
    "def extract_model_names() -> List[str]:\n",
    "    \"\"\"Extract just the model names from the data\"\"\"\n",
    "    return [model[0] for model in martian_models_data]\n",
    "\n",
    "def extract_param_size(param_str: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"Convert parameter string to float (in billions)\"\"\"\n",
    "    if not param_str:\n",
    "        return None\n",
    "    \n",
    "    # Remove 'B' and convert to float\n",
    "    if param_str.endswith('B'):\n",
    "        param_str = param_str[:-1]\n",
    "    \n",
    "    # Handle mixtral format like \"8x22B\" -> approximate total params\n",
    "    if 'x' in param_str:\n",
    "        parts = param_str.split('x')\n",
    "        try:\n",
    "            return float(parts[0]) * float(parts[1])\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        return float(param_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_models_by_provider() -> Dict[str, List[Tuple[str, float, float, float, Optional[str]]]]:\n",
    "    \"\"\"Group models by provider\"\"\"\n",
    "    providers = {}\n",
    "    for model_data in martian_models_data:\n",
    "        provider = model_data[0].split('/')[0]\n",
    "        if provider not in providers:\n",
    "            providers[provider] = []\n",
    "        providers[provider].append(model_data)\n",
    "    return providers\n",
    "\n",
    "def get_cost_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Get cost statistics\"\"\"\n",
    "    input_costs = [model[1] for model in martian_models_data]\n",
    "    output_costs = [model[2] for model in martian_models_data]\n",
    "    \n",
    "    return {\n",
    "        'input_cost_range': (min(input_costs), max(input_costs)),\n",
    "        'output_cost_range': (min(output_costs), max(output_costs)),\n",
    "        'avg_input_cost': sum(input_costs) / len(input_costs),\n",
    "        'avg_output_cost': sum(output_costs) / len(output_costs)\n",
    "    }\n",
    "\n",
    "def get_param_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Get parameter count statistics\"\"\"\n",
    "    params_with_data = [extract_param_size(model[4]) for model in martian_models_data if model[4]]\n",
    "    params_with_data = [p for p in params_with_data if p is not None]\n",
    "    \n",
    "    if not params_with_data:\n",
    "        return {'count': 0}\n",
    "    \n",
    "    return {\n",
    "        'count': len(params_with_data),\n",
    "        'range': (min(params_with_data), max(params_with_data)),\n",
    "        'avg_params': sum(params_with_data) / len(params_with_data),\n",
    "        'models_with_param_info': len([m for m in martian_models_data if m[4]]),\n",
    "        'total_models': len(martian_models_data)\n",
    "    }\n",
    "\n",
    "def find_cheapest_models(top_n: int = 5) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Find the cheapest models by input cost\"\"\"\n",
    "    sorted_by_input = sorted(martian_models_data, key=lambda x: x[1])\n",
    "    return [(model[0], model[1], model[2]) for model in sorted_by_input[:top_n]]\n",
    "\n",
    "def find_most_expensive_models(top_n: int = 5) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Find the most expensive models by input cost\"\"\"\n",
    "    sorted_by_input = sorted(martian_models_data, key=lambda x: x[1], reverse=True)\n",
    "    return [(model[0], model[1], model[2]) for model in sorted_by_input[:top_n]]\n",
    "\n",
    "def find_largest_models(top_n: int = 10) -> List[Tuple[str, Optional[str], float]]:\n",
    "    \"\"\"Find the largest models by parameter count\"\"\"\n",
    "    models_with_params = [(model[0], model[4], extract_param_size(model[4])) \n",
    "                         for model in martian_models_data if model[4]]\n",
    "    models_with_params = [(m[0], m[1], m[2]) for m in models_with_params if m[2] is not None]\n",
    "    sorted_by_params = sorted(models_with_params, key=lambda x: x[2], reverse=True)\n",
    "    return sorted_by_params[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "martian_models = extract_model_names()\n",
    "\n",
    "# Print comprehensive statistics\n",
    "\n",
    "print(\"=== MARTIAN AI MODELS ANALYSIS ===\\n\")\n",
    "\n",
    "print(f\"📊 Total models: {len(martian_models_data)}\")\n",
    "\n",
    "# Provider breakdown\n",
    "providers = get_models_by_provider()\n",
    "print(f\"🏢 Number of providers: {len(providers)}\")\n",
    "#print(\"\\n📈 Models per provider:\")\n",
    "#for provider, models in sorted(providers.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "#    print(f\"  {provider:20} {len(models):3d} models\")\n",
    "\n",
    "# Cost analysis\n",
    "cost_stats = get_cost_stats()\n",
    "print(f\"\\n💰 Cost Analysis (per 1M tokens):\")\n",
    "print(f\"  Input cost range:  ${cost_stats['input_cost_range'][0]:.2f} - ${cost_stats['input_cost_range'][1]:.2f}\")\n",
    "print(f\"  Output cost range: ${cost_stats['output_cost_range'][0]:.2f} - ${cost_stats['output_cost_range'][1]:.2f}\")\n",
    "print(f\"  Average input:     ${cost_stats['avg_input_cost']:.2f}\")\n",
    "print(f\"  Average output:    ${cost_stats['avg_output_cost']:.2f}\")\n",
    "\n",
    "# Parameter analysis\n",
    "param_stats = get_param_stats()\n",
    "print(f\"\\n🧠 Parameter Analysis:\")\n",
    "print(f\"  Models with param info: {param_stats['models_with_param_info']}/{param_stats['total_models']}\")\n",
    "if param_stats['count'] > 0:\n",
    "    print(f\"  Parameter range: {param_stats['range'][0]:.1f}B - {param_stats['range'][1]:.1f}B\")\n",
    "    print(f\"  Average size: {param_stats['avg_params']:.1f}B parameters\")\n",
    "\n",
    "# Top lists\n",
    "print(f\"\\n💸 5 Cheapest Models (input cost):\")\n",
    "for i, (model, input_cost, output_cost) in enumerate(find_cheapest_models(), 1):\n",
    "    print(f\"  {i}. {model:40} ${input_cost:.2f}/${output_cost:.2f}\")\n",
    "\n",
    "print(f\"\\n💎 5 Most Expensive Models (input cost):\")\n",
    "for i, (model, input_cost, output_cost) in enumerate(find_most_expensive_models(), 1):\n",
    "    print(f\"  {i}. {model:40} ${input_cost:.2f}/${output_cost:.2f}\")\n",
    "\n",
    "print(f\"\\n🦣 10 Largest Models:\")\n",
    "for i, (model, param_str, param_float) in enumerate(find_largest_models(), 1):\n",
    "    print(f\"  {i:2d}. {model:40} {param_str:>8} ({param_float:.1f}B)\")\n",
    "\n",
    "print(f\"\\n📋 Sample model data structure:\")\n",
    "print(f\"   Format: (model_name, input_cost_per_1M, output_cost_per_1M, request_cost, params)\")\n",
    "for i in range(3):\n",
    "    model = martian_models_data[i]\n",
    "    print(f\"   {model}\")\n",
    "\n",
    "print(f\"\\n✅ Use 'martian_models' for just the model names list\")\n",
    "print(f\"✅ Use 'martian_models_data' for full data with costs and parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8794",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import time\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MARTIAN_API_KEY = os.getenv(\"MARTIAN_API_KEY\")\n",
    "assert MARTIAN_API_KEY, \"API key not found. Please set MARTIAN_API_KEY in your .env file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.withmartian.com/v1\",\n",
    "    api_key=MARTIAN_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_inference(model_name, prompt, ground_truth, timeout=300):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        max_tokens=1024, # Mandatory param for some models. Ignored by others.\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    success = is_ground_truth_correct(answer, ground_truth)\n",
    "    return answer, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_progressive(the_models, tests, max_workers=8):\n",
    "    model_scores = []\n",
    "\n",
    "    def score_model(model_name):\n",
    "        score = 0\n",
    "        for test_idx, (prompt, ground_truth) in enumerate(tests):\n",
    "            try:\n",
    "                answer, success = run_model_inference(model_name, prompt, ground_truth)\n",
    "                if success:\n",
    "                    score = test_idx + 1\n",
    "                else:\n",
    "                    break\n",
    "            except openai.APIError as e:\n",
    "                if hasattr(e, 'status_code'):\n",
    "                    score = -e.status_code\n",
    "                else:\n",
    "                    score = -999\n",
    "                break\n",
    "            except Exception as e:\n",
    "                score = -999\n",
    "                break\n",
    "        return {\"model\": model_name, \"score\": score}\n",
    "\n",
    "    print(f\"Evaluating {len(the_models)} models concurrently with {max_workers} workers...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_model = {executor.submit(score_model, model_name): model_name for model_name in the_models}\n",
    "        for idx, future in enumerate(as_completed(future_to_model), 1):\n",
    "            model_name = future_to_model[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = {\"model\": model_name, \"score\": -999}\n",
    "            print(f\"[{idx}/{len(the_models)}] {result['model']}: Score = {result['score']}\")\n",
    "            model_scores.append(result)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a759f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated martian_models_data structure to extract model names\n",
    "the_models = [model[0] for model in martian_models_data]\n",
    "model_scores = evaluate_models_progressive(the_models, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af377696",
   "metadata": {},
   "source": [
    "# Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d238c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    " \n",
    "# Summarize number of models by score\n",
    "print(f\"Total number of models: {len(martian_models)}\")\n",
    "score_counts = Counter([m['score'] for m in model_scores])\n",
    "print(\"Score summary (number of models by score):\")\n",
    "for score, count in sorted(score_counts.items()):\n",
    "    print(f\"Score {score}: {count} models\") \n",
    "\n",
    "# Print total number of models with negative and non-negative scores\n",
    "num_negative = sum(count for score, count in score_counts.items() if isinstance(score, int) and score < 0)\n",
    "num_nonneg = sum(count for score, count in score_counts.items() if isinstance(score, int) and score >= 0)\n",
    "print(f\"\\nTotal models with negative score: {num_negative}\")\n",
    "print(f\"Total models with non-negative score: {num_nonneg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the results with categorical x-axis for scores (excluding negative scores)\n",
    "scores_sorted = sorted([s for s in score_counts.keys() if isinstance(s, int) and s >= 0], key=lambda x: x)\n",
    "counts_sorted = [score_counts[s] for s in scores_sorted]\n",
    "plt.figure(figsize=(10,3))\n",
    "bars = plt.bar(range(len(scores_sorted)), counts_sorted, color='skyblue')\n",
    "plt.xlabel('Model Score on Addition Tasks', fontsize=18)\n",
    "plt.ylabel('Number of Models', fontsize=18)\n",
    "# No title per user request\n",
    "plt.xticks(range(len(scores_sorted)), [str(s) for s in scores_sorted], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "# Add count labels inside bars only where count > 4, with larger font\n",
    "for bar, count in zip(bars, counts_sorted):\n",
    "    if count > 4:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(count), ha='center', va='center', fontsize=18, color='black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Output Token Cost vs. Model Score (Score as categorical x-axis, only non-negative scores)\n",
    "import numpy as np\n",
    "model_name_to_output_cost = {model[0]: model[2] for model in martian_models_data}\n",
    "model_names = [entry['model'] for entry in model_scores]\n",
    "scores = [entry['score'] for entry in model_scores]\n",
    "output_costs = [model_name_to_output_cost.get(name, np.nan) for name in model_names]\n",
    "# Filter for non-negative scores only\n",
    "filtered = [(score, cost) for score, cost in zip(scores, output_costs) if isinstance(score, int) and score >= 0]\n",
    "if filtered:\n",
    "    filtered_scores, filtered_output_costs = zip(*filtered)\n",
    "else:\n",
    "    filtered_scores, filtered_output_costs = [], []\n",
    "# Prepare categorical x-axis for scores\n",
    "unique_scores = sorted(set(filtered_scores), key=lambda x: (isinstance(x, int), x))\n",
    "score_to_x = {score: i for i, score in enumerate(unique_scores)}\n",
    "x_vals = [score_to_x[score] for score in filtered_scores]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x_vals, filtered_output_costs, alpha=0.6, color='purple')\n",
    "plt.xlabel('Model Score on Addition Tasks', fontsize=18)\n",
    "plt.ylabel('Model Output Token Cost (USD per 1M tokens)', fontsize=16)\n",
    "# No title per user request\n",
    "plt.xticks(range(len(unique_scores)), [str(s) for s in unique_scores], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4675d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only models with a negative score or score 12\n",
    "score_to_models = {}\n",
    "for entry in model_scores:\n",
    "    score = entry['score']\n",
    "    if score <= 0 or score >= 10:\n",
    "        score_to_models.setdefault(score, []).append(entry['model'])\n",
    "\n",
    "for score in sorted(score_to_models):\n",
    "    print(f\"\\nModels with score {score}:\")\n",
    "    for model in score_to_models[score]:\n",
    "        print(f\"  {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc0d0a",
   "metadata": {},
   "source": [
    "# Check zero-score model output\n",
    "\n",
    "On occasion, check the output of the \"0 score\" models to see if unusual answer formats are hiding correct answers.\n",
    "If so, is_ground_truth_correct should be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf101ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run models that scored 0 on test[0] and show their outputs\n",
    "def run_zero_score_models():\n",
    "    zero_score_models = [m['model'] for m in model_scores if m['score'] == 0]\n",
    "    print(f\"Re-running {len(zero_score_models)} models that scored 0 on the first test...\")\n",
    "    outputs = []\n",
    "    for model_name in zero_score_models:\n",
    "        try:\n",
    "            answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "            success = False\n",
    "        outputs.append({'model': model_name, 'output': answer, 'success': success})\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\nSuccess: {success}\\n{'-'*40}\")\n",
    "\n",
    "\n",
    "#run_zero_score_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800d6bc",
   "metadata": {},
   "source": [
    "# Check -400 score models\n",
    "\n",
    "Models can generate -400 scores if a mandatory parameter is missing. This may hide a model that can do addition.\n",
    "Run the models that give a -400 score and see if the detailed error message shows the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4176071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run models that scored -400 and show their outputs and errors\n",
    "def run_minus_400_score_models():\n",
    "    minus_400_models = [m['model'] for m in model_scores if m['score'] == -400]\n",
    "    print(f\"Re-running {len(minus_400_models)} models that scored -400 on the first test...\")\n",
    "    outputs = []\n",
    "    for model_name in minus_400_models:\n",
    "        try:\n",
    "            answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "            success = False\n",
    "        outputs.append({'model': model_name, 'output': answer, 'success': success})\n",
    "        print(f\"Model: {model_name}\\nOutput/Error: {answer}\\nSuccess: {success}\\n{'-'*40}\")\n",
    "\n",
    "#run_minus_400_score_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173192cb",
   "metadata": {},
   "source": [
    "# High Variability Example\n",
    "The model \"alfredpros/codellama-7b-instruct-solidity\" has high variability. Sometimes answer is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b587aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_high_variability_model():\n",
    "    model_name = \"alfredpros/codellama-7b-instruct-solidity\"\n",
    "    for i in range(5):\n",
    "        answer, success = run_model_inference(model_name, tests[0][0], tests[0][1])\n",
    "        print(f\"Success: {success}. Output: {answer}\")\n",
    "\n",
    "#run_high_variability_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb89f",
   "metadata": {},
   "source": [
    "# Ask how the models that score >=10 work\n",
    "\n",
    "Inconclusive: \n",
    "\n",
    "Sometimes the detailed methodology is mathematically incorrect.\n",
    "Sometimes the outline methodology looks like a sentence from a text book.\n",
    "The bext valid explanations say the model does addition like a human - line up digit pairs - then sum right digit-pair to-left digit pair calculating carries \n",
    "The \"concise\" setting makes these model reply immediately - unless they have many hidden \"thinking tokens\" the human approach is not feasible.\n",
    "\n",
    "Some of these models gpt-4o-mini-search-preview and gpt-4o-search-preview explicitly say they have search/tool access. \n",
    "So they can just call a python function to calculate the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_work_for_score_best_models():\n",
    "    best_models = [m['model'] for m in model_scores if m['score'] >= 12]\n",
    "    ground_prompt = \"998522030597+1477969406=\"\n",
    "    concise_prompt = f\"Answer concisely: {ground_prompt}\"\n",
    "    explain_prompt = f\"Show your work for {ground_prompt}\"\n",
    "    ground_truth = str(998522030597 + 1477969406)\n",
    "\n",
    "    print(f\"Querying {len(best_models)} models that scored best...\")\n",
    "    for model_name in best_models:\n",
    "        try:\n",
    "            answer, _ = run_model_inference(model_name, concise_prompt, ground_truth)\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\n{'-'*40}\")\n",
    "\n",
    "        try:\n",
    "            answer, _ = run_model_inference(model_name, explain_prompt, ground_truth)\n",
    "        except Exception as e:\n",
    "            answer = f\"Error: {str(e)}\"\n",
    "        print(f\"Model: {model_name}\\nOutput: {answer}\\n{'-'*40}\")\n",
    "\n",
    "#show_work_for_score_best_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
